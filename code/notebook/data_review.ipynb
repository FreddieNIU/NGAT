{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import ast\n",
    "import sys \n",
    "import itertools\n",
    "from collections import Counter\n",
    "ROOT_FOLDER = \"/home/yingjie/Year_3/\"\n",
    "# ROOT_FOLDER = \"/home/yingjie_niu/Year_3/\"\n",
    "sys.path.append(ROOT_FOLDER+\"GNN_longterm/code/graph_building/\") \n",
    "from edge_generator import EdgeGenerator\n",
    "from node_generator import NodeGenerator\n",
    "from graph_generator import GraphGenerator\n",
    "from firmgraph_dataset import FirmRelationGraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda'), 1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device, torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review ACL2018 Dataset \n",
    "01/01/2014-01/01/2016\n",
    "\n",
    "Check the tweets available period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset spy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl18_fp = \"/home/yingjie/Year_3/GNN_longterm/data/raw/ACL2018/\"\n",
    "acl18_price_raw = acl18_fp+\"price/raw/\"\n",
    "acl18_tweet_preprocessed = acl18_fp+\"tweet/preprocessed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERIOD_START = '2014-01-02'\n",
    "PERIOD_END = '2016-01-04'\n",
    "# Find out the firms whose tweets data is not from 2014-01 -- 2015-12, return a list of firms\n",
    "firms = []\n",
    "for ticker_folder in os.listdir(acl18_tweet_preprocessed):\n",
    "    if ticker_folder[0] != '.':\n",
    "        tweet_list = os.listdir(acl18_tweet_preprocessed+ticker_folder)\n",
    "        if sorted(tweet_list)[0][:7] != '2014-01':\n",
    "            print(ticker_folder, sorted(tweet_list)[0],sorted(tweet_list)[-1])\n",
    "            if ticker_folder not in firms:\n",
    "                firms.append(ticker_folder)\n",
    "        if sorted(tweet_list)[-1][:7] != '2015-12':\n",
    "            print(ticker_folder, sorted(tweet_list)[0],sorted(tweet_list)[-1])\n",
    "            if ticker_folder not in firms:\n",
    "                firms.append(ticker_folder)\n",
    "        \n",
    "print(len(firms), firms)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log return</th>\n",
       "      <th>hist_rmean1</th>\n",
       "      <th>r_mean1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.512104</td>\n",
       "      <td>-0.512104</td>\n",
       "      <td>1.283925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.283925</td>\n",
       "      <td>1.283925</td>\n",
       "      <td>-0.247075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.247075</td>\n",
       "      <td>-0.247075</td>\n",
       "      <td>-0.283711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.283711</td>\n",
       "      <td>-0.283711</td>\n",
       "      <td>-1.631558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.631558</td>\n",
       "      <td>-1.631558</td>\n",
       "      <td>1.562427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.562427</td>\n",
       "      <td>1.562427</td>\n",
       "      <td>-0.290500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.290500</td>\n",
       "      <td>-0.290500</td>\n",
       "      <td>0.184779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.184779</td>\n",
       "      <td>0.184779</td>\n",
       "      <td>0.119139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.119139</td>\n",
       "      <td>0.119139</td>\n",
       "      <td>1.435995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.435995</td>\n",
       "      <td>1.435995</td>\n",
       "      <td>-0.601989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.601989</td>\n",
       "      <td>-0.601989</td>\n",
       "      <td>-1.218880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.218880</td>\n",
       "      <td>-1.218880</td>\n",
       "      <td>0.177217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.177217</td>\n",
       "      <td>0.177217</td>\n",
       "      <td>-0.879615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.879615</td>\n",
       "      <td>-0.879615</td>\n",
       "      <td>1.185179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.185179</td>\n",
       "      <td>1.185179</td>\n",
       "      <td>-0.257441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.257441</td>\n",
       "      <td>-0.257441</td>\n",
       "      <td>0.755121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.755121</td>\n",
       "      <td>0.755121</td>\n",
       "      <td>1.827628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.827628</td>\n",
       "      <td>1.827628</td>\n",
       "      <td>2.327422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.327422</td>\n",
       "      <td>2.327422</td>\n",
       "      <td>1.834841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.834841</td>\n",
       "      <td>1.834841</td>\n",
       "      <td>-0.874209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    log return  hist_rmean1   r_mean1\n",
       "0    -0.512104    -0.512104  1.283925\n",
       "1     1.283925     1.283925 -0.247075\n",
       "2    -0.247075    -0.247075 -0.283711\n",
       "3    -0.283711    -0.283711 -1.631558\n",
       "4    -1.631558    -1.631558  1.562427\n",
       "5     1.562427     1.562427 -0.290500\n",
       "6    -0.290500    -0.290500  0.184779\n",
       "7     0.184779     0.184779  0.119139\n",
       "8     0.119139     0.119139  1.435995\n",
       "9     1.435995     1.435995 -0.601989\n",
       "10   -0.601989    -0.601989 -1.218880\n",
       "11   -1.218880    -1.218880  0.177217\n",
       "12    0.177217     0.177217 -0.879615\n",
       "13   -0.879615    -0.879615  1.185179\n",
       "14    1.185179     1.185179 -0.257441\n",
       "15   -0.257441    -0.257441  0.755121\n",
       "16    0.755121     0.755121  1.827628\n",
       "17    1.827628     1.827628  2.327422\n",
       "18    2.327422     2.327422  1.834841\n",
       "19    1.834841     1.834841 -0.874209"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/yingjie_niu/Year_3/GNN_longterm/data/raw/ACL2018/price/raw/AAPL.csv\")\n",
    "df['log return'] = 100 * np.log(df['Adj Close']/df['Adj Close'].shift(1))\n",
    "\n",
    "def rescale(df):\n",
    "    df[\"log C/O\"] = 100 * np.log(df['Close']/df['Open'])\n",
    "    df[\"log H/O\"] = 100 * np.log(df['High']/df['Open'])\n",
    "    df[\"log L/O\"] = 100 * np.log(df['Low']/df['Open'])\n",
    "    ## We normalize Open and Volumn\n",
    "    cols_to_seperate_norm = ['Open', 'Volume']\n",
    "    df[cols_to_seperate_norm] = df[cols_to_seperate_norm].apply(lambda x: (x - x.mean()) / x.std())\n",
    "    return df\n",
    "\n",
    "df = rescale(df)\n",
    "begin_idx = df[df['Date']==\"2014-01-02\"].index[0]\n",
    "end_idx = df[df['Date']==\"2016-01-04\"].index[0]\n",
    "for m in [1, 5, 10, 21]:   \n",
    "    # for each step length, calculate the r_mean and std of log returns\n",
    "    df['r_mean'+str(m)] = df['log return'].shift(-m).rolling(window=m).mean()\n",
    "    df['std'+str(m)] = df['log return'].shift(-m).rolling(window=m).std()\n",
    "\n",
    "    # # 计算r_mean 和 std的第二种方法\n",
    "    # df['r_mean'+str(m)] = df['log return'].rolling(window=m+self.estimation_window).mean().shift(-m)\n",
    "    # df['std'+str(m)] = df['log return'].rolling(window=m+self.estimation_window).std().shift(-m)\n",
    "    \n",
    "\n",
    "    # for each step length, also calculate the past 5-day 10-day and 21-day average of log return, these are used in the input (node features)\n",
    "    df['hist_rmean'+str(m)] = df['log return'].rolling(window=m).mean()\n",
    "    df['hist_std'+str(m)] = df['log return'].rolling(window=m).std()\n",
    "target_df = df.iloc[begin_idx-2*21+1:end_idx]\n",
    "target_df.reset_index(drop=True, inplace=True)\n",
    "target_df[['log return', 'hist_rmean1', 'r_mean1']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.602367894813303"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df['log return'][5:10].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:r_mean1         0.744101\n",
      "r_mean5         1.159842\n",
      "hist_rmean5     0.090223\n",
      "r_mean10        0.606195\n",
      "hist_rmean10   -0.761904\n",
      "r_mean21        0.229636\n",
      "hist_rmean21   -0.292128\n",
      "Name: 63, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r_mean1</th>\n",
       "      <th>r_mean5</th>\n",
       "      <th>hist_rmean5</th>\n",
       "      <th>r_mean10</th>\n",
       "      <th>hist_rmean10</th>\n",
       "      <th>r_mean21</th>\n",
       "      <th>hist_rmean21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.717699</td>\n",
       "      <td>-0.303798</td>\n",
       "      <td>-0.585545</td>\n",
       "      <td>0.094056</td>\n",
       "      <td>-0.009743</td>\n",
       "      <td>-0.282590</td>\n",
       "      <td>-0.180979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.631279</td>\n",
       "      <td>0.233796</td>\n",
       "      <td>-0.529192</td>\n",
       "      <td>0.210167</td>\n",
       "      <td>-0.164917</td>\n",
       "      <td>-0.220739</td>\n",
       "      <td>-0.239534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-1.285214</td>\n",
       "      <td>0.505108</td>\n",
       "      <td>-0.636009</td>\n",
       "      <td>0.231359</td>\n",
       "      <td>-0.478382</td>\n",
       "      <td>-0.184643</td>\n",
       "      <td>-0.142935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.669484</td>\n",
       "      <td>0.650234</td>\n",
       "      <td>-0.609780</td>\n",
       "      <td>0.176433</td>\n",
       "      <td>-0.564361</td>\n",
       "      <td>-0.038889</td>\n",
       "      <td>-0.258332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.522125</td>\n",
       "      <td>0.288002</td>\n",
       "      <td>-0.299466</td>\n",
       "      <td>0.324177</td>\n",
       "      <td>-0.564678</td>\n",
       "      <td>0.055326</td>\n",
       "      <td>-0.282808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.970273</td>\n",
       "      <td>0.491910</td>\n",
       "      <td>-0.303798</td>\n",
       "      <td>-0.561061</td>\n",
       "      <td>-0.444672</td>\n",
       "      <td>0.030107</td>\n",
       "      <td>-0.222534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.987838</td>\n",
       "      <td>0.186538</td>\n",
       "      <td>0.233796</td>\n",
       "      <td>-0.872261</td>\n",
       "      <td>-0.147698</td>\n",
       "      <td>0.011304</td>\n",
       "      <td>-0.121750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.559581</td>\n",
       "      <td>-0.042389</td>\n",
       "      <td>0.505108</td>\n",
       "      <td>-1.090434</td>\n",
       "      <td>-0.065451</td>\n",
       "      <td>-0.087203</td>\n",
       "      <td>0.025099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-2.480647</td>\n",
       "      <td>-0.297368</td>\n",
       "      <td>0.650234</td>\n",
       "      <td>-1.018083</td>\n",
       "      <td>0.020227</td>\n",
       "      <td>-0.043080</td>\n",
       "      <td>-0.027843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.541667</td>\n",
       "      <td>0.360352</td>\n",
       "      <td>0.288002</td>\n",
       "      <td>-0.751459</td>\n",
       "      <td>-0.005732</td>\n",
       "      <td>-0.000735</td>\n",
       "      <td>-0.124480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.443412</td>\n",
       "      <td>-1.614032</td>\n",
       "      <td>0.491910</td>\n",
       "      <td>-0.761904</td>\n",
       "      <td>0.094056</td>\n",
       "      <td>-0.129589</td>\n",
       "      <td>-0.014722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.843204</td>\n",
       "      <td>-1.931059</td>\n",
       "      <td>0.186538</td>\n",
       "      <td>-0.731835</td>\n",
       "      <td>0.210167</td>\n",
       "      <td>-0.203895</td>\n",
       "      <td>0.061264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>-1.834475</td>\n",
       "      <td>-2.138478</td>\n",
       "      <td>-0.042389</td>\n",
       "      <td>-0.758040</td>\n",
       "      <td>0.231359</td>\n",
       "      <td>-0.223241</td>\n",
       "      <td>0.061700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.807953</td>\n",
       "      <td>-1.738799</td>\n",
       "      <td>-0.297368</td>\n",
       "      <td>-0.435662</td>\n",
       "      <td>0.176433</td>\n",
       "      <td>-0.185698</td>\n",
       "      <td>-0.204986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-8.330252</td>\n",
       "      <td>-1.863270</td>\n",
       "      <td>0.360352</td>\n",
       "      <td>-0.338895</td>\n",
       "      <td>0.324177</td>\n",
       "      <td>-0.267330</td>\n",
       "      <td>-0.146254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>-1.141726</td>\n",
       "      <td>0.090223</td>\n",
       "      <td>-1.614032</td>\n",
       "      <td>0.625033</td>\n",
       "      <td>-0.561061</td>\n",
       "      <td>0.223404</td>\n",
       "      <td>-0.511203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.193892</td>\n",
       "      <td>0.467389</td>\n",
       "      <td>-1.931059</td>\n",
       "      <td>0.738458</td>\n",
       "      <td>-0.872261</td>\n",
       "      <td>0.264849</td>\n",
       "      <td>-0.533288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.163924</td>\n",
       "      <td>0.622399</td>\n",
       "      <td>-2.138478</td>\n",
       "      <td>0.915389</td>\n",
       "      <td>-1.090434</td>\n",
       "      <td>0.287816</td>\n",
       "      <td>-0.494928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.185598</td>\n",
       "      <td>0.867474</td>\n",
       "      <td>-1.738799</td>\n",
       "      <td>0.890915</td>\n",
       "      <td>-1.018083</td>\n",
       "      <td>0.311307</td>\n",
       "      <td>-0.542615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1.437213</td>\n",
       "      <td>1.185479</td>\n",
       "      <td>-1.863270</td>\n",
       "      <td>0.909055</td>\n",
       "      <td>-0.751459</td>\n",
       "      <td>0.312498</td>\n",
       "      <td>-0.466332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.744101</td>\n",
       "      <td>1.159842</td>\n",
       "      <td>0.090223</td>\n",
       "      <td>0.606195</td>\n",
       "      <td>-0.761904</td>\n",
       "      <td>0.229636</td>\n",
       "      <td>-0.292128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     r_mean1   r_mean5  hist_rmean5  r_mean10  hist_rmean10  r_mean21  \\\n",
       "43 -0.717699 -0.303798    -0.585545  0.094056     -0.009743 -0.282590   \n",
       "44  0.631279  0.233796    -0.529192  0.210167     -0.164917 -0.220739   \n",
       "45 -1.285214  0.505108    -0.636009  0.231359     -0.478382 -0.184643   \n",
       "46 -0.669484  0.650234    -0.609780  0.176433     -0.564361 -0.038889   \n",
       "47  0.522125  0.288002    -0.299466  0.324177     -0.564678  0.055326   \n",
       "48  1.970273  0.491910    -0.303798 -0.561061     -0.444672  0.030107   \n",
       "49  1.987838  0.186538     0.233796 -0.872261     -0.147698  0.011304   \n",
       "50 -0.559581 -0.042389     0.505108 -1.090434     -0.065451 -0.087203   \n",
       "51 -2.480647 -0.297368     0.650234 -1.018083      0.020227 -0.043080   \n",
       "52  1.541667  0.360352     0.288002 -0.751459     -0.005732 -0.000735   \n",
       "53  0.443412 -1.614032     0.491910 -0.761904      0.094056 -0.129589   \n",
       "54  0.843204 -1.931059     0.186538 -0.731835      0.210167 -0.203895   \n",
       "55 -1.834475 -2.138478    -0.042389 -0.758040      0.231359 -0.223241   \n",
       "56  0.807953 -1.738799    -0.297368 -0.435662      0.176433 -0.185698   \n",
       "57 -8.330252 -1.863270     0.360352 -0.338895      0.324177 -0.267330   \n",
       "58 -1.141726  0.090223    -1.614032  0.625033     -0.561061  0.223404   \n",
       "59 -0.193892  0.467389    -1.931059  0.738458     -0.872261  0.264849   \n",
       "60  0.163924  0.622399    -2.138478  0.915389     -1.090434  0.287816   \n",
       "61  0.185598  0.867474    -1.738799  0.890915     -1.018083  0.311307   \n",
       "62  1.437213  1.185479    -1.863270  0.909055     -0.751459  0.312498   \n",
       "63  0.744101  1.159842     0.090223  0.606195     -0.761904  0.229636   \n",
       "\n",
       "    hist_rmean21  \n",
       "43     -0.180979  \n",
       "44     -0.239534  \n",
       "45     -0.142935  \n",
       "46     -0.258332  \n",
       "47     -0.282808  \n",
       "48     -0.222534  \n",
       "49     -0.121750  \n",
       "50      0.025099  \n",
       "51     -0.027843  \n",
       "52     -0.124480  \n",
       "53     -0.014722  \n",
       "54      0.061264  \n",
       "55      0.061700  \n",
       "56     -0.204986  \n",
       "57     -0.146254  \n",
       "58     -0.511203  \n",
       "59     -0.533288  \n",
       "60     -0.494928  \n",
       "61     -0.542615  \n",
       "62     -0.466332  \n",
       "63     -0.292128  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day = \"2014-02-04\"\n",
    "ticker_target_df = target_df\n",
    "estimation_end = ticker_target_df.index[ticker_target_df['Date'] == day].item()+1   # datafrme slice need the ending index+1 so that the record of that day be included\n",
    "estimation_start = estimation_end - 21\n",
    "ticker_target_df_of_day = ticker_target_df.iloc[estimation_start:estimation_end] \n",
    "ticker_day_Y = ticker_target_df_of_day[['r_mean1','r_mean5',\"hist_rmean5\", 'r_mean10',\"hist_rmean10\", 'r_mean21',\"hist_rmean21\"]]\n",
    "y = ticker_day_Y.iloc[-1]\n",
    "print(f\"y:{y}\")\n",
    "ticker_day_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7441014247130084 1.4372129459933163\n",
      "1.1598416863190268 0.0902233889610136\n",
      "0.6061948824600764 -0.7619041205903253\n",
      "0.22963645756913345 -0.2825896169284437\n",
      "r_mean1         0.000000\n",
      "r_mean5         1.000000\n",
      "hist_rmean5     0.090223\n",
      "r_mean10        1.000000\n",
      "hist_rmean10   -0.761904\n",
      "r_mean21        1.000000\n",
      "hist_rmean21   -0.292128\n",
      "Name: 63, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "iter = [(\"r_mean1\", 1), (\"r_mean5\", 5), (\"r_mean10\", 10), (\"r_mean21\", 20)]\n",
    "for col, lagging in iter:\n",
    "    print(ticker_day_Y[col].iloc[-1], ticker_day_Y[col].iloc[-1-lagging])\n",
    "    y[col] = 1 if ticker_day_Y[col].iloc[-1] > ticker_day_Y[col].iloc[-1-lagging] else 0\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_p['log return'][1:6].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_p['r_mean5'] = aapl_p['log return'].shift(-5).rolling(window=5).mean()\n",
    "aapl_p['std5'] = aapl_p['log return'].shift(-5).rolling(window=5).std()\n",
    "start = aapl_p[aapl_p['Date']==PERIOD_START].index[0]\n",
    "end = aapl_p[aapl_p['Date']==PERIOD_END].index[0]\n",
    "aapl_p.iloc[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the prediction target \n",
    "\n",
    "* mean of return\n",
    "* mean of volatility\n",
    "\n",
    "Different from most stock trend/movement prediction tasks which is usually predicting next day movement (positive/negative), we are interested in a long-term average of the return and volatility. Let R(t) represent the return on day t, we want to learn a mapping function $F_1()$ and $F_2()$ that \n",
    "\n",
    "$$F_1([R(t-n, R(t)], N(t))) = mean([R(t+1),R(t+m)])$$\n",
    "\n",
    "$$F_2([R(t-n, R(t)], N(t))) = std([R(t+1),R(t+m)])$$\n",
    "\n",
    "where $n$ is the estimation window length, and $m$ is the step length. $N(t)$ represent the textual data on day $t$, usually tweets or news (depends on the dataset). Following the experiment setting in [1], they have $n = 100$ on small dataset (1-year) and $n = 250$ for large dataset (2-years). And they set $m = [1, 5, 30]$ which represents 1-day, 5-days, and 30-days ahead prediction respectively. Paper [2] also sets the step length to be 1-day, 1-week and 1-month. However, since the transcation data they used is Bitcoin which is tradable 24/7, in their case 1-week = 7-days not 5 trading days, and similarly 1-month = 30 days not 22 trading days. \n",
    "\n",
    "In our experiment setting, we try $n = [20,40, 100, 250]$, and $m = [ 5, 10, 20]$, which represents 1-week, 2-weeks, and 1-month, respectively. We omit m=1-day because we cannot calculate the realized std of 1-day based on daily return data.\n",
    "\n",
    "Since the text data (tweet) in ACL2018 dataset is available from 2014/01/01 - 2015/12/31, $t \\in [2014/01/01 - 2015/12/31]$. And we need $n$ day ahead historical data as well.\n",
    "\n",
    "[1] Wu, K., Karmakar, S. A model-free approach to do long-term volatility forecasting and its variants. Financ Innov 9, 59 (2023). https://doi.org/10.1186/s40854-023-00466-6\n",
    "\n",
    "[2] Shen, D., Urquhart, A. and Wang, P., 2020. Forecasting the volatility of Bitcoin: The importance of jumps and structural breaks. European Financial Management, 26(5), pp.1294-1323."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_target_for_one_stock(csv_f, estimation_window=100, dataset='acl2018'):\n",
    "    \"\"\"\n",
    "    Input: a price dataframe of one stock, estimation window length, dataset name\n",
    "    Output: a dataframe with the prediction targets as columns\n",
    "    \"\"\"\n",
    "    steps = [5,10,20]\n",
    "    if dataset=='acl2018':\n",
    "        # log return = 100 * r(t+1)/r(t)\n",
    "        csv_f['log return'] = 100 * np.log(csv_f['Adj Close']/csv_f['Adj Close'].shift(1))\n",
    "        begin_idx = csv_f[csv_f['Date']==PERIOD_START].index[0]\n",
    "        end_idx = csv_f[csv_f['Date']==PERIOD_END].index[0]\n",
    "        # for each step length, calculate the r_mean and std of log returns\n",
    "        for m in steps:\n",
    "            csv_f['r_mean'+str(m)] = csv_f['log return'].shift(-m).rolling(window=m).mean()\n",
    "            csv_f['std'+str(m)] = csv_f['log return'].shift(-m).rolling(window=m).std()\n",
    "        if begin_idx < estimation_window:\n",
    "            raise ValueError(f'Historical data length not enough. There are {begin_idx} days ahead of 2014-01-02!')\n",
    "    return csv_f.iloc[begin_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_target_for_one_stock(aapl_p, estimation_window=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(df['r_mean20'], label='20')\n",
    "plt.plot(df['r_mean10'], label='10')\n",
    "plt.plot(df['r_mean5'], label='5')\n",
    "plt.title('Log Return Mean in 5/10/20 steps')\n",
    "plt.xlabel('Trading Day')\n",
    "plt.ylabel('Log Return Mean')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(df['std20'], label='20')\n",
    "plt.plot(df['std10'], label='10')\n",
    "plt.plot(df['std5'], label='5')\n",
    "plt.title('Log Return std in 5/10/20 steps')\n",
    "plt.xlabel('Trading Day')\n",
    "plt.ylabel('Log Return Mean')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Edge based on Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_day_tweet(tweet_fp):\n",
    "    \"\"\"\n",
    "    Read the tweets file of a firm on a single day.\n",
    "    Input: tweet file path\n",
    "    Output: return a list of dicts of tweets on this day\n",
    "    \"\"\"\n",
    "    with open(tweet_fp,'r') as f:\n",
    "        tweets = []\n",
    "        for line in f:\n",
    "            line_dict = json.loads(line)\n",
    "            tweets.append(line_dict)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_2014_1_1_tweet = read_day_tweet(tweet_fp=acl18_tweet_preprocessed+'AAPL/2014-01-01')\n",
    "aapl_2014_1_1_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in aapl_2014_1_1_tweet:\n",
    "    tweet_arr = np.array(tweet['text'])\n",
    "    # find the index of \"$\" in the tweet\n",
    "    idx = tweet_arr=='$'\n",
    "    # find the index of any word following \"$\", which is usually the ticker of a firm\n",
    "    idx = np.array(pd.Series(idx).shift(1)).astype('bool')\n",
    "    idx[0] = False\n",
    "    print('Tweet #',tweet['user_id_str'], tweet_arr[idx])\n",
    "    for firm in tweet_arr[idx]:\n",
    "        if firm.upper() in VALID_NODE_LIST:\n",
    "            print(firm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Graph Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import yfinance\n",
    "\n",
    "raw_data = yfinance.download (tickers = \"^GSPC\", start = \"2022-05-31\", \n",
    "                              end = \"2024-05-21\", interval = \"1d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-05-31</th>\n",
       "      <td>4151.089844</td>\n",
       "      <td>4168.339844</td>\n",
       "      <td>4104.879883</td>\n",
       "      <td>4132.149902</td>\n",
       "      <td>4132.149902</td>\n",
       "      <td>6822640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-01</th>\n",
       "      <td>4149.779785</td>\n",
       "      <td>4166.540039</td>\n",
       "      <td>4073.850098</td>\n",
       "      <td>4101.229980</td>\n",
       "      <td>4101.229980</td>\n",
       "      <td>4531800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-02</th>\n",
       "      <td>4095.409912</td>\n",
       "      <td>4177.509766</td>\n",
       "      <td>4074.370117</td>\n",
       "      <td>4176.819824</td>\n",
       "      <td>4176.819824</td>\n",
       "      <td>4405790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-03</th>\n",
       "      <td>4137.569824</td>\n",
       "      <td>4142.669922</td>\n",
       "      <td>4098.669922</td>\n",
       "      <td>4108.540039</td>\n",
       "      <td>4108.540039</td>\n",
       "      <td>3711110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-06</th>\n",
       "      <td>4134.720215</td>\n",
       "      <td>4168.779785</td>\n",
       "      <td>4109.180176</td>\n",
       "      <td>4121.430176</td>\n",
       "      <td>4121.430176</td>\n",
       "      <td>4332700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-14</th>\n",
       "      <td>5221.100098</td>\n",
       "      <td>5250.370117</td>\n",
       "      <td>5217.979980</td>\n",
       "      <td>5246.680176</td>\n",
       "      <td>5246.680176</td>\n",
       "      <td>4763580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-15</th>\n",
       "      <td>5263.259766</td>\n",
       "      <td>5311.759766</td>\n",
       "      <td>5263.259766</td>\n",
       "      <td>5308.149902</td>\n",
       "      <td>5308.149902</td>\n",
       "      <td>4360810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-16</th>\n",
       "      <td>5310.069824</td>\n",
       "      <td>5325.490234</td>\n",
       "      <td>5296.189941</td>\n",
       "      <td>5297.100098</td>\n",
       "      <td>5297.100098</td>\n",
       "      <td>3817470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-17</th>\n",
       "      <td>5303.100098</td>\n",
       "      <td>5305.450195</td>\n",
       "      <td>5283.589844</td>\n",
       "      <td>5303.270020</td>\n",
       "      <td>5303.270020</td>\n",
       "      <td>3578120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-20</th>\n",
       "      <td>5305.350098</td>\n",
       "      <td>5325.319824</td>\n",
       "      <td>5302.399902</td>\n",
       "      <td>5308.129883</td>\n",
       "      <td>5308.129883</td>\n",
       "      <td>3420100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>496 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Open         High          Low        Close    Adj Close  \\\n",
       "Date                                                                          \n",
       "2022-05-31  4151.089844  4168.339844  4104.879883  4132.149902  4132.149902   \n",
       "2022-06-01  4149.779785  4166.540039  4073.850098  4101.229980  4101.229980   \n",
       "2022-06-02  4095.409912  4177.509766  4074.370117  4176.819824  4176.819824   \n",
       "2022-06-03  4137.569824  4142.669922  4098.669922  4108.540039  4108.540039   \n",
       "2022-06-06  4134.720215  4168.779785  4109.180176  4121.430176  4121.430176   \n",
       "...                 ...          ...          ...          ...          ...   \n",
       "2024-05-14  5221.100098  5250.370117  5217.979980  5246.680176  5246.680176   \n",
       "2024-05-15  5263.259766  5311.759766  5263.259766  5308.149902  5308.149902   \n",
       "2024-05-16  5310.069824  5325.490234  5296.189941  5297.100098  5297.100098   \n",
       "2024-05-17  5303.100098  5305.450195  5283.589844  5303.270020  5303.270020   \n",
       "2024-05-20  5305.350098  5325.319824  5302.399902  5308.129883  5308.129883   \n",
       "\n",
       "                Volume  \n",
       "Date                    \n",
       "2022-05-31  6822640000  \n",
       "2022-06-01  4531800000  \n",
       "2022-06-02  4405790000  \n",
       "2022-06-03  3711110000  \n",
       "2022-06-06  4332700000  \n",
       "...                ...  \n",
       "2024-05-14  4763580000  \n",
       "2024-05-15  4360810000  \n",
       "2024-05-16  3817470000  \n",
       "2024-05-17  3578120000  \n",
       "2024-05-20  3420100000  \n",
       "\n",
       "[496 rows x 6 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator for ACL2018 is initialized ...\n",
      "Total Firms:  87 , Valid Firms:  76 , Incomplete Record Firms:  76\n",
      "504 trading days.  698 natual days.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "100%|██████████| 504/504 [00:38<00:00, 13.10it/s]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = FirmRelationGraph(\n",
    "    dynamic=\"cooccur\",\n",
    "    num_node_features=8,\n",
    "    matrix_node_features=True,\n",
    "    node_normalize='mean',\n",
    "    num_Y_features=4,\n",
    "    Y_type=\"mean\",\n",
    "    classification=True,\n",
    "    graph_level_task=False,\n",
    "    estimation_window=21,\n",
    "    num_edge_features=1,\n",
    "    weighted_edge=True,\n",
    "    threshold=1,\n",
    "    root = ROOT_FOLDER+\"GNN_longterm/data/graph_sets\",\n",
    "    name=\"ACL2018\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[76, 21, 8], edge_index=[2, 11], edge_attr=[11, 1], y=[76, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[39, 31,  1, 14, 20, 27, 31, 39, 58, 27, 20],\n",
       "         [31, 39,  1, 14, 20, 27, 31, 39, 58, 20, 27]]),\n",
       " tensor([[1.0476],\n",
       "         [1.0476],\n",
       "         [1.4762],\n",
       "         [1.0952],\n",
       "         [1.9524],\n",
       "         [1.8095],\n",
       "         [1.3810],\n",
       "         [1.6190],\n",
       "         [1.0952],\n",
       "         [1.4286],\n",
       "         [1.4286]], dtype=torch.float64))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[3].edge_index, dataset[3].edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CELG'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.graphgenerator.index_to_ticker(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(x=[76, 21, 8], edge_index=[2, 92], edge_attr=[92, 1], y=[76, 4]),\n",
       " 504,\n",
       " 454,\n",
       " 50)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_len, test_len = int(len(dataset)/10), int(len(dataset)/10)\n",
    "train_len = len(dataset)-dev_len-test_len\n",
    "dataset[0], len(dataset), train_len+dev_len, test_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1.],\n",
       "       [0., 1., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_period = dataset.graphgenerator.last_period\n",
    "keys = list(last_period.keys())\n",
    "last_period[keys[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>log return</th>\n",
       "      <th>r_mean1</th>\n",
       "      <th>hist_rmean1</th>\n",
       "      <th>r_mean5</th>\n",
       "      <th>hist_rmean5</th>\n",
       "      <th>r_mean10</th>\n",
       "      <th>hist_rmean10</th>\n",
       "      <th>r_mean21</th>\n",
       "      <th>hist_rmean21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3855.929932</td>\n",
       "      <td>-1.133611</td>\n",
       "      <td>-1.726467</td>\n",
       "      <td>-1.133611</td>\n",
       "      <td>-1.112555</td>\n",
       "      <td>-0.394229</td>\n",
       "      <td>-0.170009</td>\n",
       "      <td>-0.134621</td>\n",
       "      <td>-0.202802</td>\n",
       "      <td>-0.439192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3789.929932</td>\n",
       "      <td>-1.726467</td>\n",
       "      <td>-0.846329</td>\n",
       "      <td>-1.726467</td>\n",
       "      <td>-0.377639</td>\n",
       "      <td>-0.807148</td>\n",
       "      <td>-0.017562</td>\n",
       "      <td>-0.489017</td>\n",
       "      <td>-0.158602</td>\n",
       "      <td>-0.418394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3757.989990</td>\n",
       "      <td>-0.846329</td>\n",
       "      <td>-1.738283</td>\n",
       "      <td>-0.846329</td>\n",
       "      <td>-0.635429</td>\n",
       "      <td>-0.748769</td>\n",
       "      <td>-0.035908</td>\n",
       "      <td>-0.639539</td>\n",
       "      <td>-0.006644</td>\n",
       "      <td>-0.448027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3693.229980</td>\n",
       "      <td>-1.738283</td>\n",
       "      <td>-1.039436</td>\n",
       "      <td>-1.738283</td>\n",
       "      <td>-0.591399</td>\n",
       "      <td>-0.952264</td>\n",
       "      <td>-0.146112</td>\n",
       "      <td>-0.964928</td>\n",
       "      <td>0.132378</td>\n",
       "      <td>-0.544668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3655.040039</td>\n",
       "      <td>-1.039436</td>\n",
       "      <td>-0.212261</td>\n",
       "      <td>-1.039436</td>\n",
       "      <td>0.127579</td>\n",
       "      <td>-1.296825</td>\n",
       "      <td>-0.117375</td>\n",
       "      <td>-1.174158</td>\n",
       "      <td>0.258712</td>\n",
       "      <td>-0.660800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>5243.770020</td>\n",
       "      <td>-0.201561</td>\n",
       "      <td>-0.726539</td>\n",
       "      <td>-0.201561</td>\n",
       "      <td>-0.158451</td>\n",
       "      <td>0.036610</td>\n",
       "      <td>-0.353146</td>\n",
       "      <td>0.244548</td>\n",
       "      <td>-0.192810</td>\n",
       "      <td>0.135866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>5205.810059</td>\n",
       "      <td>-0.726539</td>\n",
       "      <td>0.109053</td>\n",
       "      <td>-0.726539</td>\n",
       "      <td>0.015746</td>\n",
       "      <td>-0.047505</td>\n",
       "      <td>-0.301079</td>\n",
       "      <td>0.108912</td>\n",
       "      <td>-0.174601</td>\n",
       "      <td>0.063288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>5211.490234</td>\n",
       "      <td>0.109053</td>\n",
       "      <td>-1.241104</td>\n",
       "      <td>0.109053</td>\n",
       "      <td>-0.196104</td>\n",
       "      <td>0.030380</td>\n",
       "      <td>-0.369958</td>\n",
       "      <td>0.063485</td>\n",
       "      <td>-0.136522</td>\n",
       "      <td>0.074167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>5147.209961</td>\n",
       "      <td>-1.241104</td>\n",
       "      <td>1.103805</td>\n",
       "      <td>-1.241104</td>\n",
       "      <td>0.200461</td>\n",
       "      <td>-0.389713</td>\n",
       "      <td>-0.267953</td>\n",
       "      <td>-0.149273</td>\n",
       "      <td>-0.018000</td>\n",
       "      <td>0.063854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>5204.339844</td>\n",
       "      <td>1.103805</td>\n",
       "      <td>-0.037470</td>\n",
       "      <td>1.103805</td>\n",
       "      <td>-0.313452</td>\n",
       "      <td>-0.191269</td>\n",
       "      <td>-0.466305</td>\n",
       "      <td>-0.071205</td>\n",
       "      <td>-0.021642</td>\n",
       "      <td>0.091997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>388 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Adj Close  log return   r_mean1  hist_rmean1   r_mean5  hist_rmean5  \\\n",
       "41   3855.929932   -1.133611 -1.726467    -1.133611 -1.112555    -0.394229   \n",
       "42   3789.929932   -1.726467 -0.846329    -1.726467 -0.377639    -0.807148   \n",
       "43   3757.989990   -0.846329 -1.738283    -0.846329 -0.635429    -0.748769   \n",
       "44   3693.229980   -1.738283 -1.039436    -1.738283 -0.591399    -0.952264   \n",
       "45   3655.040039   -1.039436 -0.212261    -1.039436  0.127579    -1.296825   \n",
       "..           ...         ...       ...          ...       ...          ...   \n",
       "424  5243.770020   -0.201561 -0.726539    -0.201561 -0.158451     0.036610   \n",
       "425  5205.810059   -0.726539  0.109053    -0.726539  0.015746    -0.047505   \n",
       "426  5211.490234    0.109053 -1.241104     0.109053 -0.196104     0.030380   \n",
       "427  5147.209961   -1.241104  1.103805    -1.241104  0.200461    -0.389713   \n",
       "428  5204.339844    1.103805 -0.037470     1.103805 -0.313452    -0.191269   \n",
       "\n",
       "     r_mean10  hist_rmean10  r_mean21  hist_rmean21  \n",
       "41  -0.170009     -0.134621 -0.202802     -0.439192  \n",
       "42  -0.017562     -0.489017 -0.158602     -0.418394  \n",
       "43  -0.035908     -0.639539 -0.006644     -0.448027  \n",
       "44  -0.146112     -0.964928  0.132378     -0.544668  \n",
       "45  -0.117375     -1.174158  0.258712     -0.660800  \n",
       "..        ...           ...       ...           ...  \n",
       "424 -0.353146      0.244548 -0.192810      0.135866  \n",
       "425 -0.301079      0.108912 -0.174601      0.063288  \n",
       "426 -0.369958      0.063485 -0.136522      0.074167  \n",
       "427 -0.267953     -0.149273 -0.018000      0.063854  \n",
       "428 -0.466305     -0.071205 -0.021642      0.091997  \n",
       "\n",
       "[388 rows x 10 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.graphgenerator.node_generator.graph_target_df.iloc[41:][[\"Adj Close\", \"log return\", 'r_mean1','hist_rmean1', 'r_mean5','hist_rmean5', 'r_mean10', 'hist_rmean10', 'r_mean21','hist_rmean21']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([0.6, 0.4])\n",
    "torch.where(input < 0.5, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2015-10-21',\n",
       " '2015-10-22',\n",
       " '2015-10-23',\n",
       " '2015-10-26',\n",
       " '2015-10-27',\n",
       " '2015-10-28',\n",
       " '2015-10-29',\n",
       " '2015-10-30',\n",
       " '2015-11-02',\n",
       " '2015-11-03',\n",
       " '2015-11-04',\n",
       " '2015-11-05',\n",
       " '2015-11-06',\n",
       " '2015-11-09',\n",
       " '2015-11-10',\n",
       " '2015-11-11',\n",
       " '2015-11-12',\n",
       " '2015-11-13',\n",
       " '2015-11-16',\n",
       " '2015-11-17',\n",
       " '2015-11-18',\n",
       " '2015-11-19',\n",
       " '2015-11-20',\n",
       " '2015-11-23',\n",
       " '2015-11-24',\n",
       " '2015-11-25',\n",
       " '2015-11-27',\n",
       " '2015-11-30',\n",
       " '2015-12-01',\n",
       " '2015-12-02',\n",
       " '2015-12-03',\n",
       " '2015-12-04',\n",
       " '2015-12-07',\n",
       " '2015-12-08',\n",
       " '2015-12-09',\n",
       " '2015-12-10',\n",
       " '2015-12-11',\n",
       " '2015-12-14',\n",
       " '2015-12-15',\n",
       " '2015-12-16',\n",
       " '2015-12-17',\n",
       " '2015-12-18',\n",
       " '2015-12-21',\n",
       " '2015-12-22',\n",
       " '2015-12-23',\n",
       " '2015-12-24',\n",
       " '2015-12-28',\n",
       " '2015-12-29',\n",
       " '2015-12-30',\n",
       " '2015-12-31']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.graphgenerator.node_generator.valid_trading_days[train_len+dev_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    if torch.nan in dataset[i].y:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False,  True])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(5)\n",
    "a.le(0.5) == torch.tensor([0,0,0,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 1, 1, 0, 1]), tensor([0.4305, 0.1026, 0.2154, 0.7808, 0.4627]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.le(0.5).long(), a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4223],\n",
      "        [0.5481],\n",
      "        [0.5004],\n",
      "        [0.4600],\n",
      "        [0.6194]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4252],\n",
      "        [0.4103],\n",
      "        [0.4151],\n",
      "        [0.4354],\n",
      "        [0.4522]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4449],\n",
      "        [0.5046],\n",
      "        [0.5180],\n",
      "        [0.4886],\n",
      "        [0.4399]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4776],\n",
      "        [0.4386],\n",
      "        [0.4270],\n",
      "        [0.4754],\n",
      "        [0.5631]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4590],\n",
      "        [0.4294],\n",
      "        [0.4956],\n",
      "        [0.5178],\n",
      "        [0.5678]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4484],\n",
      "        [0.4809],\n",
      "        [0.5111],\n",
      "        [0.4316],\n",
      "        [0.4367]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4631],\n",
      "        [0.4068],\n",
      "        [0.4656],\n",
      "        [0.4339],\n",
      "        [0.4792]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4987],\n",
      "        [0.4373],\n",
      "        [0.4516],\n",
      "        [0.4589],\n",
      "        [0.4297]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4766],\n",
      "        [0.4291],\n",
      "        [0.5343],\n",
      "        [0.4754],\n",
      "        [0.4374]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4324],\n",
      "        [0.4311],\n",
      "        [0.5290],\n",
      "        [0.3820],\n",
      "        [0.4938]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5181],\n",
      "        [0.4158],\n",
      "        [0.5366],\n",
      "        [0.5136],\n",
      "        [0.4374]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4980],\n",
      "        [0.4400],\n",
      "        [0.4798],\n",
      "        [0.4797],\n",
      "        [0.3897]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4633],\n",
      "        [0.4209],\n",
      "        [0.4243],\n",
      "        [0.4289],\n",
      "        [0.4234]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4659],\n",
      "        [0.4536],\n",
      "        [0.4272],\n",
      "        [0.4386],\n",
      "        [0.4076]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4529],\n",
      "        [0.4074],\n",
      "        [0.5193],\n",
      "        [0.4327],\n",
      "        [0.6043]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4266],\n",
      "        [0.4379],\n",
      "        [0.4802],\n",
      "        [0.4463],\n",
      "        [0.4005]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.3868],\n",
      "        [0.4309],\n",
      "        [0.4728],\n",
      "        [0.4092],\n",
      "        [0.4201]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4800],\n",
      "        [0.4378],\n",
      "        [0.4240],\n",
      "        [0.4769],\n",
      "        [0.4502]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4811],\n",
      "        [0.4960],\n",
      "        [0.3941],\n",
      "        [0.3815],\n",
      "        [0.4822]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5207],\n",
      "        [0.4230],\n",
      "        [0.5868],\n",
      "        [0.5314],\n",
      "        [0.4234]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5504],\n",
      "        [0.4090],\n",
      "        [0.5132],\n",
      "        [0.5896],\n",
      "        [0.4144]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4961],\n",
      "        [0.5099],\n",
      "        [0.4773],\n",
      "        [0.5061],\n",
      "        [0.4643]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4705],\n",
      "        [0.4353],\n",
      "        [0.5116],\n",
      "        [0.4547],\n",
      "        [0.4886]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4591],\n",
      "        [0.3984],\n",
      "        [0.4855],\n",
      "        [0.4490],\n",
      "        [0.4620]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.3973],\n",
      "        [0.4454],\n",
      "        [0.4444],\n",
      "        [0.5497],\n",
      "        [0.4540]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4856],\n",
      "        [0.4683],\n",
      "        [0.4033],\n",
      "        [0.5752],\n",
      "        [0.4424]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4555],\n",
      "        [0.4842],\n",
      "        [0.4506],\n",
      "        [0.5530],\n",
      "        [0.4326]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4835],\n",
      "        [0.5239],\n",
      "        [0.5294],\n",
      "        [0.4500],\n",
      "        [0.4616]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4759],\n",
      "        [0.4304],\n",
      "        [0.4393],\n",
      "        [0.4524],\n",
      "        [0.4365]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4596],\n",
      "        [0.4659],\n",
      "        [0.4842],\n",
      "        [0.4762],\n",
      "        [0.4471]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4391],\n",
      "        [0.5663],\n",
      "        [0.5120],\n",
      "        [0.4581],\n",
      "        [0.4454]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 1.])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "tensor([[0.4432],\n",
      "        [0.4703],\n",
      "        [0.4444],\n",
      "        [0.4483],\n",
      "        [0.5112]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 0.])\n",
      "Epoch [1/20], Loss: 0.6723\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4436],\n",
      "        [0.3877],\n",
      "        [0.4768],\n",
      "        [0.4470],\n",
      "        [0.5455]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4212],\n",
      "        [0.4298],\n",
      "        [0.4463],\n",
      "        [0.5046],\n",
      "        [0.4179]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5405],\n",
      "        [0.4140],\n",
      "        [0.4050],\n",
      "        [0.4480],\n",
      "        [0.4917]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4896],\n",
      "        [0.4276],\n",
      "        [0.4023],\n",
      "        [0.5005],\n",
      "        [0.3997]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4562],\n",
      "        [0.4766],\n",
      "        [0.4433],\n",
      "        [0.4594],\n",
      "        [0.3749]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4499],\n",
      "        [0.5313],\n",
      "        [0.4600],\n",
      "        [0.5649],\n",
      "        [0.4239]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5134],\n",
      "        [0.4853],\n",
      "        [0.4679],\n",
      "        [0.4769],\n",
      "        [0.4304]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4494],\n",
      "        [0.4434],\n",
      "        [0.4859],\n",
      "        [0.4192],\n",
      "        [0.4732]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4991],\n",
      "        [0.4730],\n",
      "        [0.4889],\n",
      "        [0.4889],\n",
      "        [0.4862]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5183],\n",
      "        [0.4545],\n",
      "        [0.5146],\n",
      "        [0.4686],\n",
      "        [0.5194]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4655],\n",
      "        [0.4557],\n",
      "        [0.4473],\n",
      "        [0.5356],\n",
      "        [0.4545]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4501],\n",
      "        [0.5230],\n",
      "        [0.5087],\n",
      "        [0.4531],\n",
      "        [0.4489]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4772],\n",
      "        [0.4700],\n",
      "        [0.4967],\n",
      "        [0.4280],\n",
      "        [0.4545]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5111],\n",
      "        [0.5625],\n",
      "        [0.4636],\n",
      "        [0.4661],\n",
      "        [0.4868]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4652],\n",
      "        [0.5477],\n",
      "        [0.4300],\n",
      "        [0.5331],\n",
      "        [0.4489]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5508],\n",
      "        [0.5258],\n",
      "        [0.5861],\n",
      "        [0.4348],\n",
      "        [0.5670]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4322],\n",
      "        [0.4914],\n",
      "        [0.4994],\n",
      "        [0.5168],\n",
      "        [0.4888]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4677],\n",
      "        [0.5023],\n",
      "        [0.5157],\n",
      "        [0.4455],\n",
      "        [0.4602]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4407],\n",
      "        [0.4493],\n",
      "        [0.4648],\n",
      "        [0.4650],\n",
      "        [0.5385]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4140],\n",
      "        [0.5110],\n",
      "        [0.4155],\n",
      "        [0.4658],\n",
      "        [0.5381]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.3720],\n",
      "        [0.4329],\n",
      "        [0.4463],\n",
      "        [0.5596],\n",
      "        [0.5613]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5311],\n",
      "        [0.5003],\n",
      "        [0.4780],\n",
      "        [0.4551],\n",
      "        [0.4978]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4446],\n",
      "        [0.5115],\n",
      "        [0.4975],\n",
      "        [0.4172],\n",
      "        [0.4655]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5754],\n",
      "        [0.4249],\n",
      "        [0.4792],\n",
      "        [0.4467],\n",
      "        [0.5066]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4730],\n",
      "        [0.4812],\n",
      "        [0.4327],\n",
      "        [0.4124],\n",
      "        [0.4934]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5741],\n",
      "        [0.4710],\n",
      "        [0.4857],\n",
      "        [0.5681],\n",
      "        [0.5193]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5066],\n",
      "        [0.4341],\n",
      "        [0.4810],\n",
      "        [0.5836],\n",
      "        [0.4192]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5368],\n",
      "        [0.4695],\n",
      "        [0.5923],\n",
      "        [0.4518],\n",
      "        [0.4535]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4414],\n",
      "        [0.4454],\n",
      "        [0.4797],\n",
      "        [0.4487],\n",
      "        [0.4478]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4982],\n",
      "        [0.4645],\n",
      "        [0.4308],\n",
      "        [0.5597],\n",
      "        [0.4623]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5113],\n",
      "        [0.4531],\n",
      "        [0.5504],\n",
      "        [0.4637],\n",
      "        [0.4878]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 1.])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "tensor([[0.4241],\n",
      "        [0.4949],\n",
      "        [0.4348],\n",
      "        [0.4305],\n",
      "        [0.4270]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 1.])\n",
      "Epoch [2/20], Loss: 0.7469\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4817],\n",
      "        [0.3835],\n",
      "        [0.4417],\n",
      "        [0.4697],\n",
      "        [0.3865]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4812],\n",
      "        [0.5011],\n",
      "        [0.4404],\n",
      "        [0.4324],\n",
      "        [0.4564]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4822],\n",
      "        [0.4669],\n",
      "        [0.5332],\n",
      "        [0.4229],\n",
      "        [0.4661]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5277],\n",
      "        [0.5429],\n",
      "        [0.4883],\n",
      "        [0.4636],\n",
      "        [0.4995]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4845],\n",
      "        [0.4856],\n",
      "        [0.5102],\n",
      "        [0.4582],\n",
      "        [0.4585]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4534],\n",
      "        [0.5001],\n",
      "        [0.5221],\n",
      "        [0.4660],\n",
      "        [0.5358]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5085],\n",
      "        [0.4736],\n",
      "        [0.4985],\n",
      "        [0.4315],\n",
      "        [0.4063]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5144],\n",
      "        [0.4038],\n",
      "        [0.4900],\n",
      "        [0.4074],\n",
      "        [0.4408]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4910],\n",
      "        [0.4284],\n",
      "        [0.4759],\n",
      "        [0.4853],\n",
      "        [0.4653]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5832],\n",
      "        [0.4227],\n",
      "        [0.6044],\n",
      "        [0.4429],\n",
      "        [0.4630]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4962],\n",
      "        [0.4836],\n",
      "        [0.4354],\n",
      "        [0.4889],\n",
      "        [0.4906]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4789],\n",
      "        [0.5151],\n",
      "        [0.5251],\n",
      "        [0.4116],\n",
      "        [0.4968]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4905],\n",
      "        [0.4975],\n",
      "        [0.3996],\n",
      "        [0.4998],\n",
      "        [0.4989]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5182],\n",
      "        [0.5308],\n",
      "        [0.4647],\n",
      "        [0.4996],\n",
      "        [0.5302]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4750],\n",
      "        [0.4665],\n",
      "        [0.5238],\n",
      "        [0.5413],\n",
      "        [0.4289]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5576],\n",
      "        [0.5394],\n",
      "        [0.4885],\n",
      "        [0.4851],\n",
      "        [0.4919]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5034],\n",
      "        [0.5395],\n",
      "        [0.4507],\n",
      "        [0.5718],\n",
      "        [0.4821]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4535],\n",
      "        [0.4562],\n",
      "        [0.4672],\n",
      "        [0.4538],\n",
      "        [0.5303]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4388],\n",
      "        [0.4449],\n",
      "        [0.4397],\n",
      "        [0.5123],\n",
      "        [0.5014]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4967],\n",
      "        [0.4609],\n",
      "        [0.4317],\n",
      "        [0.4468],\n",
      "        [0.5108]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4746],\n",
      "        [0.4398],\n",
      "        [0.4571],\n",
      "        [0.4686],\n",
      "        [0.5655]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4537],\n",
      "        [0.4066],\n",
      "        [0.5035],\n",
      "        [0.5036],\n",
      "        [0.4285]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5750],\n",
      "        [0.4883],\n",
      "        [0.4265],\n",
      "        [0.4190],\n",
      "        [0.5004]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4562],\n",
      "        [0.4732],\n",
      "        [0.4299],\n",
      "        [0.5284],\n",
      "        [0.5666]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5112],\n",
      "        [0.4974],\n",
      "        [0.4594],\n",
      "        [0.4476],\n",
      "        [0.4770]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4497],\n",
      "        [0.4425],\n",
      "        [0.3780],\n",
      "        [0.4685],\n",
      "        [0.5264]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4888],\n",
      "        [0.4740],\n",
      "        [0.4473],\n",
      "        [0.4307],\n",
      "        [0.5091]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4462],\n",
      "        [0.4549],\n",
      "        [0.4342],\n",
      "        [0.5069],\n",
      "        [0.4625]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4321],\n",
      "        [0.4486],\n",
      "        [0.5008],\n",
      "        [0.5595],\n",
      "        [0.5260]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4424],\n",
      "        [0.4464],\n",
      "        [0.4678],\n",
      "        [0.5394],\n",
      "        [0.5801]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5182],\n",
      "        [0.5157],\n",
      "        [0.4844],\n",
      "        [0.5708],\n",
      "        [0.4621]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 1.])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "tensor([[0.4671],\n",
      "        [0.5323],\n",
      "        [0.4643],\n",
      "        [0.5189],\n",
      "        [0.4902]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 1.])\n",
      "Epoch [3/20], Loss: 0.6828\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4554],\n",
      "        [0.5970],\n",
      "        [0.4636],\n",
      "        [0.4956],\n",
      "        [0.4457]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4333],\n",
      "        [0.5529],\n",
      "        [0.4704],\n",
      "        [0.4192],\n",
      "        [0.4623]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4765],\n",
      "        [0.5145],\n",
      "        [0.5203],\n",
      "        [0.5798],\n",
      "        [0.4837]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4648],\n",
      "        [0.4656],\n",
      "        [0.4675],\n",
      "        [0.4928],\n",
      "        [0.6549]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.3840],\n",
      "        [0.4461],\n",
      "        [0.5829],\n",
      "        [0.4094],\n",
      "        [0.5363]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4591],\n",
      "        [0.5080],\n",
      "        [0.4332],\n",
      "        [0.6034],\n",
      "        [0.4534]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4727],\n",
      "        [0.4841],\n",
      "        [0.4577],\n",
      "        [0.4961],\n",
      "        [0.5075]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5180],\n",
      "        [0.4960],\n",
      "        [0.4754],\n",
      "        [0.4805],\n",
      "        [0.4593]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5151],\n",
      "        [0.4244],\n",
      "        [0.5134],\n",
      "        [0.4615],\n",
      "        [0.5244]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5559],\n",
      "        [0.4536],\n",
      "        [0.5260],\n",
      "        [0.4933],\n",
      "        [0.4407]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4621],\n",
      "        [0.4621],\n",
      "        [0.5012],\n",
      "        [0.5865],\n",
      "        [0.5062]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5046],\n",
      "        [0.5634],\n",
      "        [0.4347],\n",
      "        [0.4472],\n",
      "        [0.4411]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4442],\n",
      "        [0.4844],\n",
      "        [0.5594],\n",
      "        [0.4738],\n",
      "        [0.4861]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5107],\n",
      "        [0.4703],\n",
      "        [0.4927],\n",
      "        [0.4890],\n",
      "        [0.4747]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5174],\n",
      "        [0.5394],\n",
      "        [0.5154],\n",
      "        [0.5067],\n",
      "        [0.4967]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4911],\n",
      "        [0.4958],\n",
      "        [0.5392],\n",
      "        [0.4504],\n",
      "        [0.5085]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4813],\n",
      "        [0.5347],\n",
      "        [0.4884],\n",
      "        [0.5212],\n",
      "        [0.5185]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6065],\n",
      "        [0.4692],\n",
      "        [0.3823],\n",
      "        [0.4434],\n",
      "        [0.5642]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5109],\n",
      "        [0.4867],\n",
      "        [0.4786],\n",
      "        [0.4790],\n",
      "        [0.5232]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5052],\n",
      "        [0.4219],\n",
      "        [0.5901],\n",
      "        [0.5097],\n",
      "        [0.4511]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4667],\n",
      "        [0.4544],\n",
      "        [0.6064],\n",
      "        [0.4077],\n",
      "        [0.4684]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4511],\n",
      "        [0.5147],\n",
      "        [0.4776],\n",
      "        [0.4787],\n",
      "        [0.4993]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5934],\n",
      "        [0.4858],\n",
      "        [0.5045],\n",
      "        [0.4364],\n",
      "        [0.5289]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5041],\n",
      "        [0.4510],\n",
      "        [0.5229],\n",
      "        [0.4764],\n",
      "        [0.4818]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4370],\n",
      "        [0.5141],\n",
      "        [0.5806],\n",
      "        [0.4725],\n",
      "        [0.4493]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4471],\n",
      "        [0.4690],\n",
      "        [0.4680],\n",
      "        [0.4558],\n",
      "        [0.5324]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5462],\n",
      "        [0.5574],\n",
      "        [0.4379],\n",
      "        [0.3530],\n",
      "        [0.4659]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4781],\n",
      "        [0.5118],\n",
      "        [0.4678],\n",
      "        [0.5899],\n",
      "        [0.4881]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4794],\n",
      "        [0.4758],\n",
      "        [0.5120],\n",
      "        [0.5658],\n",
      "        [0.4889]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5730],\n",
      "        [0.5341],\n",
      "        [0.5533],\n",
      "        [0.4037],\n",
      "        [0.4643]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5717],\n",
      "        [0.4644],\n",
      "        [0.5173],\n",
      "        [0.5150],\n",
      "        [0.4739]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 1.])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "tensor([[0.4898],\n",
      "        [0.6153],\n",
      "        [0.5223],\n",
      "        [0.4547],\n",
      "        [0.4325]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 1.])\n",
      "Epoch [4/20], Loss: 0.6727\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4078],\n",
      "        [0.6402],\n",
      "        [0.4339],\n",
      "        [0.4683],\n",
      "        [0.5197]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4831],\n",
      "        [0.5204],\n",
      "        [0.5001],\n",
      "        [0.4381],\n",
      "        [0.4778]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4464],\n",
      "        [0.4362],\n",
      "        [0.4625],\n",
      "        [0.4465],\n",
      "        [0.5178]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5038],\n",
      "        [0.5280],\n",
      "        [0.4307],\n",
      "        [0.5823],\n",
      "        [0.4843]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5398],\n",
      "        [0.4946],\n",
      "        [0.4338],\n",
      "        [0.4674],\n",
      "        [0.5767]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5004],\n",
      "        [0.4347],\n",
      "        [0.5342],\n",
      "        [0.5051],\n",
      "        [0.4482]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4644],\n",
      "        [0.4631],\n",
      "        [0.5014],\n",
      "        [0.4272],\n",
      "        [0.5262]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5110],\n",
      "        [0.4878],\n",
      "        [0.4896],\n",
      "        [0.4898],\n",
      "        [0.4936]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5156],\n",
      "        [0.4481],\n",
      "        [0.4430],\n",
      "        [0.5269],\n",
      "        [0.4479]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4886],\n",
      "        [0.4794],\n",
      "        [0.5518],\n",
      "        [0.4891],\n",
      "        [0.3741]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5186],\n",
      "        [0.4535],\n",
      "        [0.4764],\n",
      "        [0.4797],\n",
      "        [0.4528]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4526],\n",
      "        [0.5737],\n",
      "        [0.4761],\n",
      "        [0.5919],\n",
      "        [0.5405]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4258],\n",
      "        [0.4659],\n",
      "        [0.4320],\n",
      "        [0.4740],\n",
      "        [0.4939]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4639],\n",
      "        [0.4349],\n",
      "        [0.5074],\n",
      "        [0.4250],\n",
      "        [0.4891]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4706],\n",
      "        [0.6163],\n",
      "        [0.5186],\n",
      "        [0.4825],\n",
      "        [0.5259]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4926],\n",
      "        [0.4157],\n",
      "        [0.4863],\n",
      "        [0.4824],\n",
      "        [0.4840]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4827],\n",
      "        [0.5096],\n",
      "        [0.6031],\n",
      "        [0.4745],\n",
      "        [0.4949]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5365],\n",
      "        [0.4855],\n",
      "        [0.5371],\n",
      "        [0.4130],\n",
      "        [0.5332]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4886],\n",
      "        [0.5262],\n",
      "        [0.4727],\n",
      "        [0.5400],\n",
      "        [0.4556]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4641],\n",
      "        [0.4683],\n",
      "        [0.5121],\n",
      "        [0.4642],\n",
      "        [0.5777]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4856],\n",
      "        [0.4604],\n",
      "        [0.4850],\n",
      "        [0.4530],\n",
      "        [0.6095]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4675],\n",
      "        [0.5227],\n",
      "        [0.4962],\n",
      "        [0.4033],\n",
      "        [0.4036]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5837],\n",
      "        [0.5679],\n",
      "        [0.4769],\n",
      "        [0.5455],\n",
      "        [0.4919]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4288],\n",
      "        [0.4865],\n",
      "        [0.4827],\n",
      "        [0.5105],\n",
      "        [0.4174]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4923],\n",
      "        [0.4490],\n",
      "        [0.4702],\n",
      "        [0.5179],\n",
      "        [0.4669]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4375],\n",
      "        [0.4873],\n",
      "        [0.4998],\n",
      "        [0.4980],\n",
      "        [0.4688]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4544],\n",
      "        [0.4735],\n",
      "        [0.4805],\n",
      "        [0.4033],\n",
      "        [0.4313]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4851],\n",
      "        [0.4828],\n",
      "        [0.5124],\n",
      "        [0.5864],\n",
      "        [0.5591]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4697],\n",
      "        [0.5064],\n",
      "        [0.5047],\n",
      "        [0.6029],\n",
      "        [0.5769]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4741],\n",
      "        [0.4699],\n",
      "        [0.5029],\n",
      "        [0.5734],\n",
      "        [0.4312]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4877],\n",
      "        [0.4985],\n",
      "        [0.5372],\n",
      "        [0.5177],\n",
      "        [0.4698]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 1.])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "tensor([[0.4383],\n",
      "        [0.5141],\n",
      "        [0.4595],\n",
      "        [0.4424],\n",
      "        [0.5594]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "Epoch [5/20], Loss: 0.7054\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4574],\n",
      "        [0.5164],\n",
      "        [0.4754],\n",
      "        [0.4612],\n",
      "        [0.4884]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4604],\n",
      "        [0.4365],\n",
      "        [0.5534],\n",
      "        [0.5422],\n",
      "        [0.5260]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4312],\n",
      "        [0.4933],\n",
      "        [0.5671],\n",
      "        [0.4760],\n",
      "        [0.4883]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4705],\n",
      "        [0.4121],\n",
      "        [0.4761],\n",
      "        [0.5302],\n",
      "        [0.5635]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6447],\n",
      "        [0.4148],\n",
      "        [0.5994],\n",
      "        [0.5047],\n",
      "        [0.4743]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4799],\n",
      "        [0.5128],\n",
      "        [0.5398],\n",
      "        [0.4537],\n",
      "        [0.4743]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4669],\n",
      "        [0.5010],\n",
      "        [0.5881],\n",
      "        [0.3984],\n",
      "        [0.3851]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4872],\n",
      "        [0.4531],\n",
      "        [0.4645],\n",
      "        [0.4666],\n",
      "        [0.5184]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4836],\n",
      "        [0.4966],\n",
      "        [0.4527],\n",
      "        [0.4568],\n",
      "        [0.6051]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5037],\n",
      "        [0.4531],\n",
      "        [0.5200],\n",
      "        [0.5584],\n",
      "        [0.5543]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4875],\n",
      "        [0.4817],\n",
      "        [0.5005],\n",
      "        [0.5165],\n",
      "        [0.5124]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4946],\n",
      "        [0.5009],\n",
      "        [0.4074],\n",
      "        [0.3824],\n",
      "        [0.4621]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4773],\n",
      "        [0.4903],\n",
      "        [0.5348],\n",
      "        [0.4739],\n",
      "        [0.6475]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5696],\n",
      "        [0.5586],\n",
      "        [0.4645],\n",
      "        [0.4417],\n",
      "        [0.4511]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5236],\n",
      "        [0.5489],\n",
      "        [0.5519],\n",
      "        [0.4429],\n",
      "        [0.6300]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5364],\n",
      "        [0.4738],\n",
      "        [0.5525],\n",
      "        [0.4723],\n",
      "        [0.5093]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4659],\n",
      "        [0.5724],\n",
      "        [0.4529],\n",
      "        [0.4740],\n",
      "        [0.5205]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5235],\n",
      "        [0.4438],\n",
      "        [0.4986],\n",
      "        [0.5412],\n",
      "        [0.4252]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4700],\n",
      "        [0.4406],\n",
      "        [0.5498],\n",
      "        [0.4393],\n",
      "        [0.4768]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5237],\n",
      "        [0.4839],\n",
      "        [0.4793],\n",
      "        [0.5045],\n",
      "        [0.5403]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4774],\n",
      "        [0.5287],\n",
      "        [0.5870],\n",
      "        [0.4538],\n",
      "        [0.5318]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4589],\n",
      "        [0.5122],\n",
      "        [0.5006],\n",
      "        [0.5837],\n",
      "        [0.5026]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4519],\n",
      "        [0.4314],\n",
      "        [0.5207],\n",
      "        [0.5325],\n",
      "        [0.5842]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4702],\n",
      "        [0.4700],\n",
      "        [0.4917],\n",
      "        [0.4686],\n",
      "        [0.4953]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5676],\n",
      "        [0.4971],\n",
      "        [0.5965],\n",
      "        [0.6012],\n",
      "        [0.5230]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5038],\n",
      "        [0.4961],\n",
      "        [0.4725],\n",
      "        [0.5237],\n",
      "        [0.5249]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4596],\n",
      "        [0.5033],\n",
      "        [0.4727],\n",
      "        [0.4817],\n",
      "        [0.5052]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4822],\n",
      "        [0.4790],\n",
      "        [0.5100],\n",
      "        [0.5884],\n",
      "        [0.5594]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4960],\n",
      "        [0.5050],\n",
      "        [0.5372],\n",
      "        [0.5359],\n",
      "        [0.5307]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4621],\n",
      "        [0.4927],\n",
      "        [0.5558],\n",
      "        [0.5273],\n",
      "        [0.5623]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4507],\n",
      "        [0.4197],\n",
      "        [0.4742],\n",
      "        [0.4902],\n",
      "        [0.5345]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 1.])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "tensor([[0.5162],\n",
      "        [0.5067],\n",
      "        [0.4576],\n",
      "        [0.4768],\n",
      "        [0.4531]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 0.])\n",
      "Epoch [6/20], Loss: 0.6874\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5681],\n",
      "        [0.5022],\n",
      "        [0.4689],\n",
      "        [0.5016],\n",
      "        [0.4904]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5216],\n",
      "        [0.5192],\n",
      "        [0.5220],\n",
      "        [0.5030],\n",
      "        [0.5202]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5477],\n",
      "        [0.6105],\n",
      "        [0.5174],\n",
      "        [0.5397],\n",
      "        [0.5355]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5119],\n",
      "        [0.5745],\n",
      "        [0.4843],\n",
      "        [0.4750],\n",
      "        [0.4056]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4734],\n",
      "        [0.5809],\n",
      "        [0.5495],\n",
      "        [0.5228],\n",
      "        [0.4578]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5406],\n",
      "        [0.5842],\n",
      "        [0.6019],\n",
      "        [0.5801],\n",
      "        [0.5432]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6005],\n",
      "        [0.5035],\n",
      "        [0.4306],\n",
      "        [0.6271],\n",
      "        [0.4727]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4681],\n",
      "        [0.6057],\n",
      "        [0.4652],\n",
      "        [0.5193],\n",
      "        [0.4621]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4781],\n",
      "        [0.5414],\n",
      "        [0.4775],\n",
      "        [0.5584],\n",
      "        [0.5282]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4465],\n",
      "        [0.4913],\n",
      "        [0.4459],\n",
      "        [0.5972],\n",
      "        [0.5169]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4625],\n",
      "        [0.4731],\n",
      "        [0.4858],\n",
      "        [0.4841],\n",
      "        [0.4623]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4448],\n",
      "        [0.4157],\n",
      "        [0.6103],\n",
      "        [0.4359],\n",
      "        [0.4683]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5403],\n",
      "        [0.5412],\n",
      "        [0.6122],\n",
      "        [0.5138],\n",
      "        [0.4662]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4568],\n",
      "        [0.4793],\n",
      "        [0.5854],\n",
      "        [0.4477],\n",
      "        [0.4898]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4389],\n",
      "        [0.5068],\n",
      "        [0.5375],\n",
      "        [0.4870],\n",
      "        [0.4787]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4217],\n",
      "        [0.5937],\n",
      "        [0.4111],\n",
      "        [0.5611],\n",
      "        [0.4869]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5635],\n",
      "        [0.4505],\n",
      "        [0.5178],\n",
      "        [0.4588],\n",
      "        [0.3890]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5145],\n",
      "        [0.5688],\n",
      "        [0.4679],\n",
      "        [0.5949],\n",
      "        [0.5200]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4587],\n",
      "        [0.5451],\n",
      "        [0.5648],\n",
      "        [0.5949],\n",
      "        [0.5373]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5678],\n",
      "        [0.5092],\n",
      "        [0.4632],\n",
      "        [0.4525],\n",
      "        [0.4871]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5771],\n",
      "        [0.5067],\n",
      "        [0.4591],\n",
      "        [0.5036],\n",
      "        [0.4749]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4787],\n",
      "        [0.5811],\n",
      "        [0.5567],\n",
      "        [0.4598],\n",
      "        [0.4743]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5839],\n",
      "        [0.5074],\n",
      "        [0.4834],\n",
      "        [0.4922],\n",
      "        [0.4423]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5158],\n",
      "        [0.5240],\n",
      "        [0.4651],\n",
      "        [0.5140],\n",
      "        [0.6147]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5946],\n",
      "        [0.5013],\n",
      "        [0.4654],\n",
      "        [0.5112],\n",
      "        [0.5206]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6242],\n",
      "        [0.4656],\n",
      "        [0.4975],\n",
      "        [0.5050],\n",
      "        [0.4507]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4628],\n",
      "        [0.4664],\n",
      "        [0.4060],\n",
      "        [0.5440],\n",
      "        [0.4861]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5135],\n",
      "        [0.4786],\n",
      "        [0.4452],\n",
      "        [0.5662],\n",
      "        [0.4525]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4922],\n",
      "        [0.5154],\n",
      "        [0.5120],\n",
      "        [0.5059],\n",
      "        [0.4260]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4989],\n",
      "        [0.4777],\n",
      "        [0.3699],\n",
      "        [0.4581],\n",
      "        [0.5138]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4933],\n",
      "        [0.4764],\n",
      "        [0.4232],\n",
      "        [0.4545],\n",
      "        [0.4727]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 1.])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "tensor([[0.4881],\n",
      "        [0.4470],\n",
      "        [0.5360],\n",
      "        [0.4656],\n",
      "        [0.4358]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 0.])\n",
      "Epoch [7/20], Loss: 0.6180\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5348],\n",
      "        [0.4695],\n",
      "        [0.4600],\n",
      "        [0.4659],\n",
      "        [0.5028]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6447],\n",
      "        [0.5064],\n",
      "        [0.4570],\n",
      "        [0.5072],\n",
      "        [0.3870]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4636],\n",
      "        [0.4903],\n",
      "        [0.4991],\n",
      "        [0.4786],\n",
      "        [0.4851]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5066],\n",
      "        [0.4205],\n",
      "        [0.5070],\n",
      "        [0.4877],\n",
      "        [0.5213]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5162],\n",
      "        [0.5229],\n",
      "        [0.4939],\n",
      "        [0.5145],\n",
      "        [0.5636]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.3956],\n",
      "        [0.4522],\n",
      "        [0.5593],\n",
      "        [0.4159],\n",
      "        [0.5442]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5619],\n",
      "        [0.5727],\n",
      "        [0.5499],\n",
      "        [0.5054],\n",
      "        [0.4800]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4906],\n",
      "        [0.4739],\n",
      "        [0.5138],\n",
      "        [0.5799],\n",
      "        [0.4615]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4661],\n",
      "        [0.4824],\n",
      "        [0.5238],\n",
      "        [0.5556],\n",
      "        [0.4494]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6028],\n",
      "        [0.4980],\n",
      "        [0.4759],\n",
      "        [0.5297],\n",
      "        [0.5422]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.7489],\n",
      "        [0.4875],\n",
      "        [0.5077],\n",
      "        [0.5083],\n",
      "        [0.4803]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5492],\n",
      "        [0.4489],\n",
      "        [0.4536],\n",
      "        [0.4558],\n",
      "        [0.5666]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4980],\n",
      "        [0.4789],\n",
      "        [0.4580],\n",
      "        [0.5492],\n",
      "        [0.4633]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4472],\n",
      "        [0.5240],\n",
      "        [0.4854],\n",
      "        [0.4878],\n",
      "        [0.4627]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4794],\n",
      "        [0.3997],\n",
      "        [0.5482],\n",
      "        [0.4020],\n",
      "        [0.5169]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5403],\n",
      "        [0.4666],\n",
      "        [0.5010],\n",
      "        [0.4830],\n",
      "        [0.4641]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4303],\n",
      "        [0.5728],\n",
      "        [0.5128],\n",
      "        [0.4642],\n",
      "        [0.4976]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4789],\n",
      "        [0.4366],\n",
      "        [0.5664],\n",
      "        [0.5545],\n",
      "        [0.5400]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5684],\n",
      "        [0.4616],\n",
      "        [0.4799],\n",
      "        [0.4677],\n",
      "        [0.5178]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5044],\n",
      "        [0.5353],\n",
      "        [0.4606],\n",
      "        [0.5361],\n",
      "        [0.4898]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4931],\n",
      "        [0.5202],\n",
      "        [0.6245],\n",
      "        [0.5930],\n",
      "        [0.5090]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4441],\n",
      "        [0.5273],\n",
      "        [0.5594],\n",
      "        [0.5175],\n",
      "        [0.6290]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5703],\n",
      "        [0.5028],\n",
      "        [0.5323],\n",
      "        [0.4781],\n",
      "        [0.4890]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4745],\n",
      "        [0.5152],\n",
      "        [0.5058],\n",
      "        [0.5565],\n",
      "        [0.4659]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4685],\n",
      "        [0.4717],\n",
      "        [0.5565],\n",
      "        [0.4696],\n",
      "        [0.4120]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4203],\n",
      "        [0.4681],\n",
      "        [0.5701],\n",
      "        [0.5919],\n",
      "        [0.4580]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5628],\n",
      "        [0.5642],\n",
      "        [0.4774],\n",
      "        [0.5567],\n",
      "        [0.5510]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5153],\n",
      "        [0.4099],\n",
      "        [0.5912],\n",
      "        [0.4531],\n",
      "        [0.5153]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5408],\n",
      "        [0.4411],\n",
      "        [0.6528],\n",
      "        [0.4286],\n",
      "        [0.5046]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5229],\n",
      "        [0.5523],\n",
      "        [0.4634],\n",
      "        [0.4364],\n",
      "        [0.4700]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5002],\n",
      "        [0.4531],\n",
      "        [0.4737],\n",
      "        [0.5173],\n",
      "        [0.5652]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 0.])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "tensor([[0.5175],\n",
      "        [0.5273],\n",
      "        [0.5264],\n",
      "        [0.4422],\n",
      "        [0.5494]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "Epoch [8/20], Loss: 0.6837\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5004],\n",
      "        [0.4920],\n",
      "        [0.4546],\n",
      "        [0.4269],\n",
      "        [0.5535]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5066],\n",
      "        [0.5651],\n",
      "        [0.4731],\n",
      "        [0.6209],\n",
      "        [0.5142]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4982],\n",
      "        [0.5440],\n",
      "        [0.5402],\n",
      "        [0.4966],\n",
      "        [0.5018]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5580],\n",
      "        [0.4763],\n",
      "        [0.4954],\n",
      "        [0.5231],\n",
      "        [0.5386]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6092],\n",
      "        [0.5732],\n",
      "        [0.5083],\n",
      "        [0.4531],\n",
      "        [0.4792]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4585],\n",
      "        [0.5197],\n",
      "        [0.4842],\n",
      "        [0.4555],\n",
      "        [0.5308]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4391],\n",
      "        [0.4816],\n",
      "        [0.5410],\n",
      "        [0.5626],\n",
      "        [0.6360]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5223],\n",
      "        [0.4968],\n",
      "        [0.4976],\n",
      "        [0.5228],\n",
      "        [0.4605]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4892],\n",
      "        [0.4823],\n",
      "        [0.4955],\n",
      "        [0.4870],\n",
      "        [0.4780]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4688],\n",
      "        [0.3891],\n",
      "        [0.5722],\n",
      "        [0.5435],\n",
      "        [0.4790]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6061],\n",
      "        [0.4633],\n",
      "        [0.4947],\n",
      "        [0.4947],\n",
      "        [0.4192]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5051],\n",
      "        [0.5253],\n",
      "        [0.4235],\n",
      "        [0.4814],\n",
      "        [0.5162]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5660],\n",
      "        [0.5179],\n",
      "        [0.4716],\n",
      "        [0.4768],\n",
      "        [0.5128]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4982],\n",
      "        [0.5209],\n",
      "        [0.4596],\n",
      "        [0.5045],\n",
      "        [0.5708]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5288],\n",
      "        [0.5259],\n",
      "        [0.4747],\n",
      "        [0.5917],\n",
      "        [0.5523]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5317],\n",
      "        [0.4606],\n",
      "        [0.5229],\n",
      "        [0.5440],\n",
      "        [0.6626]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6137],\n",
      "        [0.5223],\n",
      "        [0.5083],\n",
      "        [0.4693],\n",
      "        [0.4144]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6032],\n",
      "        [0.5046],\n",
      "        [0.5423],\n",
      "        [0.3719],\n",
      "        [0.4170]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4871],\n",
      "        [0.4660],\n",
      "        [0.5547],\n",
      "        [0.5763],\n",
      "        [0.5666]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5963],\n",
      "        [0.4015],\n",
      "        [0.4880],\n",
      "        [0.4469],\n",
      "        [0.4614]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5861],\n",
      "        [0.5916],\n",
      "        [0.5361],\n",
      "        [0.4509],\n",
      "        [0.4440]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5631],\n",
      "        [0.4874],\n",
      "        [0.4467],\n",
      "        [0.4785],\n",
      "        [0.5587]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4619],\n",
      "        [0.5304],\n",
      "        [0.5671],\n",
      "        [0.5025],\n",
      "        [0.4738]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4636],\n",
      "        [0.5611],\n",
      "        [0.4944],\n",
      "        [0.4680],\n",
      "        [0.4778]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5959],\n",
      "        [0.4730],\n",
      "        [0.5456],\n",
      "        [0.4820],\n",
      "        [0.4973]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5447],\n",
      "        [0.5914],\n",
      "        [0.5266],\n",
      "        [0.5912],\n",
      "        [0.5480]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4916],\n",
      "        [0.5210],\n",
      "        [0.5364],\n",
      "        [0.4935],\n",
      "        [0.4269]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4554],\n",
      "        [0.4933],\n",
      "        [0.5312],\n",
      "        [0.4836],\n",
      "        [0.5807]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4618],\n",
      "        [0.5210],\n",
      "        [0.5441],\n",
      "        [0.5303],\n",
      "        [0.5022]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.3991],\n",
      "        [0.7095],\n",
      "        [0.4873],\n",
      "        [0.5811],\n",
      "        [0.4786]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4826],\n",
      "        [0.4735],\n",
      "        [0.4692],\n",
      "        [0.4952],\n",
      "        [0.4506]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 1.])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "tensor([[0.5458],\n",
      "        [0.5212],\n",
      "        [0.5505],\n",
      "        [0.4917],\n",
      "        [0.5181]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "Epoch [9/20], Loss: 0.6988\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4817],\n",
      "        [0.5209],\n",
      "        [0.4868],\n",
      "        [0.5242],\n",
      "        [0.5715]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5706],\n",
      "        [0.4435],\n",
      "        [0.5206],\n",
      "        [0.6343],\n",
      "        [0.4007]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5580],\n",
      "        [0.6847],\n",
      "        [0.5131],\n",
      "        [0.4931],\n",
      "        [0.4795]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6141],\n",
      "        [0.5703],\n",
      "        [0.4751],\n",
      "        [0.6363],\n",
      "        [0.4793]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4731],\n",
      "        [0.5877],\n",
      "        [0.4799],\n",
      "        [0.4748],\n",
      "        [0.5297]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4817],\n",
      "        [0.5571],\n",
      "        [0.4753],\n",
      "        [0.4537],\n",
      "        [0.6287]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5065],\n",
      "        [0.4053],\n",
      "        [0.5726],\n",
      "        [0.5244],\n",
      "        [0.5257]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4675],\n",
      "        [0.4549],\n",
      "        [0.5013],\n",
      "        [0.5299],\n",
      "        [0.6308]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4534],\n",
      "        [0.5716],\n",
      "        [0.3620],\n",
      "        [0.5421],\n",
      "        [0.4134]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5778],\n",
      "        [0.5357],\n",
      "        [0.5266],\n",
      "        [0.4505],\n",
      "        [0.4611]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6082],\n",
      "        [0.5521],\n",
      "        [0.5142],\n",
      "        [0.5544],\n",
      "        [0.4456]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.3417],\n",
      "        [0.6242],\n",
      "        [0.4437],\n",
      "        [0.6161],\n",
      "        [0.5430]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4620],\n",
      "        [0.5013],\n",
      "        [0.5071],\n",
      "        [0.5582],\n",
      "        [0.4486]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5183],\n",
      "        [0.5514],\n",
      "        [0.5336],\n",
      "        [0.5168],\n",
      "        [0.3894]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4912],\n",
      "        [0.4885],\n",
      "        [0.4622],\n",
      "        [0.5143],\n",
      "        [0.4854]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5029],\n",
      "        [0.4823],\n",
      "        [0.4166],\n",
      "        [0.5713],\n",
      "        [0.5148]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4910],\n",
      "        [0.5418],\n",
      "        [0.5390],\n",
      "        [0.5537],\n",
      "        [0.4954]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5435],\n",
      "        [0.5079],\n",
      "        [0.5031],\n",
      "        [0.4406],\n",
      "        [0.5496]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4844],\n",
      "        [0.4299],\n",
      "        [0.5052],\n",
      "        [0.4795],\n",
      "        [0.5494]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4934],\n",
      "        [0.5192],\n",
      "        [0.4553],\n",
      "        [0.4947],\n",
      "        [0.5421]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4052],\n",
      "        [0.4614],\n",
      "        [0.4779],\n",
      "        [0.5006],\n",
      "        [0.5049]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4929],\n",
      "        [0.4981],\n",
      "        [0.4721],\n",
      "        [0.4641],\n",
      "        [0.5408]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5228],\n",
      "        [0.5192],\n",
      "        [0.4306],\n",
      "        [0.4434],\n",
      "        [0.4619]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4615],\n",
      "        [0.3753],\n",
      "        [0.4705],\n",
      "        [0.5788],\n",
      "        [0.4422]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5133],\n",
      "        [0.5523],\n",
      "        [0.5257],\n",
      "        [0.3950],\n",
      "        [0.5037]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6118],\n",
      "        [0.6207],\n",
      "        [0.4549],\n",
      "        [0.4827],\n",
      "        [0.4021]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4746],\n",
      "        [0.6849],\n",
      "        [0.4231],\n",
      "        [0.4892],\n",
      "        [0.4311]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5638],\n",
      "        [0.5514],\n",
      "        [0.5587],\n",
      "        [0.4463],\n",
      "        [0.4628]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4789],\n",
      "        [0.5504],\n",
      "        [0.4975],\n",
      "        [0.4529],\n",
      "        [0.4470]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4850],\n",
      "        [0.5484],\n",
      "        [0.5080],\n",
      "        [0.4872],\n",
      "        [0.4625]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4501],\n",
      "        [0.4589],\n",
      "        [0.5231],\n",
      "        [0.5191],\n",
      "        [0.4883]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "tensor([[0.6327],\n",
      "        [0.5461],\n",
      "        [0.5499],\n",
      "        [0.4869],\n",
      "        [0.5080]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 1.])\n",
      "Epoch [10/20], Loss: 0.6192\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4969],\n",
      "        [0.5162],\n",
      "        [0.5779],\n",
      "        [0.6311],\n",
      "        [0.4945]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4981],\n",
      "        [0.4742],\n",
      "        [0.4977],\n",
      "        [0.5311],\n",
      "        [0.3477]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5625],\n",
      "        [0.4913],\n",
      "        [0.5516],\n",
      "        [0.4765],\n",
      "        [0.4859]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4583],\n",
      "        [0.4825],\n",
      "        [0.4313],\n",
      "        [0.5016],\n",
      "        [0.5766]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5347],\n",
      "        [0.5007],\n",
      "        [0.4673],\n",
      "        [0.4623],\n",
      "        [0.4296]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4999],\n",
      "        [0.4935],\n",
      "        [0.4773],\n",
      "        [0.5820],\n",
      "        [0.5190]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5053],\n",
      "        [0.5232],\n",
      "        [0.5791],\n",
      "        [0.3756],\n",
      "        [0.5645]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5288],\n",
      "        [0.5378],\n",
      "        [0.4777],\n",
      "        [0.6250],\n",
      "        [0.4973]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5237],\n",
      "        [0.5066],\n",
      "        [0.5181],\n",
      "        [0.5823],\n",
      "        [0.5615]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4908],\n",
      "        [0.5362],\n",
      "        [0.5151],\n",
      "        [0.5086],\n",
      "        [0.5211]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4256],\n",
      "        [0.4963],\n",
      "        [0.5880],\n",
      "        [0.6063],\n",
      "        [0.4627]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4954],\n",
      "        [0.4846],\n",
      "        [0.4806],\n",
      "        [0.5645],\n",
      "        [0.5154]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5144],\n",
      "        [0.4553],\n",
      "        [0.4540],\n",
      "        [0.4825],\n",
      "        [0.5262]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5206],\n",
      "        [0.5437],\n",
      "        [0.5113],\n",
      "        [0.5501],\n",
      "        [0.4922]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.7684],\n",
      "        [0.4741],\n",
      "        [0.4883],\n",
      "        [0.6302],\n",
      "        [0.5370]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4961],\n",
      "        [0.3952],\n",
      "        [0.6715],\n",
      "        [0.4581],\n",
      "        [0.5797]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4565],\n",
      "        [0.6388],\n",
      "        [0.5178],\n",
      "        [0.4430],\n",
      "        [0.3995]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4997],\n",
      "        [0.5050],\n",
      "        [0.4515],\n",
      "        [0.5612],\n",
      "        [0.4814]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4729],\n",
      "        [0.4651],\n",
      "        [0.5850],\n",
      "        [0.5094],\n",
      "        [0.6102]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4219],\n",
      "        [0.4630],\n",
      "        [0.4553],\n",
      "        [0.4866],\n",
      "        [0.4577]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4895],\n",
      "        [0.5104],\n",
      "        [0.6460],\n",
      "        [0.4595],\n",
      "        [0.5606]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5435],\n",
      "        [0.5758],\n",
      "        [0.4546],\n",
      "        [0.4482],\n",
      "        [0.5946]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5002],\n",
      "        [0.5499],\n",
      "        [0.5398],\n",
      "        [0.4782],\n",
      "        [0.4825]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4929],\n",
      "        [0.5773],\n",
      "        [0.5018],\n",
      "        [0.5163],\n",
      "        [0.4994]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4947],\n",
      "        [0.4067],\n",
      "        [0.5542],\n",
      "        [0.4850],\n",
      "        [0.6322]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5448],\n",
      "        [0.5207],\n",
      "        [0.5219],\n",
      "        [0.5108],\n",
      "        [0.4907]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5486],\n",
      "        [0.5120],\n",
      "        [0.5624],\n",
      "        [0.4518],\n",
      "        [0.5425]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5558],\n",
      "        [0.4951],\n",
      "        [0.4780],\n",
      "        [0.4613],\n",
      "        [0.5089]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5092],\n",
      "        [0.5283],\n",
      "        [0.4498],\n",
      "        [0.4577],\n",
      "        [0.4655]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6023],\n",
      "        [0.4645],\n",
      "        [0.5234],\n",
      "        [0.5214],\n",
      "        [0.6558]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4905],\n",
      "        [0.4402],\n",
      "        [0.4935],\n",
      "        [0.6962],\n",
      "        [0.4765]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "tensor([[0.5261],\n",
      "        [0.4632],\n",
      "        [0.5200],\n",
      "        [0.4620],\n",
      "        [0.5344]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "Epoch [11/20], Loss: 0.7240\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5215],\n",
      "        [0.4083],\n",
      "        [0.4748],\n",
      "        [0.6258],\n",
      "        [0.5193]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5079],\n",
      "        [0.5416],\n",
      "        [0.4999],\n",
      "        [0.5173],\n",
      "        [0.5946]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5569],\n",
      "        [0.4749],\n",
      "        [0.4709],\n",
      "        [0.3789],\n",
      "        [0.6383]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4641],\n",
      "        [0.4875],\n",
      "        [0.4472],\n",
      "        [0.5435],\n",
      "        [0.4472]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5207],\n",
      "        [0.5045],\n",
      "        [0.4617],\n",
      "        [0.6489],\n",
      "        [0.4984]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5535],\n",
      "        [0.4781],\n",
      "        [0.6149],\n",
      "        [0.5633],\n",
      "        [0.5465]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4529],\n",
      "        [0.5296],\n",
      "        [0.4981],\n",
      "        [0.5474],\n",
      "        [0.4904]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4453],\n",
      "        [0.4289],\n",
      "        [0.6056],\n",
      "        [0.5816],\n",
      "        [0.5060]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4618],\n",
      "        [0.5865],\n",
      "        [0.4934],\n",
      "        [0.5358],\n",
      "        [0.5004]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4478],\n",
      "        [0.4567],\n",
      "        [0.4115],\n",
      "        [0.4676],\n",
      "        [0.5745]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4582],\n",
      "        [0.6247],\n",
      "        [0.5694],\n",
      "        [0.4872],\n",
      "        [0.5337]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4941],\n",
      "        [0.5074],\n",
      "        [0.5942],\n",
      "        [0.5013],\n",
      "        [0.4682]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4871],\n",
      "        [0.4553],\n",
      "        [0.5106],\n",
      "        [0.5796],\n",
      "        [0.4441]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4369],\n",
      "        [0.5858],\n",
      "        [0.5273],\n",
      "        [0.4848],\n",
      "        [0.4769]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6867],\n",
      "        [0.4816],\n",
      "        [0.4977],\n",
      "        [0.5047],\n",
      "        [0.5027]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4910],\n",
      "        [0.4443],\n",
      "        [0.4496],\n",
      "        [0.4317],\n",
      "        [0.3879]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.3900],\n",
      "        [0.5631],\n",
      "        [0.5314],\n",
      "        [0.4359],\n",
      "        [0.3795]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5174],\n",
      "        [0.4652],\n",
      "        [0.4181],\n",
      "        [0.5833],\n",
      "        [0.5064]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5673],\n",
      "        [0.5805],\n",
      "        [0.4127],\n",
      "        [0.5493],\n",
      "        [0.4356]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5084],\n",
      "        [0.5485],\n",
      "        [0.4917],\n",
      "        [0.6058],\n",
      "        [0.5193]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5971],\n",
      "        [0.5360],\n",
      "        [0.5364],\n",
      "        [0.4701],\n",
      "        [0.5114]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6950],\n",
      "        [0.4288],\n",
      "        [0.3906],\n",
      "        [0.4246],\n",
      "        [0.4612]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5106],\n",
      "        [0.4775],\n",
      "        [0.4608],\n",
      "        [0.4802],\n",
      "        [0.4800]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5112],\n",
      "        [0.4808],\n",
      "        [0.4860],\n",
      "        [0.5486],\n",
      "        [0.4667]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4944],\n",
      "        [0.5130],\n",
      "        [0.4454],\n",
      "        [0.4564],\n",
      "        [0.4910]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4246],\n",
      "        [0.5489],\n",
      "        [0.5450],\n",
      "        [0.5268],\n",
      "        [0.5145]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4675],\n",
      "        [0.5095],\n",
      "        [0.5011],\n",
      "        [0.6438],\n",
      "        [0.4970]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4965],\n",
      "        [0.5844],\n",
      "        [0.5232],\n",
      "        [0.5909],\n",
      "        [0.4380]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5115],\n",
      "        [0.4967],\n",
      "        [0.4481],\n",
      "        [0.6325],\n",
      "        [0.5246]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5290],\n",
      "        [0.5021],\n",
      "        [0.5580],\n",
      "        [0.5584],\n",
      "        [0.5221]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5051],\n",
      "        [0.5450],\n",
      "        [0.5414],\n",
      "        [0.4650],\n",
      "        [0.4428]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 0.])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "tensor([[0.5431],\n",
      "        [0.4245],\n",
      "        [0.4728],\n",
      "        [0.4653],\n",
      "        [0.5883]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "Epoch [12/20], Loss: 0.6063\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5787],\n",
      "        [0.5221],\n",
      "        [0.3960],\n",
      "        [0.3883],\n",
      "        [0.4878]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5409],\n",
      "        [0.5003],\n",
      "        [0.5049],\n",
      "        [0.4517],\n",
      "        [0.4930]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4972],\n",
      "        [0.4820],\n",
      "        [0.5201],\n",
      "        [0.5122],\n",
      "        [0.5048]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5453],\n",
      "        [0.5053],\n",
      "        [0.4785],\n",
      "        [0.6857],\n",
      "        [0.5405]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5729],\n",
      "        [0.4942],\n",
      "        [0.5581],\n",
      "        [0.4834],\n",
      "        [0.4614]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5113],\n",
      "        [0.5638],\n",
      "        [0.6107],\n",
      "        [0.5918],\n",
      "        [0.5406]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4197],\n",
      "        [0.4938],\n",
      "        [0.4990],\n",
      "        [0.5014],\n",
      "        [0.4578]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4762],\n",
      "        [0.4150],\n",
      "        [0.4643],\n",
      "        [0.4982],\n",
      "        [0.4860]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5519],\n",
      "        [0.5436],\n",
      "        [0.4723],\n",
      "        [0.5138],\n",
      "        [0.4599]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5521],\n",
      "        [0.5428],\n",
      "        [0.4791],\n",
      "        [0.5290],\n",
      "        [0.4909]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4326],\n",
      "        [0.6118],\n",
      "        [0.4824],\n",
      "        [0.5604],\n",
      "        [0.4831]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5457],\n",
      "        [0.4198],\n",
      "        [0.4878],\n",
      "        [0.6025],\n",
      "        [0.5036]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5537],\n",
      "        [0.5882],\n",
      "        [0.5718],\n",
      "        [0.5467],\n",
      "        [0.6732]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6139],\n",
      "        [0.5268],\n",
      "        [0.5240],\n",
      "        [0.5361],\n",
      "        [0.5243]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4647],\n",
      "        [0.4815],\n",
      "        [0.5694],\n",
      "        [0.4853],\n",
      "        [0.5251]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5087],\n",
      "        [0.4974],\n",
      "        [0.4864],\n",
      "        [0.5511],\n",
      "        [0.4980]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5102],\n",
      "        [0.5899],\n",
      "        [0.4633],\n",
      "        [0.4851],\n",
      "        [0.5067]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4793],\n",
      "        [0.4398],\n",
      "        [0.5157],\n",
      "        [0.5758],\n",
      "        [0.5135]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5082],\n",
      "        [0.4587],\n",
      "        [0.4617],\n",
      "        [0.4715],\n",
      "        [0.5567]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5091],\n",
      "        [0.6780],\n",
      "        [0.6470],\n",
      "        [0.5417],\n",
      "        [0.4548]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5248],\n",
      "        [0.5258],\n",
      "        [0.4401],\n",
      "        [0.5580],\n",
      "        [0.5511]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4530],\n",
      "        [0.4634],\n",
      "        [0.5015],\n",
      "        [0.4978],\n",
      "        [0.5239]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5253],\n",
      "        [0.4497],\n",
      "        [0.5451],\n",
      "        [0.4570],\n",
      "        [0.4581]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5215],\n",
      "        [0.4775],\n",
      "        [0.5241],\n",
      "        [0.4911],\n",
      "        [0.4986]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.3791],\n",
      "        [0.5509],\n",
      "        [0.5511],\n",
      "        [0.5016],\n",
      "        [0.4631]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4937],\n",
      "        [0.5078],\n",
      "        [0.4726],\n",
      "        [0.4820],\n",
      "        [0.3219]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5437],\n",
      "        [0.5058],\n",
      "        [0.5790],\n",
      "        [0.5389],\n",
      "        [0.4118]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4822],\n",
      "        [0.5941],\n",
      "        [0.4831],\n",
      "        [0.4918],\n",
      "        [0.5463]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4929],\n",
      "        [0.5336],\n",
      "        [0.5349],\n",
      "        [0.5533],\n",
      "        [0.5995]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5549],\n",
      "        [0.5947],\n",
      "        [0.5372],\n",
      "        [0.3818],\n",
      "        [0.5162]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5096],\n",
      "        [0.4961],\n",
      "        [0.5240],\n",
      "        [0.5421],\n",
      "        [0.4592]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 1.])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "tensor([[0.5169],\n",
      "        [0.5954],\n",
      "        [0.4705],\n",
      "        [0.5182],\n",
      "        [0.4987]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 1.])\n",
      "Epoch [13/20], Loss: 0.6104\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5785],\n",
      "        [0.4950],\n",
      "        [0.4753],\n",
      "        [0.5664],\n",
      "        [0.6156]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4797],\n",
      "        [0.4552],\n",
      "        [0.5360],\n",
      "        [0.4766],\n",
      "        [0.3696]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5540],\n",
      "        [0.5337],\n",
      "        [0.4168],\n",
      "        [0.5213],\n",
      "        [0.7018]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4610],\n",
      "        [0.4092],\n",
      "        [0.4491],\n",
      "        [0.5662],\n",
      "        [0.5172]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4386],\n",
      "        [0.5492],\n",
      "        [0.5064],\n",
      "        [0.5026],\n",
      "        [0.4345]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6781],\n",
      "        [0.5744],\n",
      "        [0.4638],\n",
      "        [0.5287],\n",
      "        [0.4256]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5678],\n",
      "        [0.4587],\n",
      "        [0.4983],\n",
      "        [0.5916],\n",
      "        [0.5016]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4832],\n",
      "        [0.5718],\n",
      "        [0.4947],\n",
      "        [0.4258],\n",
      "        [0.4278]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4688],\n",
      "        [0.5428],\n",
      "        [0.6697],\n",
      "        [0.5557],\n",
      "        [0.4445]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5307],\n",
      "        [0.5805],\n",
      "        [0.4228],\n",
      "        [0.4798],\n",
      "        [0.4438]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4611],\n",
      "        [0.4548],\n",
      "        [0.4822],\n",
      "        [0.5598],\n",
      "        [0.4888]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5415],\n",
      "        [0.5101],\n",
      "        [0.4651],\n",
      "        [0.4712],\n",
      "        [0.4841]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5230],\n",
      "        [0.5225],\n",
      "        [0.5089],\n",
      "        [0.5005],\n",
      "        [0.5093]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5520],\n",
      "        [0.5245],\n",
      "        [0.5245],\n",
      "        [0.5964],\n",
      "        [0.4421]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5692],\n",
      "        [0.4032],\n",
      "        [0.4678],\n",
      "        [0.5434],\n",
      "        [0.5043]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5096],\n",
      "        [0.6799],\n",
      "        [0.4924],\n",
      "        [0.4808],\n",
      "        [0.5041]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4290],\n",
      "        [0.5342],\n",
      "        [0.4718],\n",
      "        [0.4777],\n",
      "        [0.5122]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5980],\n",
      "        [0.5601],\n",
      "        [0.3200],\n",
      "        [0.5119],\n",
      "        [0.5458]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5759],\n",
      "        [0.4794],\n",
      "        [0.5212],\n",
      "        [0.5137],\n",
      "        [0.5919]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5303],\n",
      "        [0.5282],\n",
      "        [0.5198],\n",
      "        [0.5931],\n",
      "        [0.5213]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4301],\n",
      "        [0.4907],\n",
      "        [0.5148],\n",
      "        [0.4906],\n",
      "        [0.5798]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5329],\n",
      "        [0.5449],\n",
      "        [0.4625],\n",
      "        [0.5016],\n",
      "        [0.4967]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6635],\n",
      "        [0.4921],\n",
      "        [0.5065],\n",
      "        [0.5039],\n",
      "        [0.5699]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6869],\n",
      "        [0.4860],\n",
      "        [0.5474],\n",
      "        [0.5233],\n",
      "        [0.5002]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6076],\n",
      "        [0.5652],\n",
      "        [0.5293],\n",
      "        [0.4766],\n",
      "        [0.6354]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5529],\n",
      "        [0.5336],\n",
      "        [0.4960],\n",
      "        [0.4430],\n",
      "        [0.5668]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.3860],\n",
      "        [0.4595],\n",
      "        [0.3248],\n",
      "        [0.5016],\n",
      "        [0.4905]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4422],\n",
      "        [0.5472],\n",
      "        [0.5189],\n",
      "        [0.4820],\n",
      "        [0.5134]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4868],\n",
      "        [0.4595],\n",
      "        [0.5214],\n",
      "        [0.5410],\n",
      "        [0.4728]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5565],\n",
      "        [0.5261],\n",
      "        [0.5902],\n",
      "        [0.3416],\n",
      "        [0.6485]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5070],\n",
      "        [0.5089],\n",
      "        [0.4007],\n",
      "        [0.5739],\n",
      "        [0.5426]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 1.])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "tensor([[0.5012],\n",
      "        [0.4717],\n",
      "        [0.6337],\n",
      "        [0.4519],\n",
      "        [0.4281]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 0.])\n",
      "Epoch [14/20], Loss: 0.6268\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4634],\n",
      "        [0.4318],\n",
      "        [0.5097],\n",
      "        [0.5567],\n",
      "        [0.4922]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5523],\n",
      "        [0.5042],\n",
      "        [0.4911],\n",
      "        [0.5601],\n",
      "        [0.5058]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6481],\n",
      "        [0.5185],\n",
      "        [0.4808],\n",
      "        [0.4993],\n",
      "        [0.4974]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4939],\n",
      "        [0.4835],\n",
      "        [0.5992],\n",
      "        [0.5654],\n",
      "        [0.5205]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4154],\n",
      "        [0.4909],\n",
      "        [0.5134],\n",
      "        [0.5231],\n",
      "        [0.4236]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5491],\n",
      "        [0.4892],\n",
      "        [0.5621],\n",
      "        [0.5145],\n",
      "        [0.4653]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6086],\n",
      "        [0.4953],\n",
      "        [0.5114],\n",
      "        [0.4822],\n",
      "        [0.5465]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5330],\n",
      "        [0.5425],\n",
      "        [0.5101],\n",
      "        [0.4935],\n",
      "        [0.5441]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5234],\n",
      "        [0.5227],\n",
      "        [0.4827],\n",
      "        [0.5596],\n",
      "        [0.4378]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5261],\n",
      "        [0.5298],\n",
      "        [0.5120],\n",
      "        [0.5515],\n",
      "        [0.5476]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4977],\n",
      "        [0.5964],\n",
      "        [0.5543],\n",
      "        [0.5022],\n",
      "        [0.4288]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4819],\n",
      "        [0.4744],\n",
      "        [0.5055],\n",
      "        [0.5271],\n",
      "        [0.4344]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5207],\n",
      "        [0.4932],\n",
      "        [0.5478],\n",
      "        [0.5167],\n",
      "        [0.4641]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5427],\n",
      "        [0.4396],\n",
      "        [0.5141],\n",
      "        [0.5630],\n",
      "        [0.6222]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5107],\n",
      "        [0.5217],\n",
      "        [0.5969],\n",
      "        [0.5481],\n",
      "        [0.5158]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5509],\n",
      "        [0.4785],\n",
      "        [0.4879],\n",
      "        [0.4874],\n",
      "        [0.4878]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5247],\n",
      "        [0.5614],\n",
      "        [0.5135],\n",
      "        [0.4598],\n",
      "        [0.4078]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5604],\n",
      "        [0.5279],\n",
      "        [0.4331],\n",
      "        [0.5460],\n",
      "        [0.4819]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5209],\n",
      "        [0.6157],\n",
      "        [0.6190],\n",
      "        [0.5166],\n",
      "        [0.5410]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5083],\n",
      "        [0.4969],\n",
      "        [0.4548],\n",
      "        [0.3861],\n",
      "        [0.6700]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5456],\n",
      "        [0.5970],\n",
      "        [0.4265],\n",
      "        [0.4916],\n",
      "        [0.5249]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4753],\n",
      "        [0.4279],\n",
      "        [0.4939],\n",
      "        [0.4496],\n",
      "        [0.4861]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4875],\n",
      "        [0.6156],\n",
      "        [0.5081],\n",
      "        [0.7094],\n",
      "        [0.5381]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4848],\n",
      "        [0.4670],\n",
      "        [0.4777],\n",
      "        [0.6541],\n",
      "        [0.5088]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4905],\n",
      "        [0.5140],\n",
      "        [0.5882],\n",
      "        [0.5541],\n",
      "        [0.5407]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5527],\n",
      "        [0.4328],\n",
      "        [0.5092],\n",
      "        [0.5528],\n",
      "        [0.6408]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5078],\n",
      "        [0.4276],\n",
      "        [0.3019],\n",
      "        [0.5038],\n",
      "        [0.5144]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5031],\n",
      "        [0.6022],\n",
      "        [0.7879],\n",
      "        [0.4963],\n",
      "        [0.5008]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4668],\n",
      "        [0.4452],\n",
      "        [0.4858],\n",
      "        [0.5904],\n",
      "        [0.5769]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6370],\n",
      "        [0.3521],\n",
      "        [0.5391],\n",
      "        [0.4501],\n",
      "        [0.5987]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5846],\n",
      "        [0.5001],\n",
      "        [0.4794],\n",
      "        [0.4397],\n",
      "        [0.4865]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 0.])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "tensor([[0.5112],\n",
      "        [0.4356],\n",
      "        [0.4861],\n",
      "        [0.5229],\n",
      "        [0.6376]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 1.])\n",
      "Epoch [15/20], Loss: 0.6790\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5224],\n",
      "        [0.5720],\n",
      "        [0.5213],\n",
      "        [0.4321],\n",
      "        [0.3775]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4954],\n",
      "        [0.4663],\n",
      "        [0.4959],\n",
      "        [0.6714],\n",
      "        [0.5039]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5903],\n",
      "        [0.5879],\n",
      "        [0.5433],\n",
      "        [0.4938],\n",
      "        [0.5033]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4704],\n",
      "        [0.5124],\n",
      "        [0.5714],\n",
      "        [0.4951],\n",
      "        [0.5222]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4624],\n",
      "        [0.4210],\n",
      "        [0.5555],\n",
      "        [0.4863],\n",
      "        [0.5027]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4425],\n",
      "        [0.5033],\n",
      "        [0.3065],\n",
      "        [0.5857],\n",
      "        [0.4117]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4498],\n",
      "        [0.4748],\n",
      "        [0.5094],\n",
      "        [0.4704],\n",
      "        [0.5325]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5153],\n",
      "        [0.5221],\n",
      "        [0.4840],\n",
      "        [0.3850],\n",
      "        [0.5235]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4777],\n",
      "        [0.5093],\n",
      "        [0.5671],\n",
      "        [0.4652],\n",
      "        [0.4645]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4861],\n",
      "        [0.5866],\n",
      "        [0.4805],\n",
      "        [0.4769],\n",
      "        [0.5887]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5997],\n",
      "        [0.6085],\n",
      "        [0.4950],\n",
      "        [0.5150],\n",
      "        [0.4602]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5828],\n",
      "        [0.5339],\n",
      "        [0.5371],\n",
      "        [0.5708],\n",
      "        [0.4708]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4769],\n",
      "        [0.5068],\n",
      "        [0.5188],\n",
      "        [0.4163],\n",
      "        [0.5621]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5047],\n",
      "        [0.6046],\n",
      "        [0.5656],\n",
      "        [0.4752],\n",
      "        [0.6999]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4988],\n",
      "        [0.5605],\n",
      "        [0.5321],\n",
      "        [0.5507],\n",
      "        [0.4935]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5055],\n",
      "        [0.5451],\n",
      "        [0.3411],\n",
      "        [0.6766],\n",
      "        [0.6201]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5980],\n",
      "        [0.4941],\n",
      "        [0.5511],\n",
      "        [0.4607],\n",
      "        [0.4567]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4402],\n",
      "        [0.4847],\n",
      "        [0.5123],\n",
      "        [0.5151],\n",
      "        [0.5467]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5776],\n",
      "        [0.5160],\n",
      "        [0.5033],\n",
      "        [0.5321],\n",
      "        [0.5792]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5958],\n",
      "        [0.4516],\n",
      "        [0.4885],\n",
      "        [0.5439],\n",
      "        [0.5023]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4432],\n",
      "        [0.5600],\n",
      "        [0.6013],\n",
      "        [0.4876],\n",
      "        [0.4828]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5552],\n",
      "        [0.5027],\n",
      "        [0.4828],\n",
      "        [0.5265],\n",
      "        [0.4840]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6270],\n",
      "        [0.6109],\n",
      "        [0.5174],\n",
      "        [0.5084],\n",
      "        [0.4903]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4644],\n",
      "        [0.4459],\n",
      "        [0.4825],\n",
      "        [0.5364],\n",
      "        [0.4797]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.7194],\n",
      "        [0.4513],\n",
      "        [0.5730],\n",
      "        [0.5439],\n",
      "        [0.5225]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.3663],\n",
      "        [0.5911],\n",
      "        [0.4965],\n",
      "        [0.4775],\n",
      "        [0.5941]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4995],\n",
      "        [0.5158],\n",
      "        [0.4933],\n",
      "        [0.5235],\n",
      "        [0.5593]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5021],\n",
      "        [0.6520],\n",
      "        [0.4896],\n",
      "        [0.4853],\n",
      "        [0.5107]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5515],\n",
      "        [0.4245],\n",
      "        [0.6290],\n",
      "        [0.4412],\n",
      "        [0.4341]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5403],\n",
      "        [0.4850],\n",
      "        [0.4878],\n",
      "        [0.5182],\n",
      "        [0.5018]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5650],\n",
      "        [0.5773],\n",
      "        [0.5626],\n",
      "        [0.5215],\n",
      "        [0.5299]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 1.])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "tensor([[0.5036],\n",
      "        [0.4897],\n",
      "        [0.4346],\n",
      "        [0.4684],\n",
      "        [0.5067]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "Epoch [16/20], Loss: 0.7109\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5187],\n",
      "        [0.5326],\n",
      "        [0.6888],\n",
      "        [0.4846],\n",
      "        [0.5148]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.7938],\n",
      "        [0.6125],\n",
      "        [0.4790],\n",
      "        [0.5226],\n",
      "        [0.4860]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6728],\n",
      "        [0.4777],\n",
      "        [0.5281],\n",
      "        [0.5979],\n",
      "        [0.6723]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5620],\n",
      "        [0.4740],\n",
      "        [0.4643],\n",
      "        [0.4347],\n",
      "        [0.4716]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5574],\n",
      "        [0.4135],\n",
      "        [0.3801],\n",
      "        [0.4448],\n",
      "        [0.5216]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5164],\n",
      "        [0.5365],\n",
      "        [0.5685],\n",
      "        [0.6778],\n",
      "        [0.4778]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.3990],\n",
      "        [0.5477],\n",
      "        [0.3535],\n",
      "        [0.4630],\n",
      "        [0.5632]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4338],\n",
      "        [0.4639],\n",
      "        [0.4588],\n",
      "        [0.5289],\n",
      "        [0.4407]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5804],\n",
      "        [0.4887],\n",
      "        [0.5000],\n",
      "        [0.4618],\n",
      "        [0.4198]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5304],\n",
      "        [0.5136],\n",
      "        [0.5119],\n",
      "        [0.5171],\n",
      "        [0.4228]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4105],\n",
      "        [0.6107],\n",
      "        [0.4927],\n",
      "        [0.4990],\n",
      "        [0.4987]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4987],\n",
      "        [0.6002],\n",
      "        [0.4260],\n",
      "        [0.5047],\n",
      "        [0.5699]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6052],\n",
      "        [0.5294],\n",
      "        [0.4733],\n",
      "        [0.5726],\n",
      "        [0.4547]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5161],\n",
      "        [0.4661],\n",
      "        [0.5347],\n",
      "        [0.4498],\n",
      "        [0.4775]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5430],\n",
      "        [0.4840],\n",
      "        [0.4916],\n",
      "        [0.4623],\n",
      "        [0.4819]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5189],\n",
      "        [0.5956],\n",
      "        [0.5305],\n",
      "        [0.5802],\n",
      "        [0.5553]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4565],\n",
      "        [0.4369],\n",
      "        [0.4908],\n",
      "        [0.5174],\n",
      "        [0.5248]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5427],\n",
      "        [0.5463],\n",
      "        [0.4985],\n",
      "        [0.5414],\n",
      "        [0.5190]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6264],\n",
      "        [0.5287],\n",
      "        [0.4933],\n",
      "        [0.5118],\n",
      "        [0.4696]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6132],\n",
      "        [0.4727],\n",
      "        [0.5421],\n",
      "        [0.6637],\n",
      "        [0.5668]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5119],\n",
      "        [0.5377],\n",
      "        [0.5251],\n",
      "        [0.6277],\n",
      "        [0.5280]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5066],\n",
      "        [0.5056],\n",
      "        [0.6103],\n",
      "        [0.4826],\n",
      "        [0.5633]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6132],\n",
      "        [0.5125],\n",
      "        [0.5067],\n",
      "        [0.4915],\n",
      "        [0.5998]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4833],\n",
      "        [0.4597],\n",
      "        [0.5785],\n",
      "        [0.6502],\n",
      "        [0.5253]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5381],\n",
      "        [0.4232],\n",
      "        [0.4630],\n",
      "        [0.5998],\n",
      "        [0.4559]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5346],\n",
      "        [0.3426],\n",
      "        [0.4489],\n",
      "        [0.5698],\n",
      "        [0.4982]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5551],\n",
      "        [0.5423],\n",
      "        [0.4290],\n",
      "        [0.3583],\n",
      "        [0.4988]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5232],\n",
      "        [0.6017],\n",
      "        [0.5175],\n",
      "        [0.5042],\n",
      "        [0.5680]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5222],\n",
      "        [0.4533],\n",
      "        [0.4849],\n",
      "        [0.5051],\n",
      "        [0.5662]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4992],\n",
      "        [0.4864],\n",
      "        [0.5762],\n",
      "        [0.5190],\n",
      "        [0.4694]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5063],\n",
      "        [0.5075],\n",
      "        [0.5199],\n",
      "        [0.5802],\n",
      "        [0.4091]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "tensor([[0.4708],\n",
      "        [0.4703],\n",
      "        [0.5583],\n",
      "        [0.6389],\n",
      "        [0.5108]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 0.])\n",
      "Epoch [17/20], Loss: 0.7516\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4760],\n",
      "        [0.5825],\n",
      "        [0.3628],\n",
      "        [0.3803],\n",
      "        [0.3868]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5947],\n",
      "        [0.5156],\n",
      "        [0.4257],\n",
      "        [0.5732],\n",
      "        [0.5572]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4737],\n",
      "        [0.6351],\n",
      "        [0.5654],\n",
      "        [0.4424],\n",
      "        [0.5594]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5460],\n",
      "        [0.5697],\n",
      "        [0.5130],\n",
      "        [0.5212],\n",
      "        [0.5108]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5017],\n",
      "        [0.5362],\n",
      "        [0.5326],\n",
      "        [0.5017],\n",
      "        [0.5648]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5174],\n",
      "        [0.4383],\n",
      "        [0.5401],\n",
      "        [0.4525],\n",
      "        [0.5219]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5161],\n",
      "        [0.3719],\n",
      "        [0.5797],\n",
      "        [0.4989],\n",
      "        [0.4167]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5012],\n",
      "        [0.6165],\n",
      "        [0.4866],\n",
      "        [0.5038],\n",
      "        [0.5586]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4300],\n",
      "        [0.5648],\n",
      "        [0.6062],\n",
      "        [0.5636],\n",
      "        [0.5290]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4704],\n",
      "        [0.4684],\n",
      "        [0.5274],\n",
      "        [0.4046],\n",
      "        [0.5193]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5556],\n",
      "        [0.5075],\n",
      "        [0.4974],\n",
      "        [0.4874],\n",
      "        [0.3360]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5099],\n",
      "        [0.4766],\n",
      "        [0.4545],\n",
      "        [0.4938],\n",
      "        [0.5080]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6013],\n",
      "        [0.4231],\n",
      "        [0.4112],\n",
      "        [0.4301],\n",
      "        [0.4870]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5747],\n",
      "        [0.5899],\n",
      "        [0.5773],\n",
      "        [0.5528],\n",
      "        [0.4805]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4950],\n",
      "        [0.4728],\n",
      "        [0.5244],\n",
      "        [0.7169],\n",
      "        [0.4993]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4611],\n",
      "        [0.4277],\n",
      "        [0.4648],\n",
      "        [0.5182],\n",
      "        [0.5606]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4072],\n",
      "        [0.5374],\n",
      "        [0.5697],\n",
      "        [0.5171],\n",
      "        [0.5735]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4850],\n",
      "        [0.5643],\n",
      "        [0.5663],\n",
      "        [0.4844],\n",
      "        [0.4147]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5572],\n",
      "        [0.6215],\n",
      "        [0.5757],\n",
      "        [0.4924],\n",
      "        [0.5142]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4837],\n",
      "        [0.5600],\n",
      "        [0.4458],\n",
      "        [0.5865],\n",
      "        [0.4894]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.3956],\n",
      "        [0.3208],\n",
      "        [0.5131],\n",
      "        [0.6115],\n",
      "        [0.4069]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5416],\n",
      "        [0.5607],\n",
      "        [0.5738],\n",
      "        [0.3983],\n",
      "        [0.3785]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.3693],\n",
      "        [0.5829],\n",
      "        [0.4768],\n",
      "        [0.5254],\n",
      "        [0.5840]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5826],\n",
      "        [0.5754],\n",
      "        [0.5374],\n",
      "        [0.4891],\n",
      "        [0.4656]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5209],\n",
      "        [0.4828],\n",
      "        [0.5320],\n",
      "        [0.4985],\n",
      "        [0.4445]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5080],\n",
      "        [0.4517],\n",
      "        [0.5137],\n",
      "        [0.5462],\n",
      "        [0.4730]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5946],\n",
      "        [0.4577],\n",
      "        [0.5301],\n",
      "        [0.4826],\n",
      "        [0.5528]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5488],\n",
      "        [0.5353],\n",
      "        [0.5273],\n",
      "        [0.5171],\n",
      "        [0.5189]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5216],\n",
      "        [0.4280],\n",
      "        [0.5297],\n",
      "        [0.4620],\n",
      "        [0.4479]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4761],\n",
      "        [0.4686],\n",
      "        [0.5511],\n",
      "        [0.5504],\n",
      "        [0.4640]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4374],\n",
      "        [0.5783],\n",
      "        [0.6162],\n",
      "        [0.5264],\n",
      "        [0.5538]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 0.])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "tensor([[0.5820],\n",
      "        [0.3477],\n",
      "        [0.5824],\n",
      "        [0.3406],\n",
      "        [0.5796]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "Epoch [18/20], Loss: 0.5279\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5518],\n",
      "        [0.4289],\n",
      "        [0.5198],\n",
      "        [0.6028],\n",
      "        [0.6604]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4769],\n",
      "        [0.5287],\n",
      "        [0.5347],\n",
      "        [0.4635],\n",
      "        [0.4451]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5541],\n",
      "        [0.6259],\n",
      "        [0.5265],\n",
      "        [0.6583],\n",
      "        [0.4004]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5840],\n",
      "        [0.5011],\n",
      "        [0.4872],\n",
      "        [0.4670],\n",
      "        [0.5443]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5763],\n",
      "        [0.5337],\n",
      "        [0.4580],\n",
      "        [0.4900],\n",
      "        [0.5221]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4727],\n",
      "        [0.5211],\n",
      "        [0.5807],\n",
      "        [0.6068],\n",
      "        [0.4577]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5690],\n",
      "        [0.5723],\n",
      "        [0.5521],\n",
      "        [0.5782],\n",
      "        [0.4525]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4991],\n",
      "        [0.3253],\n",
      "        [0.5117],\n",
      "        [0.6030],\n",
      "        [0.4687]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4297],\n",
      "        [0.3305],\n",
      "        [0.5204],\n",
      "        [0.4187],\n",
      "        [0.5061]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5067],\n",
      "        [0.5324],\n",
      "        [0.5758],\n",
      "        [0.4173],\n",
      "        [0.5736]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5446],\n",
      "        [0.4323],\n",
      "        [0.4722],\n",
      "        [0.4288],\n",
      "        [0.4573]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6234],\n",
      "        [0.5575],\n",
      "        [0.5458],\n",
      "        [0.3966],\n",
      "        [0.5095]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5046],\n",
      "        [0.4794],\n",
      "        [0.5450],\n",
      "        [0.4779],\n",
      "        [0.4784]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6394],\n",
      "        [0.5496],\n",
      "        [0.5601],\n",
      "        [0.5056],\n",
      "        [0.5406]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4825],\n",
      "        [0.6232],\n",
      "        [0.5249],\n",
      "        [0.5396],\n",
      "        [0.4159]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.3371],\n",
      "        [0.5648],\n",
      "        [0.4447],\n",
      "        [0.4343],\n",
      "        [0.5440]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6110],\n",
      "        [0.5088],\n",
      "        [0.5048],\n",
      "        [0.5079],\n",
      "        [0.4516]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4686],\n",
      "        [0.5187],\n",
      "        [0.4895],\n",
      "        [0.4878],\n",
      "        [0.5270]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6249],\n",
      "        [0.5953],\n",
      "        [0.3413],\n",
      "        [0.5305],\n",
      "        [0.4574]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4960],\n",
      "        [0.4870],\n",
      "        [0.5203],\n",
      "        [0.5499],\n",
      "        [0.5507]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4764],\n",
      "        [0.5064],\n",
      "        [0.5513],\n",
      "        [0.5125],\n",
      "        [0.5936]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5795],\n",
      "        [0.5833],\n",
      "        [0.5339],\n",
      "        [0.4991],\n",
      "        [0.4560]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4635],\n",
      "        [0.4959],\n",
      "        [0.4980],\n",
      "        [0.5101],\n",
      "        [0.4055]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6255],\n",
      "        [0.3664],\n",
      "        [0.6363],\n",
      "        [0.4479],\n",
      "        [0.5659]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5222],\n",
      "        [0.5365],\n",
      "        [0.5694],\n",
      "        [0.6289],\n",
      "        [0.4610]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.3919],\n",
      "        [0.5229],\n",
      "        [0.4764],\n",
      "        [0.5355],\n",
      "        [0.5371]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5145],\n",
      "        [0.4746],\n",
      "        [0.5751],\n",
      "        [0.5356],\n",
      "        [0.6198]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4305],\n",
      "        [0.5038],\n",
      "        [0.6031],\n",
      "        [0.6668],\n",
      "        [0.5707]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4713],\n",
      "        [0.4740],\n",
      "        [0.4789],\n",
      "        [0.5459],\n",
      "        [0.5799]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5408],\n",
      "        [0.5047],\n",
      "        [0.5580],\n",
      "        [0.5346],\n",
      "        [0.5513]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5790],\n",
      "        [0.5424],\n",
      "        [0.4880],\n",
      "        [0.5141],\n",
      "        [0.3684]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 1., 0.])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "tensor([[0.5146],\n",
      "        [0.5887],\n",
      "        [0.5268],\n",
      "        [0.5116],\n",
      "        [0.5083]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 1.])\n",
      "Epoch [19/20], Loss: 0.7490\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6223],\n",
      "        [0.4359],\n",
      "        [0.4410],\n",
      "        [0.4567],\n",
      "        [0.3896]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5064],\n",
      "        [0.4543],\n",
      "        [0.5239],\n",
      "        [0.4915],\n",
      "        [0.5200]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5772],\n",
      "        [0.5099],\n",
      "        [0.5434],\n",
      "        [0.4972],\n",
      "        [0.3044]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6111],\n",
      "        [0.5007],\n",
      "        [0.4683],\n",
      "        [0.5873],\n",
      "        [0.5100]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4316],\n",
      "        [0.6093],\n",
      "        [0.5742],\n",
      "        [0.5900],\n",
      "        [0.6124]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4166],\n",
      "        [0.5041],\n",
      "        [0.4130],\n",
      "        [0.6632],\n",
      "        [0.6082]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4993],\n",
      "        [0.6240],\n",
      "        [0.5047],\n",
      "        [0.5211],\n",
      "        [0.4857]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6454],\n",
      "        [0.4983],\n",
      "        [0.5423],\n",
      "        [0.5444],\n",
      "        [0.3319]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4837],\n",
      "        [0.5674],\n",
      "        [0.4850],\n",
      "        [0.5729],\n",
      "        [0.5618]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4897],\n",
      "        [0.3705],\n",
      "        [0.5804],\n",
      "        [0.5680],\n",
      "        [0.5254]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5586],\n",
      "        [0.5505],\n",
      "        [0.5437],\n",
      "        [0.5681],\n",
      "        [0.5342]], grad_fn=<SliceBackward0>) tensor([0., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4572],\n",
      "        [0.4750],\n",
      "        [0.5804],\n",
      "        [0.5754],\n",
      "        [0.5589]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6271],\n",
      "        [0.4810],\n",
      "        [0.6296],\n",
      "        [0.5074],\n",
      "        [0.5644]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5570],\n",
      "        [0.4120],\n",
      "        [0.5805],\n",
      "        [0.5030],\n",
      "        [0.4243]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5540],\n",
      "        [0.5184],\n",
      "        [0.5466],\n",
      "        [0.4482],\n",
      "        [0.5864]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5132],\n",
      "        [0.5815],\n",
      "        [0.5393],\n",
      "        [0.4882],\n",
      "        [0.4746]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5900],\n",
      "        [0.5679],\n",
      "        [0.4882],\n",
      "        [0.6322],\n",
      "        [0.5203]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4638],\n",
      "        [0.6151],\n",
      "        [0.5181],\n",
      "        [0.5344],\n",
      "        [0.5809]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4632],\n",
      "        [0.5460],\n",
      "        [0.5063],\n",
      "        [0.5288],\n",
      "        [0.4800]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.6586],\n",
      "        [0.5426],\n",
      "        [0.6077],\n",
      "        [0.5583],\n",
      "        [0.3714]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4810],\n",
      "        [0.3640],\n",
      "        [0.4935],\n",
      "        [0.4061],\n",
      "        [0.5097]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5415],\n",
      "        [0.5784],\n",
      "        [0.5297],\n",
      "        [0.5406],\n",
      "        [0.4694]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4537],\n",
      "        [0.4909],\n",
      "        [0.4296],\n",
      "        [0.5192],\n",
      "        [0.6754]], grad_fn=<SliceBackward0>) tensor([1., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5347],\n",
      "        [0.5857],\n",
      "        [0.5975],\n",
      "        [0.4838],\n",
      "        [0.5922]], grad_fn=<SliceBackward0>) tensor([0., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5436],\n",
      "        [0.5016],\n",
      "        [0.4399],\n",
      "        [0.4915],\n",
      "        [0.5475]], grad_fn=<SliceBackward0>) tensor([1., 1., 0., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5503],\n",
      "        [0.5332],\n",
      "        [0.3969],\n",
      "        [0.5984],\n",
      "        [0.5110]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 1., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5288],\n",
      "        [0.5207],\n",
      "        [0.4897],\n",
      "        [0.5603],\n",
      "        [0.5286]], grad_fn=<SliceBackward0>) tensor([1., 0., 0., 0., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.2757],\n",
      "        [0.5033],\n",
      "        [0.5183],\n",
      "        [0.4789],\n",
      "        [0.4943]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 1., 0.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.4060],\n",
      "        [0.4686],\n",
      "        [0.5798],\n",
      "        [0.4334],\n",
      "        [0.6044]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5859],\n",
      "        [0.4639],\n",
      "        [0.5322],\n",
      "        [0.5424],\n",
      "        [0.5616]], grad_fn=<SliceBackward0>) tensor([0., 0., 1., 0., 1.])\n",
      "torch.Size([32, 1]) torch.Size([32])\n",
      "tensor([[0.5021],\n",
      "        [0.4948],\n",
      "        [0.4975],\n",
      "        [0.4709],\n",
      "        [0.5972]], grad_fn=<SliceBackward0>) tensor([1., 1., 1., 0., 0.])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "tensor([[0.5614],\n",
      "        [0.5879],\n",
      "        [0.4960],\n",
      "        [0.4081],\n",
      "        [0.4636]], grad_fn=<SliceBackward0>) tensor([0., 1., 0., 0., 0.])\n",
      "Epoch [20/20], Loss: 0.6254\n",
      "Accuracy: 59.20%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Generate synthetic data\n",
    "torch.manual_seed(0)\n",
    "X = torch.randn(1000, 20)  # 1000 samples, 20 features each\n",
    "y = (torch.rand(1000) > 0.5).float()  # Binary labels\n",
    "\n",
    "# Create a DataLoader\n",
    "dataset = TensorDataset(X, y)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the model\n",
    "class BinaryClassificationModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BinaryClassificationModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 10)\n",
    "        self.layer2 = nn.Linear(10, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = BinaryClassificationModel(input_dim=20)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in data_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        print(outputs.shape,labels.shape)\n",
    "        print(outputs[:5], labels[:5])\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    outputs = model(X)\n",
    "    predicted = (outputs.squeeze() > 0.5).float()\n",
    "    accuracy = (predicted == y).sum().item() / y.size(0)\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = []\n",
    "q.append(1)\n",
    "q.append(2)\n",
    "q.append(3)\n",
    "q.pop(0)\n",
    "q.append(4)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 50, 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_len, test_len = int(len(dataset)/10), int(len(dataset)/10)\n",
    "train_len = len(dataset)-dev_len-test_len\n",
    "train_len, dev_len, test_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-01-01 : 5\n",
      "2014-01-02 : 1\n",
      "2014-01-03 : 2\n",
      "2014-01-04 : 1\n",
      "2014-01-05 : 2\n",
      "2014-01-06 : 3\n",
      "2014-01-07 : 2\n",
      "2014-01-08 : 3\n",
      "2014-01-09 : 2\n",
      "2014-01-10 : 1\n",
      "2014-01-11 : 1\n",
      "2014-01-12 : 1\n",
      "2014-01-13 : 4\n",
      "2014-01-14 : 7\n",
      "2014-01-15 : 2\n",
      "2014-01-16 : 4\n",
      "2014-01-19 : 1\n",
      "2014-01-20 : 1\n",
      "2014-01-21 : 5\n",
      "2014-01-22 : 2\n",
      "2014-01-23 : 3\n",
      "2014-01-24 : 4\n",
      "2014-01-25 : 1\n",
      "2014-01-26 : 2\n",
      "2014-01-27 : 5\n",
      "2014-01-28 : 2\n",
      "2014-01-29 : 4\n",
      "2014-01-30 : 4\n",
      "2014-01-31 : 6\n",
      "2014-02-01 : 1\n",
      "2014-02-02 : 4\n",
      "2014-02-03 : 4\n",
      "2014-02-04 : 2\n",
      "2014-02-05 : 4\n",
      "2014-02-06 : 4\n",
      "2014-02-07 : 9\n",
      "2014-02-08 : 5\n",
      "2014-02-09 : 5\n",
      "2014-02-10 : 5\n",
      "2014-02-11 : 6\n",
      "2014-02-12 : 4\n",
      "2014-02-13 : 4\n",
      "2014-02-14 : 3\n",
      "2014-02-15 : 3\n",
      "2014-02-16 : 3\n",
      "2014-02-17 : 1\n",
      "2014-02-18 : 1\n",
      "2014-02-19 : 2\n",
      "2014-02-20 : 7\n",
      "2014-02-21 : 5\n",
      "2014-02-22 : 11\n",
      "2014-02-23 : 2\n",
      "2014-02-24 : 2\n",
      "2014-02-25 : 7\n",
      "2014-02-26 : 6\n",
      "2014-02-27 : 7\n",
      "2014-02-28 : 2\n",
      "2014-03-01 : 1\n",
      "2014-03-02 : 1\n",
      "2014-03-03 : 1\n",
      "2014-03-04 : 3\n",
      "2014-03-05 : 4\n",
      "2014-03-06 : 2\n",
      "2014-03-07 : 1\n",
      "2014-03-08 : 1\n",
      "2014-03-09 : 1\n",
      "2014-03-10 : 3\n",
      "2014-03-11 : 3\n",
      "2014-03-12 : 4\n",
      "2014-03-13 : 3\n",
      "2014-03-14 : 1\n",
      "2014-03-15 : 1\n",
      "2014-03-16 : 1\n",
      "2014-03-17 : 1\n",
      "2014-03-18 : 14\n",
      "2014-03-19 : 6\n",
      "2014-03-20 : 5\n",
      "2014-03-21 : 1\n",
      "2014-03-22 : 1\n",
      "2014-03-23 : 1\n",
      "2014-03-24 : 4\n",
      "2014-03-25 : 3\n",
      "2014-03-26 : 2\n",
      "2014-03-27 : 2\n",
      "2014-03-28 : 2\n",
      "2014-03-29 : 1\n",
      "2014-03-30 : 2\n",
      "2014-03-31 : 5\n",
      "2014-04-01 : 4\n",
      "2014-04-02 : 14\n",
      "2014-04-03 : 6\n",
      "2014-04-04 : 3\n",
      "2014-04-05 : 2\n",
      "2014-04-06 : 3\n",
      "2014-04-07 : 6\n",
      "2014-04-08 : 3\n",
      "2014-04-09 : 3\n",
      "2014-04-10 : 11\n",
      "2014-04-11 : 14\n",
      "2014-04-12 : 4\n",
      "2014-04-13 : 2\n",
      "2014-04-14 : 3\n",
      "2014-04-15 : 3\n",
      "2014-04-16 : 2\n",
      "2014-04-17 : 6\n",
      "2014-04-18 : 2\n",
      "2014-04-19 : 1\n",
      "2014-04-20 : 1\n",
      "2014-04-21 : 3\n",
      "2014-04-22 : 5\n",
      "2014-04-23 : 9\n",
      "2014-04-24 : 12\n",
      "2014-04-25 : 14\n",
      "2014-04-26 : 12\n",
      "2014-04-27 : 1\n",
      "2014-04-28 : 18\n",
      "2014-04-29 : 5\n",
      "2014-04-30 : 7\n",
      "2014-05-01 : 2\n",
      "2014-05-02 : 2\n",
      "2014-05-03 : 1\n",
      "2014-05-04 : 1\n",
      "2014-05-05 : 2\n",
      "2014-05-06 : 7\n",
      "2014-05-07 : 3\n",
      "2014-05-08 : 1\n",
      "2014-05-09 : 2\n",
      "2014-05-10 : 1\n",
      "2014-05-11 : 1\n",
      "2014-05-12 : 2\n",
      "2014-05-13 : 3\n",
      "2014-05-14 : 9\n",
      "2014-05-15 : 1\n",
      "2014-05-16 : 6\n",
      "2014-05-17 : 3\n",
      "2014-05-18 : 13\n",
      "2014-05-19 : 4\n",
      "2014-05-20 : 5\n",
      "2014-05-21 : 532\n",
      "2014-05-22 : 75\n",
      "2014-05-23 : 2\n",
      "2014-05-24 : 1\n",
      "2014-05-25 : 1\n",
      "2014-05-26 : 1\n",
      "2014-05-27 : 30\n",
      "2014-05-28 : 5\n",
      "2014-05-29 : 8\n",
      "2014-05-30 : 36\n",
      "2014-05-31 : 1\n",
      "2014-06-01 : 1\n",
      "2014-06-02 : 60\n",
      "2014-06-03 : 39\n",
      "2014-06-04 : 244\n",
      "2014-06-05 : 29\n",
      "2014-06-06 : 3\n",
      "2014-06-07 : 2\n",
      "2014-06-08 : 1\n",
      "2014-06-09 : 2\n",
      "2014-06-10 : 2\n",
      "2014-06-11 : 42\n",
      "2014-06-12 : 3\n",
      "2014-06-13 : 5\n",
      "2014-06-14 : 1\n",
      "2014-06-15 : 1\n",
      "2014-06-16 : 8\n",
      "2014-06-18 : 21\n",
      "2014-06-19 : 11\n",
      "2014-06-20 : 1\n",
      "2014-06-21 : 1\n",
      "2014-06-22 : 1\n",
      "2014-06-23 : 26\n",
      "2014-06-24 : 18\n",
      "2014-06-25 : 12\n",
      "2014-06-26 : 4\n",
      "2014-06-27 : 3\n",
      "2014-06-28 : 1\n",
      "2014-06-29 : 1\n",
      "2014-06-30 : 2\n",
      "2014-07-01 : 1\n",
      "2014-07-02 : 2\n",
      "2014-07-03 : 3\n",
      "2014-07-04 : 1\n",
      "2014-07-05 : 2\n",
      "2014-07-06 : 2\n",
      "2014-07-07 : 2\n",
      "2014-07-08 : 4\n",
      "2014-07-09 : 2\n",
      "2014-07-10 : 64\n",
      "2014-07-11 : 3\n",
      "2014-07-12 : 1\n",
      "2014-07-13 : 2\n",
      "2014-07-14 : 3\n",
      "2014-07-15 : 3\n",
      "2014-07-16 : 4\n",
      "2014-07-17 : 15\n",
      "2014-07-18 : 4\n",
      "2014-07-19 : 1\n",
      "2014-07-20 : 3\n",
      "2014-07-21 : 5\n",
      "2014-07-22 : 3\n",
      "2014-07-23 : 25\n",
      "2014-07-24 : 15\n",
      "2014-07-25 : 3\n",
      "2014-07-26 : 1\n",
      "2014-07-27 : 1\n",
      "2014-07-28 : 3\n",
      "2014-07-29 : 1\n",
      "2014-07-30 : 49\n",
      "2014-07-31 : 18\n",
      "2014-08-01 : 3\n",
      "2014-08-02 : 2\n",
      "2014-08-03 : 1\n",
      "2014-08-04 : 6\n",
      "2014-08-05 : 1\n",
      "2014-08-07 : 2\n",
      "2014-08-08 : 2\n",
      "2014-08-09 : 1\n",
      "2014-08-10 : 1\n",
      "2014-08-11 : 2\n",
      "2014-08-12 : 2\n",
      "2014-08-13 : 1\n",
      "2014-08-14 : 1\n",
      "2014-08-15 : 3\n",
      "2014-08-16 : 1\n",
      "2014-08-17 : 2\n",
      "2014-08-18 : 2\n",
      "2014-08-19 : 1\n",
      "2014-08-20 : 1\n",
      "2014-08-21 : 2\n",
      "2014-08-22 : 2\n",
      "2014-08-23 : 1\n",
      "2014-08-24 : 2\n",
      "2014-08-25 : 3\n",
      "2014-08-26 : 1\n",
      "2014-08-27 : 2\n",
      "2014-08-28 : 2\n",
      "2014-08-29 : 1\n",
      "2014-08-30 : 2\n",
      "2014-08-31 : 2\n",
      "2014-09-01 : 14\n",
      "2014-09-02 : 7\n",
      "2014-09-03 : 7\n",
      "2014-09-04 : 1\n",
      "2014-09-05 : 1\n",
      "2014-09-06 : 1\n",
      "2014-09-07 : 1\n",
      "2014-09-08 : 5\n",
      "2014-09-09 : 17\n",
      "2014-09-10 : 14\n",
      "2014-09-11 : 1\n",
      "2014-09-12 : 4\n",
      "2014-09-13 : 1\n",
      "2014-09-14 : 1\n",
      "2014-09-15 : 2\n",
      "2014-09-16 : 2\n",
      "2014-09-17 : 5\n",
      "2014-09-18 : 1\n",
      "2014-09-19 : 3\n",
      "2014-09-20 : 2\n",
      "2014-09-21 : 2\n",
      "2014-09-22 : 15\n",
      "2014-09-23 : 6\n",
      "2014-09-29 : 1\n",
      "2014-09-30 : 1\n",
      "2014-10-01 : 3\n",
      "2014-10-02 : 3\n",
      "2014-10-03 : 1\n",
      "2014-10-04 : 1\n",
      "2014-10-05 : 2\n",
      "2014-10-06 : 1\n",
      "2014-10-07 : 1\n",
      "2014-10-08 : 2\n",
      "2014-10-09 : 1\n",
      "2014-10-10 : 3\n",
      "2014-10-11 : 1\n",
      "2014-10-12 : 1\n",
      "2014-10-13 : 2\n",
      "2014-10-14 : 3\n",
      "2014-10-15 : 2\n",
      "2014-10-16 : 4\n",
      "2014-10-17 : 2\n",
      "2014-10-18 : 1\n",
      "2014-10-19 : 1\n",
      "2014-10-20 : 2\n",
      "2014-10-21 : 30\n",
      "2014-10-22 : 13\n",
      "2014-10-23 : 1\n",
      "2014-10-24 : 2\n",
      "2014-10-25 : 13\n",
      "2014-10-26 : 40\n",
      "2014-10-27 : 4\n",
      "2014-10-28 : 36\n",
      "2014-10-29 : 2\n",
      "2014-10-30 : 2\n",
      "2014-10-31 : 2\n",
      "2014-11-01 : 3\n",
      "2014-11-02 : 2\n",
      "2014-11-03 : 2\n",
      "2014-11-04 : 2\n",
      "2014-11-05 : 3\n",
      "2014-11-06 : 5\n",
      "2014-11-07 : 1\n",
      "2014-11-08 : 1\n",
      "2014-11-09 : 1\n",
      "2014-11-14 : 1\n",
      "2014-11-23 : 1\n",
      "2014-11-24 : 3\n",
      "2014-11-25 : 7\n",
      "2014-11-26 : 3\n",
      "2014-11-27 : 3\n",
      "2014-11-28 : 1\n",
      "2014-11-29 : 1\n",
      "2014-11-30 : 1\n",
      "2014-12-01 : 2\n",
      "2014-12-02 : 12\n",
      "2014-12-03 : 2\n",
      "2014-12-04 : 5\n",
      "2014-12-05 : 3\n",
      "2014-12-06 : 9\n",
      "2014-12-07 : 3\n",
      "2014-12-08 : 17\n",
      "2014-12-09 : 9\n",
      "2014-12-10 : 15\n",
      "2014-12-11 : 8\n",
      "2014-12-12 : 12\n",
      "2014-12-13 : 3\n",
      "2014-12-14 : 1\n",
      "2014-12-15 : 13\n",
      "2014-12-16 : 27\n",
      "2014-12-17 : 8\n",
      "2014-12-18 : 3\n",
      "2014-12-19 : 3\n",
      "2014-12-21 : 6\n",
      "2014-12-22 : 7\n",
      "2014-12-23 : 8\n",
      "2014-12-24 : 2\n",
      "2014-12-25 : 1\n",
      "2014-12-26 : 3\n",
      "2014-12-27 : 10\n",
      "2014-12-28 : 4\n",
      "2014-12-29 : 6\n",
      "2014-12-30 : 14\n",
      "2014-12-31 : 9\n",
      "2015-01-01 : 7\n",
      "2015-01-02 : 11\n",
      "2015-01-03 : 47\n",
      "2015-01-04 : 21\n",
      "2015-01-05 : 4\n",
      "2015-01-06 : 2\n",
      "2015-01-07 : 31\n",
      "2015-01-08 : 12\n",
      "2015-01-09 : 45\n",
      "2015-01-10 : 3\n",
      "2015-01-11 : 12\n",
      "2015-01-12 : 3\n",
      "2015-01-13 : 4\n",
      "2015-01-14 : 2\n",
      "2015-01-15 : 3\n",
      "2015-01-16 : 13\n",
      "2015-01-17 : 1\n",
      "2015-01-18 : 1\n",
      "2015-01-19 : 1\n",
      "2015-01-20 : 4\n",
      "2015-01-21 : 1\n",
      "2015-01-22 : 6\n",
      "2015-01-23 : 10\n",
      "2015-01-24 : 1\n",
      "2015-01-25 : 2\n",
      "2015-01-26 : 4\n",
      "2015-01-27 : 5\n",
      "2015-01-28 : 3\n",
      "2015-01-29 : 2\n",
      "2015-01-30 : 3\n",
      "2015-01-31 : 4\n",
      "2015-02-01 : 4\n",
      "2015-02-02 : 2\n",
      "2015-02-03 : 4\n",
      "2015-02-04 : 3\n",
      "2015-02-05 : 6\n",
      "2015-02-06 : 5\n",
      "2015-02-07 : 1\n",
      "2015-02-08 : 1\n",
      "2015-02-09 : 3\n",
      "2015-02-10 : 3\n",
      "2015-02-11 : 5\n",
      "2015-02-12 : 10\n",
      "2015-02-13 : 2\n",
      "2015-02-14 : 2\n",
      "2015-02-15 : 4\n",
      "2015-02-16 : 4\n",
      "2015-02-17 : 4\n",
      "2015-02-18 : 2\n",
      "2015-02-19 : 2\n",
      "2015-02-20 : 2\n",
      "2015-02-21 : 12\n",
      "2015-02-22 : 1\n",
      "2015-02-23 : 3\n",
      "2015-02-24 : 2\n",
      "2015-02-25 : 6\n",
      "2015-02-26 : 1\n",
      "2015-02-27 : 3\n",
      "2015-02-28 : 4\n",
      "2015-03-01 : 1\n",
      "2015-03-02 : 2\n",
      "2015-03-03 : 2\n",
      "2015-03-04 : 2\n",
      "2015-03-05 : 5\n",
      "2015-03-06 : 13\n",
      "2015-03-07 : 1\n",
      "2015-03-09 : 2\n",
      "2015-03-10 : 2\n",
      "2015-03-11 : 2\n",
      "2015-03-12 : 3\n",
      "2015-03-13 : 2\n",
      "2015-03-14 : 3\n",
      "2015-03-15 : 3\n",
      "2015-03-16 : 3\n",
      "2015-03-17 : 4\n",
      "2015-03-18 : 2\n",
      "2015-03-19 : 6\n",
      "2015-03-20 : 4\n",
      "2015-03-21 : 7\n",
      "2015-03-22 : 2\n",
      "2015-03-23 : 2\n",
      "2015-03-24 : 2\n",
      "2015-03-25 : 5\n",
      "2015-03-26 : 4\n",
      "2015-03-27 : 2\n",
      "2015-03-28 : 3\n",
      "2015-03-29 : 2\n",
      "2015-03-30 : 5\n",
      "2015-03-31 : 5\n",
      "2015-04-01 : 13\n",
      "2015-04-02 : 8\n",
      "2015-04-03 : 26\n",
      "2015-04-04 : 55\n",
      "2015-04-05 : 50\n",
      "2015-04-06 : 10\n",
      "2015-04-07 : 2\n",
      "2015-04-08 : 2\n",
      "2015-04-09 : 3\n",
      "2015-04-10 : 6\n",
      "2015-04-11 : 4\n",
      "2015-04-12 : 4\n",
      "2015-04-13 : 19\n",
      "2015-04-14 : 5\n",
      "2015-04-15 : 5\n",
      "2015-04-16 : 5\n",
      "2015-04-17 : 3\n",
      "2015-04-18 : 2\n",
      "2015-04-19 : 7\n",
      "2015-04-20 : 3\n",
      "2015-04-21 : 2\n",
      "2015-04-22 : 6\n",
      "2015-04-23 : 2\n",
      "2015-04-24 : 5\n",
      "2015-04-25 : 7\n",
      "2015-04-26 : 2\n",
      "2015-04-27 : 5\n",
      "2015-04-28 : 5\n",
      "2015-04-29 : 1\n",
      "2015-04-30 : 2\n",
      "2015-05-01 : 7\n",
      "2015-05-02 : 3\n",
      "2015-05-03 : 2\n",
      "2015-05-04 : 2\n",
      "2015-05-05 : 5\n",
      "2015-05-06 : 2\n",
      "2015-05-07 : 5\n",
      "2015-05-08 : 2\n",
      "2015-05-09 : 2\n",
      "2015-05-10 : 1\n",
      "2015-05-11 : 3\n",
      "2015-05-12 : 5\n",
      "2015-05-13 : 3\n",
      "2015-05-14 : 5\n",
      "2015-05-15 : 12\n",
      "2015-05-16 : 4\n",
      "2015-05-17 : 3\n",
      "2015-05-18 : 3\n",
      "2015-05-19 : 1\n",
      "2015-05-20 : 6\n",
      "2015-05-21 : 2\n",
      "2015-05-22 : 3\n",
      "2015-05-23 : 2\n",
      "2015-05-24 : 1\n",
      "2015-05-25 : 2\n",
      "2015-05-26 : 4\n",
      "2015-05-27 : 5\n",
      "2015-05-28 : 4\n",
      "2015-05-29 : 3\n",
      "2015-05-30 : 2\n",
      "2015-05-31 : 3\n",
      "2015-06-01 : 2\n",
      "2015-06-02 : 1\n",
      "2015-06-03 : 2\n",
      "2015-06-04 : 2\n",
      "2015-06-06 : 3\n",
      "2015-06-07 : 3\n",
      "2015-06-08 : 3\n",
      "2015-06-09 : 6\n",
      "2015-06-10 : 2\n",
      "2015-06-11 : 2\n",
      "2015-06-12 : 4\n",
      "2015-06-13 : 36\n",
      "2015-06-14 : 3\n",
      "2015-06-15 : 4\n",
      "2015-06-16 : 3\n",
      "2015-06-17 : 3\n",
      "2015-06-18 : 3\n",
      "2015-06-19 : 2\n",
      "2015-06-20 : 3\n",
      "2015-06-21 : 2\n",
      "2015-06-22 : 1\n",
      "2015-06-24 : 2\n",
      "2015-06-25 : 2\n",
      "2015-06-26 : 1\n",
      "2015-06-27 : 2\n",
      "2015-06-28 : 2\n",
      "2015-06-29 : 3\n",
      "2015-06-30 : 1\n",
      "2015-07-01 : 3\n",
      "2015-07-02 : 3\n",
      "2015-07-03 : 2\n",
      "2015-07-04 : 2\n",
      "2015-07-05 : 7\n",
      "2015-07-06 : 14\n",
      "2015-07-07 : 2\n",
      "2015-07-09 : 4\n",
      "2015-07-10 : 5\n",
      "2015-07-11 : 2\n",
      "2015-07-12 : 3\n",
      "2015-07-13 : 4\n",
      "2015-07-14 : 7\n",
      "2015-07-15 : 7\n",
      "2015-07-16 : 10\n",
      "2015-07-17 : 11\n",
      "2015-07-18 : 5\n",
      "2015-07-19 : 5\n",
      "2015-07-20 : 8\n",
      "2015-07-21 : 7\n",
      "2015-07-22 : 9\n",
      "2015-07-23 : 9\n",
      "2015-07-24 : 7\n",
      "2015-07-25 : 2\n",
      "2015-07-26 : 3\n",
      "2015-07-27 : 6\n",
      "2015-07-28 : 10\n",
      "2015-07-29 : 8\n",
      "2015-07-30 : 12\n",
      "2015-07-31 : 4\n",
      "2015-08-01 : 1\n",
      "2015-08-02 : 1\n",
      "2015-08-03 : 5\n",
      "2015-08-04 : 4\n",
      "2015-08-05 : 4\n",
      "2015-08-06 : 6\n",
      "2015-08-07 : 6\n",
      "2015-08-08 : 5\n",
      "2015-08-09 : 1\n",
      "2015-08-10 : 6\n",
      "2015-08-11 : 8\n",
      "2015-08-12 : 16\n",
      "2015-08-13 : 4\n",
      "2015-08-14 : 1\n",
      "2015-08-16 : 9\n",
      "2015-08-17 : 5\n",
      "2015-08-18 : 6\n",
      "2015-08-19 : 5\n",
      "2015-08-20 : 8\n",
      "2015-08-21 : 9\n",
      "2015-08-22 : 1\n",
      "2015-08-23 : 3\n",
      "2015-08-24 : 10\n",
      "2015-08-25 : 3\n",
      "2015-08-26 : 4\n",
      "2015-08-27 : 7\n",
      "2015-08-28 : 1\n",
      "2015-08-29 : 3\n",
      "2015-08-30 : 6\n",
      "2015-08-31 : 2\n",
      "2015-09-01 : 5\n",
      "2015-09-02 : 4\n",
      "2015-09-03 : 1\n",
      "2015-09-04 : 5\n",
      "2015-09-05 : 3\n",
      "2015-09-06 : 3\n",
      "2015-09-07 : 2\n",
      "2015-09-08 : 4\n",
      "2015-09-09 : 23\n",
      "2015-09-10 : 8\n",
      "2015-09-11 : 2\n",
      "2015-09-12 : 2\n",
      "2015-09-13 : 4\n",
      "2015-09-14 : 2\n",
      "2015-09-15 : 5\n",
      "2015-09-16 : 4\n",
      "2015-09-19 : 8\n",
      "2015-09-20 : 3\n",
      "2015-09-21 : 5\n",
      "2015-09-22 : 3\n",
      "2015-09-23 : 4\n",
      "2015-09-24 : 3\n",
      "2015-09-25 : 7\n",
      "2015-09-26 : 3\n",
      "2015-09-27 : 4\n",
      "2015-09-28 : 10\n",
      "2015-09-29 : 6\n",
      "2015-09-30 : 7\n",
      "2015-10-01 : 2\n",
      "2015-10-02 : 6\n",
      "2015-10-03 : 2\n",
      "2015-10-04 : 3\n",
      "2015-10-05 : 3\n",
      "2015-10-06 : 5\n",
      "2015-10-07 : 13\n",
      "2015-10-08 : 5\n",
      "2015-10-09 : 4\n",
      "2015-10-10 : 17\n",
      "2015-10-11 : 3\n",
      "2015-10-12 : 6\n",
      "2015-10-13 : 3\n",
      "2015-10-14 : 7\n",
      "2015-10-15 : 4\n",
      "2015-10-16 : 3\n",
      "2015-10-17 : 5\n",
      "2015-10-18 : 19\n",
      "2015-10-19 : 2\n",
      "2015-10-20 : 2\n",
      "2015-10-21 : 2\n",
      "2015-10-22 : 11\n",
      "2015-10-23 : 16\n",
      "2015-10-24 : 3\n",
      "2015-10-25 : 2\n",
      "2015-10-26 : 22\n",
      "2015-10-27 : 2\n",
      "2015-10-28 : 8\n",
      "2015-10-29 : 8\n",
      "2015-10-30 : 6\n",
      "2015-10-31 : 6\n",
      "2015-11-01 : 2\n",
      "2015-11-02 : 6\n",
      "2015-11-03 : 5\n",
      "2015-11-04 : 10\n",
      "2015-11-05 : 10\n",
      "2015-11-06 : 7\n",
      "2015-11-07 : 5\n",
      "2015-11-08 : 13\n",
      "2015-11-09 : 6\n",
      "2015-11-10 : 2\n",
      "2015-11-11 : 4\n",
      "2015-11-12 : 7\n",
      "2015-11-13 : 5\n",
      "2015-11-14 : 1\n",
      "2015-11-15 : 1\n",
      "2015-11-16 : 16\n",
      "2015-11-17 : 9\n",
      "2015-11-18 : 15\n",
      "2015-11-19 : 10\n",
      "2015-11-20 : 24\n",
      "2015-11-21 : 2\n",
      "2015-11-22 : 2\n",
      "2015-11-23 : 5\n",
      "2015-11-24 : 6\n",
      "2015-11-25 : 4\n",
      "2015-11-26 : 3\n",
      "2015-11-28 : 2\n",
      "2015-11-29 : 7\n",
      "2015-11-30 : 1\n",
      "2015-12-01 : 2\n",
      "2015-12-02 : 12\n",
      "2015-12-03 : 7\n",
      "2015-12-04 : 4\n",
      "2015-12-05 : 1\n",
      "2015-12-06 : 3\n",
      "2015-12-07 : 2\n",
      "2015-12-08 : 3\n",
      "2015-12-09 : 3\n",
      "2015-12-10 : 3\n",
      "2015-12-11 : 6\n",
      "2015-12-12 : 2\n",
      "2015-12-13 : 5\n",
      "2015-12-14 : 6\n",
      "2015-12-15 : 2\n",
      "2015-12-17 : 1\n",
      "2015-12-19 : 1\n",
      "2015-12-20 : 1\n",
      "2015-12-21 : 4\n",
      "2015-12-22 : 14\n",
      "2015-12-23 : 5\n",
      "2015-12-24 : 3\n",
      "2015-12-25 : 2\n",
      "2015-12-26 : 3\n",
      "2015-12-27 : 6\n",
      "2015-12-28 : 3\n",
      "2015-12-29 : 5\n",
      "2015-12-30 : 5\n",
      "2015-12-31 : 7\n"
     ]
    }
   ],
   "source": [
    "edge_of_each_day = dataset.graphgenerator.edge_generator.edge_of_each_day\n",
    "for day in edge_of_each_day:\n",
    "    values = edge_of_each_day[day].values()\n",
    "    try:\n",
    "        print(day, \":\", max(values))\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('goog', 'cvx')\n",
      "('goog', 'intc')\n",
      "('goog', 'aapl')\n",
      "('goog', 'bp')\n",
      "('goog', 'csco')\n",
      "('goog', 'ngg')\n",
      "('goog', 'goog')\n"
     ]
    }
   ],
   "source": [
    "for pair in edge_of_each_day['2014-01-01']:\n",
    "    if pair[0] == 'goog':\n",
    "        print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('msft', 'fb'): 1,\n",
       " ('fb', 'msft'): 1,\n",
       " ('jnj', 'vz'): 1,\n",
       " ('vz', 'jnj'): 1,\n",
       " ('mdt', 'mdt'): 1,\n",
       " ('bud', 'bud'): 1,\n",
       " ('pfe', 'pfe'): 1,\n",
       " ('iep', 'iep'): 1,\n",
       " ('pcg', 'pcg'): 1,\n",
       " ('hon', 'hon'): 1,\n",
       " ('cvx', 'cvx'): 1,\n",
       " ('xom', 'xom'): 1,\n",
       " ('slb', 'slb'): 1,\n",
       " ('ko', 'ko'): 1,\n",
       " ('mo', 'mo'): 1,\n",
       " ('bac', 'bac'): 1,\n",
       " ('v', 'v'): 1,\n",
       " ('ma', 'ma'): 1,\n",
       " ('hsbc', 'hsbc'): 1,\n",
       " ('msft', 'msft'): 1,\n",
       " ('abbv', 'abbv'): 1,\n",
       " ('jnj', 'jnj'): 1,\n",
       " ('dhr', 'dhr'): 1,\n",
       " ('gd', 'gd'): 1,\n",
       " ('duk', 'duk'): 1,\n",
       " ('utx', 'utx'): 1,\n",
       " ('ptr', 'ptr'): 1,\n",
       " ('ppl', 'ppl'): 1,\n",
       " ('bp', 'bp'): 1,\n",
       " ('pep', 'pep'): 1,\n",
       " ('pcln', 'pcln'): 1,\n",
       " ('ngg', 'ngg'): 1,\n",
       " ('celg', 'celg'): 1,\n",
       " ('cmcsa', 'cmcsa'): 1,\n",
       " ('tm', 'tm'): 1,\n",
       " ('tot', 'tot'): 1,\n",
       " ('c', 'c'): 1,\n",
       " ('mcd', 'mcd'): 1,\n",
       " ('chtr', 'chtr'): 1,\n",
       " ('snp', 'snp'): 1,\n",
       " ('bhp', 'bhp'): 1,\n",
       " ('amgn', 'amgn'): 1,\n",
       " ('csco', 'csco'): 1,\n",
       " ('pg', 'pg'): 1,\n",
       " ('dis', 'dis'): 1,\n",
       " ('sny', 'sny'): 1,\n",
       " ('pm', 'pm'): 1,\n",
       " ('sre', 'sre'): 1,\n",
       " ('mmm', 'mmm'): 1,\n",
       " ('mrk', 'mrk'): 1,\n",
       " ('goog', 'goog'): 1,\n",
       " ('orcl', 'orcl'): 1,\n",
       " ('ups', 'ups'): 1,\n",
       " ('ul', 'ul'): 1,\n",
       " ('cat', 'cat'): 1,\n",
       " ('d', 'd'): 1,\n",
       " ('vz', 'vz'): 1,\n",
       " ('t', 't'): 1,\n",
       " ('lmt', 'lmt'): 1,\n",
       " ('so', 'so'): 1,\n",
       " ('wfc', 'wfc'): 1,\n",
       " ('ba', 'ba'): 1,\n",
       " ('un', 'un'): 1,\n",
       " ('aapl', 'aapl'): 1,\n",
       " ('fb', 'fb'): 1,\n",
       " ('intc', 'intc'): 1,\n",
       " ('ge', 'ge'): 1,\n",
       " ('unh', 'unh'): 1,\n",
       " ('amzn', 'amzn'): 1,\n",
       " ('jpm', 'jpm'): 1,\n",
       " ('exc', 'exc'): 1,\n",
       " ('rex', 'rex'): 1,\n",
       " ('wmt', 'wmt'): 1,\n",
       " ('bbl', 'bbl'): 1,\n",
       " ('chl', 'chl'): 1,\n",
       " ('aep', 'aep'): 1,\n",
       " ('nvs', 'nvs'): 1,\n",
       " ('tsm', 'tsm'): 1,\n",
       " ('nee', 'nee'): 1,\n",
       " ('hd', 'hd'): 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_of_each_day['2014-10-04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(3.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(11.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(15.)\n",
      "tensor(1.) tensor(16.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(16.)\n",
      "tensor(1.) tensor(11.)\n",
      "tensor(1.) tensor(11.)\n",
      "tensor(1.) tensor(10.)\n",
      "tensor(1.) tensor(10.)\n",
      "tensor(1.) tensor(11.)\n",
      "tensor(1.) tensor(13.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(13.)\n",
      "tensor(1.) tensor(13.)\n",
      "tensor(1.) tensor(12.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(15.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(12.)\n",
      "tensor(1.) tensor(13.)\n",
      "tensor(1.) tensor(16.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(15.)\n",
      "tensor(1.) tensor(15.)\n",
      "tensor(1.) tensor(16.)\n",
      "tensor(1.) tensor(16.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(32.)\n",
      "tensor(1.) tensor(28.)\n",
      "tensor(1.) tensor(27.)\n",
      "tensor(1.) tensor(27.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(25.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(25.)\n",
      "tensor(1.) tensor(25.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(15.)\n",
      "tensor(1.) tensor(12.)\n",
      "tensor(1.) tensor(12.)\n",
      "tensor(1.) tensor(10.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(15.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(27.)\n",
      "tensor(1.) tensor(30.)\n",
      "tensor(1.) tensor(32.)\n",
      "tensor(1.) tensor(32.)\n",
      "tensor(1.) tensor(30.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(11.)\n",
      "tensor(1.) tensor(12.)\n",
      "tensor(1.) tensor(8.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(12.)\n",
      "tensor(1.) tensor(12.)\n",
      "tensor(1.) tensor(12.)\n",
      "tensor(1.) tensor(11.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(33.)\n",
      "tensor(1.) tensor(35.)\n",
      "tensor(1.) tensor(34.)\n",
      "tensor(1.) tensor(30.)\n",
      "tensor(1.) tensor(30.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(27.)\n",
      "tensor(1.) tensor(29.)\n",
      "tensor(1.) tensor(28.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(31.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(28.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(25.)\n",
      "tensor(1.) tensor(16.)\n",
      "tensor(1.) tensor(15.)\n",
      "tensor(1.) tensor(16.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(15.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(9.)\n",
      "tensor(1.) tensor(5.)\n",
      "tensor(1.) tensor(5.)\n",
      "tensor(1.) tensor(7.)\n",
      "tensor(1.) tensor(7.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(13.)\n",
      "tensor(1.) tensor(13.)\n",
      "tensor(1.) tensor(13.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(28.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(30.)\n",
      "tensor(1.) tensor(30.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(10.)\n",
      "tensor(1.) tensor(11.)\n",
      "tensor(1.) tensor(11.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(4.)\n",
      "tensor(1.) tensor(6.)\n",
      "tensor(1.) tensor(11.)\n",
      "tensor(1.) tensor(12.)\n",
      "tensor(1.) tensor(12.)\n",
      "tensor(1.) tensor(11.)\n",
      "tensor(1.) tensor(8.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(16.)\n",
      "tensor(1.) tensor(16.)\n",
      "tensor(1.) tensor(16.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(4.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(8.)\n",
      "tensor(1.) tensor(11.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(15.)\n",
      "tensor(1.) tensor(15.)\n",
      "tensor(1.) tensor(15.)\n",
      "tensor(1.) tensor(13.)\n",
      "tensor(1.) tensor(13.)\n",
      "tensor(1.) tensor(13.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(15.)\n",
      "tensor(1.) tensor(16.)\n",
      "tensor(1.) tensor(15.)\n",
      "tensor(1.) tensor(13.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(15.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(11.)\n",
      "tensor(1.) tensor(15.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(15.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(13.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(16.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(15.)\n",
      "tensor(1.) tensor(15.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(14.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(30.)\n",
      "tensor(1.) tensor(27.)\n",
      "tensor(1.) tensor(30.)\n",
      "tensor(1.) tensor(34.)\n",
      "tensor(1.) tensor(33.)\n",
      "tensor(1.) tensor(32.)\n",
      "tensor(1.) tensor(31.)\n",
      "tensor(1.) tensor(31.)\n",
      "tensor(1.) tensor(31.)\n",
      "tensor(1.) tensor(32.)\n",
      "tensor(1.) tensor(27.)\n",
      "tensor(1.) tensor(29.)\n",
      "tensor(1.) tensor(30.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(27.)\n",
      "tensor(1.) tensor(31.)\n",
      "tensor(1.) tensor(29.)\n",
      "tensor(1.) tensor(27.)\n",
      "tensor(1.) tensor(27.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(25.)\n",
      "tensor(1.) tensor(16.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(25.)\n",
      "tensor(1.) tensor(27.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(27.)\n",
      "tensor(1.) tensor(27.)\n",
      "tensor(1.) tensor(29.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(13.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(25.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(25.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(25.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(28.)\n",
      "tensor(1.) tensor(31.)\n",
      "tensor(1.) tensor(31.)\n",
      "tensor(1.) tensor(31.)\n",
      "tensor(1.) tensor(28.)\n",
      "tensor(1.) tensor(28.)\n",
      "tensor(1.) tensor(27.)\n",
      "tensor(1.) tensor(28.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(32.)\n",
      "tensor(1.) tensor(33.)\n",
      "tensor(1.) tensor(37.)\n",
      "tensor(1.) tensor(39.)\n",
      "tensor(1.) tensor(38.)\n",
      "tensor(1.) tensor(35.)\n",
      "tensor(1.) tensor(34.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(30.)\n",
      "tensor(1.) tensor(30.)\n",
      "tensor(1.) tensor(30.)\n",
      "tensor(1.) tensor(25.)\n",
      "tensor(1.) tensor(25.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(25.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(29.)\n",
      "tensor(1.) tensor(25.)\n",
      "tensor(1.) tensor(32.)\n",
      "tensor(1.) tensor(32.)\n",
      "tensor(1.) tensor(29.)\n",
      "tensor(1.) tensor(25.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(27.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(25.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(20.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(18.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(25.)\n",
      "tensor(1.) tensor(27.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(26.)\n",
      "tensor(1.) tensor(25.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(29.)\n",
      "tensor(1.) tensor(29.)\n",
      "tensor(1.) tensor(30.)\n",
      "tensor(1.) tensor(30.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(22.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(15.)\n",
      "tensor(1.) tensor(16.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(19.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(23.)\n",
      "tensor(1.) tensor(24.)\n",
      "tensor(1.) tensor(21.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(17.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(1.) tensor(12.)\n",
      "tensor(1.) tensor(15.)\n",
      "tensor(1.) tensor(25.)\n",
      "tensor(1.) tensor(25.)\n",
      "tensor(1.) tensor(27.)\n",
      "tensor(1.) tensor(28.)\n",
      "tensor(1.) tensor(30.)\n",
      "tensor(1.) tensor(27.)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import degree\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    data = dataset[i]\n",
    "    num_degree = degree(data.edge_index[0], num_nodes=data.x.shape[0])\n",
    "    print(min(num_degree), max(num_degree))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(399, 532)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), len(os.listdir(\"/home/yingjie_niu/Year_3/GNN_longterm/data/raw/SPNews/news/preprocessed_daily_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('aapl', 'aapl'),\n",
       "  ('aapl', 'aapl'),\n",
       "  ('aapl', 'aapl'),\n",
       "  ('aapl', 'aapl'),\n",
       "  ('aapl', 'aapl'),\n",
       "  ('aapl', 'aapl'),\n",
       "  ('aapl', 'aapl'),\n",
       "  ('aapl', 'aapl'),\n",
       "  ('aapl', 'aapl'),\n",
       "  ('aapl', 'aapl'),\n",
       "  ('aapl', 'aapl'),\n",
       "  ('aapl', 'aapl')],\n",
       " 12,\n",
       " 272)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "permutations = list(itertools.permutations([\"aapl\",\"aapl\",\"aapl\", \"aapl\"], 2)) \n",
    "permutations, len(permutations), 17*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('VRSK', 'VRSK'): 3,\n",
       " ('VRSK', 'TGT'): 1,\n",
       " ('TGT', 'VRSK'): 1,\n",
       " ('SCHW', 'SCHW'): 4,\n",
       " ('COF', 'BK'): 1,\n",
       " ('COF', 'PNC'): 1,\n",
       " ('COF', 'PEP'): 1,\n",
       " ('COF', 'AIG'): 1,\n",
       " ('COF', 'DIS'): 1,\n",
       " ('COF', 'JNJ'): 1,\n",
       " ('COF', 'CMCSA'): 1,\n",
       " ('COF', 'SCHW'): 1,\n",
       " ('COF', 'MTB'): 1,\n",
       " ('COF', 'MS'): 1,\n",
       " ('COF', 'WFC'): 1,\n",
       " ('BK', 'COF'): 1,\n",
       " ('BK', 'PNC'): 1,\n",
       " ('BK', 'PEP'): 1,\n",
       " ('BK', 'AIG'): 1,\n",
       " ('BK', 'DIS'): 1,\n",
       " ('BK', 'JNJ'): 1,\n",
       " ('BK', 'CMCSA'): 1,\n",
       " ('BK', 'SCHW'): 1,\n",
       " ('BK', 'MTB'): 1,\n",
       " ('BK', 'MS'): 1,\n",
       " ('BK', 'WFC'): 1,\n",
       " ('PNC', 'COF'): 1,\n",
       " ('PNC', 'BK'): 1,\n",
       " ('PNC', 'PEP'): 1,\n",
       " ('PNC', 'AIG'): 1,\n",
       " ('PNC', 'DIS'): 1,\n",
       " ('PNC', 'JNJ'): 1,\n",
       " ('PNC', 'CMCSA'): 1,\n",
       " ('PNC', 'SCHW'): 1,\n",
       " ('PNC', 'MTB'): 1,\n",
       " ('PNC', 'MS'): 1,\n",
       " ('PNC', 'WFC'): 2,\n",
       " ('PEP', 'COF'): 1,\n",
       " ('PEP', 'BK'): 1,\n",
       " ('PEP', 'PNC'): 1,\n",
       " ('PEP', 'AIG'): 1,\n",
       " ('PEP', 'DIS'): 2,\n",
       " ('PEP', 'JNJ'): 1,\n",
       " ('PEP', 'CMCSA'): 1,\n",
       " ('PEP', 'SCHW'): 1,\n",
       " ('PEP', 'MTB'): 1,\n",
       " ('PEP', 'MS'): 1,\n",
       " ('PEP', 'WFC'): 1,\n",
       " ('AIG', 'COF'): 1,\n",
       " ('AIG', 'BK'): 1,\n",
       " ('AIG', 'PNC'): 1,\n",
       " ('AIG', 'PEP'): 1,\n",
       " ('AIG', 'DIS'): 1,\n",
       " ('AIG', 'JNJ'): 1,\n",
       " ('AIG', 'CMCSA'): 1,\n",
       " ('AIG', 'SCHW'): 1,\n",
       " ('AIG', 'MTB'): 1,\n",
       " ('AIG', 'MS'): 1,\n",
       " ('AIG', 'WFC'): 1,\n",
       " ('DIS', 'COF'): 1,\n",
       " ('DIS', 'BK'): 1,\n",
       " ('DIS', 'PNC'): 1,\n",
       " ('DIS', 'PEP'): 2,\n",
       " ('DIS', 'AIG'): 1,\n",
       " ('DIS', 'JNJ'): 1,\n",
       " ('DIS', 'CMCSA'): 1,\n",
       " ('DIS', 'SCHW'): 1,\n",
       " ('DIS', 'MTB'): 1,\n",
       " ('DIS', 'MS'): 1,\n",
       " ('DIS', 'WFC'): 1,\n",
       " ('JNJ', 'COF'): 1,\n",
       " ('JNJ', 'BK'): 1,\n",
       " ('JNJ', 'PNC'): 1,\n",
       " ('JNJ', 'PEP'): 1,\n",
       " ('JNJ', 'AIG'): 1,\n",
       " ('JNJ', 'DIS'): 1,\n",
       " ('JNJ', 'CMCSA'): 1,\n",
       " ('JNJ', 'SCHW'): 1,\n",
       " ('JNJ', 'MTB'): 1,\n",
       " ('JNJ', 'MS'): 1,\n",
       " ('JNJ', 'WFC'): 1,\n",
       " ('CMCSA', 'COF'): 1,\n",
       " ('CMCSA', 'BK'): 1,\n",
       " ('CMCSA', 'PNC'): 1,\n",
       " ('CMCSA', 'PEP'): 1,\n",
       " ('CMCSA', 'AIG'): 1,\n",
       " ('CMCSA', 'DIS'): 1,\n",
       " ('CMCSA', 'JNJ'): 1,\n",
       " ('CMCSA', 'SCHW'): 1,\n",
       " ('CMCSA', 'MTB'): 1,\n",
       " ('CMCSA', 'MS'): 1,\n",
       " ('CMCSA', 'WFC'): 1,\n",
       " ('SCHW', 'COF'): 1,\n",
       " ('SCHW', 'BK'): 1,\n",
       " ('SCHW', 'PNC'): 1,\n",
       " ('SCHW', 'PEP'): 1,\n",
       " ('SCHW', 'AIG'): 1,\n",
       " ('SCHW', 'DIS'): 1,\n",
       " ('SCHW', 'JNJ'): 1,\n",
       " ('SCHW', 'CMCSA'): 1,\n",
       " ('SCHW', 'MTB'): 1,\n",
       " ('SCHW', 'MS'): 1,\n",
       " ('SCHW', 'WFC'): 1,\n",
       " ('MTB', 'COF'): 1,\n",
       " ('MTB', 'BK'): 1,\n",
       " ('MTB', 'PNC'): 1,\n",
       " ('MTB', 'PEP'): 1,\n",
       " ('MTB', 'AIG'): 1,\n",
       " ('MTB', 'DIS'): 1,\n",
       " ('MTB', 'JNJ'): 1,\n",
       " ('MTB', 'CMCSA'): 1,\n",
       " ('MTB', 'SCHW'): 1,\n",
       " ('MTB', 'MS'): 1,\n",
       " ('MTB', 'WFC'): 1,\n",
       " ('MS', 'COF'): 1,\n",
       " ('MS', 'BK'): 1,\n",
       " ('MS', 'PNC'): 1,\n",
       " ('MS', 'PEP'): 1,\n",
       " ('MS', 'AIG'): 1,\n",
       " ('MS', 'DIS'): 1,\n",
       " ('MS', 'JNJ'): 1,\n",
       " ('MS', 'CMCSA'): 1,\n",
       " ('MS', 'SCHW'): 1,\n",
       " ('MS', 'MTB'): 1,\n",
       " ('MS', 'WFC'): 1,\n",
       " ('WFC', 'COF'): 1,\n",
       " ('WFC', 'BK'): 1,\n",
       " ('WFC', 'PNC'): 2,\n",
       " ('WFC', 'PEP'): 1,\n",
       " ('WFC', 'AIG'): 1,\n",
       " ('WFC', 'DIS'): 1,\n",
       " ('WFC', 'JNJ'): 1,\n",
       " ('WFC', 'CMCSA'): 1,\n",
       " ('WFC', 'SCHW'): 1,\n",
       " ('WFC', 'MTB'): 1,\n",
       " ('WFC', 'MS'): 1,\n",
       " ('PYPL', 'SCHW'): 1,\n",
       " ('SCHW', 'PYPL'): 1,\n",
       " ('TXN', 'TXN'): 1,\n",
       " ('TXN', 'PAYX'): 1,\n",
       " ('PAYX', 'TXN'): 1,\n",
       " ('WDC', 'WDC'): 3,\n",
       " ('RRC', 'RRC'): 1,\n",
       " ('VMC', 'NUE'): 1,\n",
       " ('NUE', 'VMC'): 1,\n",
       " ('AYI', 'AYI'): 2,\n",
       " ('UNM', 'UNM'): 2,\n",
       " ('UNM', 'TRV'): 1,\n",
       " ('TRV', 'UNM'): 1,\n",
       " ('MCD', 'UNM'): 1,\n",
       " ('UNM', 'MCD'): 1,\n",
       " ('AFL', 'UNM'): 1,\n",
       " ('UNM', 'AFL'): 1,\n",
       " ('QRVO', 'QRVO'): 7,\n",
       " ('SNA', 'PVH'): 1,\n",
       " ('PVH', 'SNA'): 1,\n",
       " ('PVH', 'PVH'): 2,\n",
       " ('TMO', 'TMO'): 1,\n",
       " ('TMO', 'SYK'): 1,\n",
       " ('SYK', 'TMO'): 1,\n",
       " ('USB', 'USB'): 2,\n",
       " ('USB', 'PNC'): 1,\n",
       " ('USB', 'WFC'): 2,\n",
       " ('PNC', 'USB'): 1,\n",
       " ('WFC', 'USB'): 2,\n",
       " ('CF', 'CF'): 6,\n",
       " ('CTAS', 'CTAS'): 2,\n",
       " ('JNJ', 'EW'): 10,\n",
       " ('EW', 'JNJ'): 10,\n",
       " ('WMT', 'WMT'): 1,\n",
       " ('WMT', 'NFLX'): 1,\n",
       " ('NFLX', 'WMT'): 1,\n",
       " ('SBUX', 'WMT'): 1,\n",
       " ('WMT', 'SBUX'): 1,\n",
       " ('WMT', 'TGT'): 2,\n",
       " ('TGT', 'WMT'): 2,\n",
       " ('RJF', 'RJF'): 1,\n",
       " ('JWN', 'JWN'): 2,\n",
       " ('UAA', 'UAA'): 2,\n",
       " ('MMC', 'MCD'): 2,\n",
       " ('MMC', 'CL'): 1,\n",
       " ('MCD', 'MMC'): 2,\n",
       " ('MCD', 'CL'): 1,\n",
       " ('CL', 'MMC'): 1,\n",
       " ('CL', 'MCD'): 1,\n",
       " ('MMC', 'VRTX'): 1,\n",
       " ('MCD', 'VRTX'): 1,\n",
       " ('VRTX', 'MMC'): 1,\n",
       " ('VRTX', 'MCD'): 1,\n",
       " ('EQR', 'EQR'): 1,\n",
       " ('XYL', 'XYL'): 5,\n",
       " ('XYL', 'AWK'): 1,\n",
       " ('AWK', 'XYL'): 1,\n",
       " ('HBAN', 'HBAN'): 1,\n",
       " ('MGM', 'WYNN'): 1,\n",
       " ('WYNN', 'MGM'): 1,\n",
       " ('MGM', 'MGM'): 3,\n",
       " ('NCLH', 'NCLH'): 2,\n",
       " ('RCL', 'NCLH'): 1,\n",
       " ('NCLH', 'RCL'): 1,\n",
       " ('MCHP', 'MCHP'): 6,\n",
       " ('MET', 'MET'): 3,\n",
       " ('MET', 'PRU'): 1,\n",
       " ('PRU', 'MET'): 1,\n",
       " ('IT', 'IT'): 4,\n",
       " ('O', 'O'): 4,\n",
       " ('KO', 'WFC'): 1,\n",
       " ('KO', 'AXP'): 4,\n",
       " ('KO', 'OXY'): 2,\n",
       " ('WFC', 'KO'): 1,\n",
       " ('WFC', 'AXP'): 1,\n",
       " ('WFC', 'OXY'): 1,\n",
       " ('AXP', 'KO'): 4,\n",
       " ('AXP', 'WFC'): 1,\n",
       " ('AXP', 'OXY'): 2,\n",
       " ('OXY', 'KO'): 2,\n",
       " ('OXY', 'WFC'): 1,\n",
       " ('OXY', 'AXP'): 2,\n",
       " ('MS', 'MS'): 5,\n",
       " ('OMC', 'OMC'): 2,\n",
       " ('OMC', 'V'): 1,\n",
       " ('V', 'OMC'): 1,\n",
       " ('FE', 'FE'): 2,\n",
       " ('CRM', 'CRM'): 4,\n",
       " ('FLS', 'GD'): 1,\n",
       " ('GD', 'FLS'): 1,\n",
       " ('FLS', 'IR'): 1,\n",
       " ('IR', 'FLS'): 1,\n",
       " ('FLS', 'GS'): 1,\n",
       " ('GS', 'FLS'): 1,\n",
       " ('FLS', 'FLS'): 2,\n",
       " ('SEE', 'SEE'): 5,\n",
       " ('RL', 'RL'): 1,\n",
       " ('PCG', 'PCG'): 3,\n",
       " ('NWS', 'NWSA'): 2,\n",
       " ('NWSA', 'NWS'): 2,\n",
       " ('ALGN', 'ALGN'): 2,\n",
       " ('MRO', 'MRO'): 3,\n",
       " ('MSI', 'MSI'): 7,\n",
       " ('MSFT', 'LMT'): 1,\n",
       " ('MSFT', 'VMC'): 1,\n",
       " ('LMT', 'MSFT'): 1,\n",
       " ('LMT', 'VMC'): 1,\n",
       " ('VMC', 'MSFT'): 1,\n",
       " ('VMC', 'LMT'): 1,\n",
       " ('MSFT', 'MSFT'): 1,\n",
       " ('MSFT', 'NVDA'): 7,\n",
       " ('NVDA', 'MSFT'): 7,\n",
       " ('SIG', 'SIG'): 2,\n",
       " ('WBA', 'WBA'): 2,\n",
       " ('WBA', 'AMGN'): 1,\n",
       " ('AMGN', 'WBA'): 1,\n",
       " ('WBA', 'V'): 1,\n",
       " ('WBA', 'VZ'): 1,\n",
       " ('V', 'WBA'): 1,\n",
       " ('V', 'VZ'): 1,\n",
       " ('VZ', 'WBA'): 1,\n",
       " ('VZ', 'V'): 1,\n",
       " ('PAYX', 'PAYX'): 3,\n",
       " ('PAYX', 'SPGI'): 1,\n",
       " ('SPGI', 'PAYX'): 1,\n",
       " ('SBUX', 'SBUX'): 3,\n",
       " ('SBUX', 'YUM'): 2,\n",
       " ('YUM', 'SBUX'): 2,\n",
       " ('SBUX', 'MCD'): 2,\n",
       " ('MCD', 'SBUX'): 2,\n",
       " ('MCD', 'YUM'): 1,\n",
       " ('YUM', 'MCD'): 1,\n",
       " ('SBUX', 'PFE'): 1,\n",
       " ('PFE', 'SBUX'): 1,\n",
       " ('PH', 'PH'): 3,\n",
       " ('IR', 'PH'): 4,\n",
       " ('PH', 'IR'): 4,\n",
       " ('SWK', 'IR'): 1,\n",
       " ('SWK', 'PH'): 1,\n",
       " ('IR', 'SWK'): 1,\n",
       " ('PH', 'SWK'): 1,\n",
       " ('SYY', 'SYY'): 2,\n",
       " ('XEL', 'XEL'): 2,\n",
       " ('ADM', 'ADM'): 3,\n",
       " ('SBUX', 'UAL'): 1,\n",
       " ('SBUX', 'ADM'): 1,\n",
       " ('UAL', 'SBUX'): 1,\n",
       " ('UAL', 'ADM'): 1,\n",
       " ('ADM', 'SBUX'): 1,\n",
       " ('ADM', 'UAL'): 1,\n",
       " ('SNPS', 'SNPS'): 1,\n",
       " ('MNST', 'MNST'): 5,\n",
       " ('MNST', 'PG'): 1,\n",
       " ('PG', 'MNST'): 1,\n",
       " ('HST', 'HST'): 6,\n",
       " ('LUV', 'LUV'): 1,\n",
       " ('LUV', 'AAL'): 1,\n",
       " ('LUV', 'UAL'): 2,\n",
       " ('AAL', 'LUV'): 1,\n",
       " ('AAL', 'UAL'): 2,\n",
       " ('UAL', 'LUV'): 2,\n",
       " ('UAL', 'AAL'): 2,\n",
       " ('PM', 'PM'): 2,\n",
       " ('WU', 'WU'): 5,\n",
       " ('TDG', 'TDG'): 3,\n",
       " ('SPG', 'SPG'): 4,\n",
       " ('CI', 'HCA'): 1,\n",
       " ('HCA', 'CI'): 1,\n",
       " ('WRK', 'PPG'): 1,\n",
       " ('PPG', 'WRK'): 1,\n",
       " ('GS', 'ZION'): 1,\n",
       " ('ZION', 'GS'): 1,\n",
       " ('GS', 'GS'): 1,\n",
       " ('GPN', 'GPN'): 2,\n",
       " ('MAT', 'GPS'): 1,\n",
       " ('GPS', 'MAT'): 1,\n",
       " ('LEG', 'LEG'): 4,\n",
       " ('PKG', 'PKG'): 4,\n",
       " ('APTV', 'APTV'): 7,\n",
       " ('APTV', 'QCOM'): 1,\n",
       " ('QCOM', 'APTV'): 1,\n",
       " ('PRGO', 'PRGO'): 4,\n",
       " ('NTRS', 'NTRS'): 1,\n",
       " ('AIG', 'AIG'): 6,\n",
       " ('HOLX', 'HOLX'): 2,\n",
       " ('HOLX', 'EW'): 3,\n",
       " ('EW', 'HOLX'): 3,\n",
       " ('NFLX', 'DIS'): 3,\n",
       " ('DIS', 'NFLX'): 3,\n",
       " ('NFLX', 'NFLX'): 2,\n",
       " ('V', 'NFLX'): 1,\n",
       " ('V', 'KO'): 1,\n",
       " ('V', 'AXP'): 2,\n",
       " ('NFLX', 'V'): 1,\n",
       " ('NFLX', 'KO'): 1,\n",
       " ('NFLX', 'AXP'): 1,\n",
       " ('KO', 'V'): 1,\n",
       " ('KO', 'NFLX'): 1,\n",
       " ('AXP', 'V'): 2,\n",
       " ('AXP', 'NFLX'): 1,\n",
       " ('PSX', 'PSX'): 6,\n",
       " ('OXY', 'PSX'): 1,\n",
       " ('PSX', 'OXY'): 1,\n",
       " ('TJX', 'TJX'): 2,\n",
       " ('AMGN', 'JNJ'): 1,\n",
       " ('JNJ', 'AMGN'): 1,\n",
       " ('NEM', 'NEM'): 1,\n",
       " ('NEM', 'PFE'): 1,\n",
       " ('PFE', 'NEM'): 1,\n",
       " ('AES', 'PEG'): 1,\n",
       " ('PEG', 'AES'): 1,\n",
       " ('PEG', 'PEG'): 2,\n",
       " ('M', 'M'): 1,\n",
       " ('GD', 'GD'): 3,\n",
       " ('WYNN', 'WYNN'): 4,\n",
       " ('ZTS', 'ZTS'): 7,\n",
       " ('NRG', 'NRG'): 4,\n",
       " ('NEE', 'PPL'): 2,\n",
       " ('PPL', 'NEE'): 2,\n",
       " ('EXC', 'NEE'): 1,\n",
       " ('NEE', 'EXC'): 1,\n",
       " ('PHM', 'MAS'): 1,\n",
       " ('MAS', 'PHM'): 1,\n",
       " ('BK', 'BK'): 1,\n",
       " ('JNJ', 'SPGI'): 1,\n",
       " ('SPGI', 'JNJ'): 1,\n",
       " ('SPGI', 'SPGI'): 4,\n",
       " ('TSN', 'MCK'): 1,\n",
       " ('MCK', 'TSN'): 1,\n",
       " ('TSN', 'TSN'): 1,\n",
       " ('SYF', 'SYF'): 1,\n",
       " ('MA', 'V'): 4,\n",
       " ('V', 'MA'): 4,\n",
       " ('AXP', 'MA'): 1,\n",
       " ('MA', 'AXP'): 1,\n",
       " ('PYPL', 'MA'): 1,\n",
       " ('MA', 'PYPL'): 1,\n",
       " ('V', 'MSFT'): 1,\n",
       " ('V', 'UPS'): 1,\n",
       " ('MA', 'MSFT'): 1,\n",
       " ('MA', 'UPS'): 1,\n",
       " ('MSFT', 'V'): 1,\n",
       " ('MSFT', 'MA'): 1,\n",
       " ('MSFT', 'UPS'): 1,\n",
       " ('UPS', 'V'): 1,\n",
       " ('UPS', 'MA'): 1,\n",
       " ('UPS', 'MSFT'): 1,\n",
       " ('MA', 'MA'): 1,\n",
       " ('QCOM', 'QCOM'): 5,\n",
       " ('EXPE', 'EXPE'): 4,\n",
       " ('LMT', 'LMT'): 5,\n",
       " ('LMT', 'NOC'): 1,\n",
       " ('NOC', 'LMT'): 1,\n",
       " ('DHI', 'PHM'): 4,\n",
       " ('PHM', 'DHI'): 4,\n",
       " ('PHM', 'PHM'): 2,\n",
       " ('BAX', 'BAX'): 2,\n",
       " ('BAX', 'SYK'): 1,\n",
       " ('SYK', 'BAX'): 1,\n",
       " ('WEC', 'WEC'): 1,\n",
       " ('JNJ', 'PFE'): 1,\n",
       " ('PFE', 'JNJ'): 1,\n",
       " ('JNJ', 'JNJ'): 2,\n",
       " ('PFE', 'MDT'): 1,\n",
       " ('MDT', 'PFE'): 1,\n",
       " ('MDT', 'MDT'): 4,\n",
       " ('GT', 'GT'): 3,\n",
       " ('RSG', 'RSG'): 2,\n",
       " ('MRK', 'MRK'): 5,\n",
       " ('MOS', 'MOS'): 7,\n",
       " ('ROK', 'ROK'): 1,\n",
       " ('ROK', 'IT'): 1,\n",
       " ('IT', 'ROK'): 1,\n",
       " ('WMB', 'WMB'): 2,\n",
       " ('BA', 'BA'): 4,\n",
       " ('ILMN', 'ILMN'): 2,\n",
       " ('PWR', 'PWR'): 5,\n",
       " ('PWR', 'URI'): 1,\n",
       " ('URI', 'PWR'): 1,\n",
       " ('SO', 'SO'): 2,\n",
       " ('CI', 'CI'): 6,\n",
       " ('SWKS', 'SWKS'): 1,\n",
       " ('NOC', 'BA'): 1,\n",
       " ('BA', 'NOC'): 1,\n",
       " ('NVDA', 'PCAR'): 3,\n",
       " ('NVDA', 'NKE'): 3,\n",
       " ('PCAR', 'NVDA'): 3,\n",
       " ('PCAR', 'NKE'): 4,\n",
       " ('PCAR', 'MSFT'): 4,\n",
       " ('NKE', 'NVDA'): 3,\n",
       " ('NKE', 'PCAR'): 4,\n",
       " ('NKE', 'MSFT'): 4,\n",
       " ('MSFT', 'PCAR'): 4,\n",
       " ('MSFT', 'NKE'): 4,\n",
       " ('NKE', 'NKE'): 2,\n",
       " ('NVDA', 'QCOM'): 1,\n",
       " ('PCAR', 'QCOM'): 1,\n",
       " ('MSFT', 'QCOM'): 1,\n",
       " ('NKE', 'QCOM'): 1,\n",
       " ('QCOM', 'NVDA'): 1,\n",
       " ('QCOM', 'PCAR'): 1,\n",
       " ('QCOM', 'MSFT'): 1,\n",
       " ('QCOM', 'NKE'): 1,\n",
       " ('ANSS', 'ANSS'): 6,\n",
       " ('VFC', 'VFC'): 1,\n",
       " ('ZION', 'ZION'): 3,\n",
       " ('WFC', 'ZION'): 1,\n",
       " ('ZION', 'WFC'): 1,\n",
       " ('WFC', 'WFC'): 2,\n",
       " ('PFE', 'PFE'): 3,\n",
       " ('FOXA', 'FOXA'): 1,\n",
       " ('FOXA', 'FOX'): 2,\n",
       " ('FOX', 'FOXA'): 2,\n",
       " ('CMCSA', 'FOX'): 1,\n",
       " ('CMCSA', 'FOXA'): 1,\n",
       " ('FOX', 'CMCSA'): 1,\n",
       " ('FOXA', 'CMCSA'): 1,\n",
       " ('PYPL', 'PYPL'): 3,\n",
       " ('IBM', 'IBM'): 6,\n",
       " ('MPC', 'MPC'): 3,\n",
       " ('AES', 'AES'): 1,\n",
       " ('GM', 'GM'): 2,\n",
       " ('GM', 'PEP'): 1,\n",
       " ('PEP', 'GM'): 1,\n",
       " ('UAL', 'UAL'): 3,\n",
       " ('PNC', 'PNC'): 6,\n",
       " ('PNW', 'PNW'): 3,\n",
       " ('TGT', 'TGT'): 1,\n",
       " ('ISRG', 'ISRG'): 2,\n",
       " ('ISRG', 'JNJ'): 2,\n",
       " ('ISRG', 'EW'): 3,\n",
       " ('JNJ', 'ISRG'): 2,\n",
       " ('EW', 'ISRG'): 3,\n",
       " ('MS', 'BXP'): 1,\n",
       " ('BXP', 'MS'): 1,\n",
       " ('BXP', 'BXP'): 2,\n",
       " ('NAVI', 'NAVI'): 1,\n",
       " ('WRK', 'WRK'): 5,\n",
       " ('CSX', 'CSX'): 2,\n",
       " ('RCL', 'RCL'): 6,\n",
       " ('ROST', 'ROST'): 3,\n",
       " ('ADI', 'ADI'): 1,\n",
       " ('SBAC', 'SBAC'): 3,\n",
       " ('SBAC', 'VZ'): 1,\n",
       " ('VZ', 'SBAC'): 1,\n",
       " ('RF', 'RF'): 1,\n",
       " ('USB', 'RF'): 1,\n",
       " ('RF', 'USB'): 1,\n",
       " ('V', 'V'): 1,\n",
       " ('APH', 'APH'): 1,\n",
       " ('MAR', 'MAR'): 5,\n",
       " ('SNA', 'SNA'): 2,\n",
       " ('KLAC', 'KLAC'): 1,\n",
       " ('VZ', 'VZ'): 7,\n",
       " ('AIV', 'AIV'): 1,\n",
       " ('TPR', 'TPR'): 1,\n",
       " ('SWK', 'SWK'): 5,\n",
       " ('NVDA', 'NVDA'): 2,\n",
       " ('MCK', 'MCK'): 1,\n",
       " ('APD', 'APD'): 2,\n",
       " ('APD', 'GS'): 1,\n",
       " ('GS', 'APD'): 1,\n",
       " ('FOX', 'FOX'): 1,\n",
       " ('CHRW', 'CHRW'): 2,\n",
       " ('CMI', 'CMI'): 6,\n",
       " ('DIS', 'DIS'): 5,\n",
       " ('DIS', 'OXY'): 1,\n",
       " ('OXY', 'DIS'): 1,\n",
       " ('EXC', 'EXC'): 4,\n",
       " ('OKE', 'OKE'): 5,\n",
       " ('MTB', 'MTB'): 1,\n",
       " ('AKAM', 'AKAM'): 1,\n",
       " ('HP', 'HP'): 1,\n",
       " ('HP', 'MPC'): 1,\n",
       " ('MPC', 'HP'): 1,\n",
       " ('FFIV', 'FFIV'): 1,\n",
       " ('NDAQ', 'NDAQ'): 2,\n",
       " ('CL', 'CL'): 3,\n",
       " ('PEP', 'CL'): 1,\n",
       " ('CL', 'PEP'): 1,\n",
       " ('JNPR', 'JNPR'): 1,\n",
       " ('XRAY', 'JNJ'): 1,\n",
       " ('XRAY', 'EW'): 1,\n",
       " ('JNJ', 'XRAY'): 1,\n",
       " ('EW', 'XRAY'): 1,\n",
       " ('XRAY', 'XRAY'): 2,\n",
       " ('CMCSA', 'CMCSA'): 5,\n",
       " ('PRU', 'PRU'): 6,\n",
       " ('DVN', 'OXY'): 3,\n",
       " ('OXY', 'DVN'): 3,\n",
       " ('DVN', 'DVN'): 1,\n",
       " ('PX', 'PX'): 1,\n",
       " ('VRTX', 'URI'): 1,\n",
       " ('VRTX', 'ULTA'): 1,\n",
       " ('URI', 'VRTX'): 1,\n",
       " ('URI', 'ULTA'): 1,\n",
       " ('ULTA', 'VRTX'): 1,\n",
       " ('ULTA', 'URI'): 1,\n",
       " ('URI', 'URI'): 1,\n",
       " ('ORCL', 'IT'): 1,\n",
       " ('IT', 'ORCL'): 1,\n",
       " ('AXP', 'ORCL'): 1,\n",
       " ('KO', 'ORCL'): 1,\n",
       " ('OXY', 'ORCL'): 1,\n",
       " ('ORCL', 'AXP'): 1,\n",
       " ('ORCL', 'KO'): 1,\n",
       " ('ORCL', 'OXY'): 1,\n",
       " ('ORCL', 'ORCL'): 1,\n",
       " ('PSA', 'PSA'): 5,\n",
       " ('MU', 'MU'): 3,\n",
       " ('MU', 'GS'): 1,\n",
       " ('GS', 'MU'): 1,\n",
       " ('MTD', 'MTD'): 4,\n",
       " ('FIS', 'FIS'): 1,\n",
       " ('YUM', 'YUM'): 5,\n",
       " ('AON', 'AON'): 2,\n",
       " ('MCD', 'MCD'): 2,\n",
       " ('KO', 'MCD'): 1,\n",
       " ('MCD', 'KO'): 1,\n",
       " ('PLD', 'PLD'): 1,\n",
       " ('TRIP', 'TRIP'): 6,\n",
       " ('RHI', 'RHI'): 5,\n",
       " ('MSFT', 'RHI'): 1,\n",
       " ('RHI', 'MSFT'): 1,\n",
       " ('NWL', 'NWL'): 3,\n",
       " ('PCAR', 'PCAR'): 2,\n",
       " ('TROW', 'TROW'): 1,\n",
       " ('UPS', 'UPS'): 4,\n",
       " ('AXP', 'AXP'): 1,\n",
       " ('AXP', 'SCHW'): 1,\n",
       " ('SCHW', 'AXP'): 1,\n",
       " ('MLM', 'MLM'): 2,\n",
       " ('NTAP', 'NTAP'): 1,\n",
       " ('EXC', 'NI'): 1,\n",
       " ('NI', 'EXC'): 1,\n",
       " ('BSX', 'SYK'): 1,\n",
       " ('BSX', 'ZBH'): 1,\n",
       " ('SYK', 'BSX'): 1,\n",
       " ('SYK', 'ZBH'): 1,\n",
       " ('ZBH', 'BSX'): 1,\n",
       " ('ZBH', 'SYK'): 1,\n",
       " ('ZBH', 'ZBH'): 5,\n",
       " ('ZBH', 'JNJ'): 1,\n",
       " ('ZBH', 'EW'): 1,\n",
       " ('JNJ', 'ZBH'): 1,\n",
       " ('EW', 'ZBH'): 1,\n",
       " ('FTI', 'MPC'): 1,\n",
       " ('MPC', 'FTI'): 1,\n",
       " ('PPL', 'PPL'): 4,\n",
       " ('RMD', 'RMD'): 1,\n",
       " ('PNR', 'PNR'): 2,\n",
       " ('TSCO', 'TSCO'): 2,\n",
       " ('ULTA', 'TSCO'): 1,\n",
       " ('TSCO', 'ULTA'): 1,\n",
       " ('MDLZ', 'MDLZ'): 5,\n",
       " ('PEP', 'MDLZ'): 1,\n",
       " ('MDLZ', 'PEP'): 1,\n",
       " ('OXY', 'OXY'): 3,\n",
       " ('BSX', 'BSX'): 1,\n",
       " ('COF', 'COF'): 1,\n",
       " ('ULTA', 'ULTA'): 3,\n",
       " ('VRTX', 'VRTX'): 4,\n",
       " ('PEP', 'KO'): 2,\n",
       " ('KO', 'PEP'): 2,\n",
       " ('KO', 'KO'): 2,\n",
       " ('MHK', 'TGT'): 1,\n",
       " ('TGT', 'MHK'): 1,\n",
       " ('INCY', 'INCY'): 5,\n",
       " ('WM', 'WM'): 1,\n",
       " ('INTU', 'INTU'): 4,\n",
       " ('INTU', 'IBM'): 1,\n",
       " ('INTU', 'MSFT'): 1,\n",
       " ('IBM', 'INTU'): 1,\n",
       " ('IBM', 'MSFT'): 1,\n",
       " ('MSFT', 'INTU'): 1,\n",
       " ('MSFT', 'IBM'): 1,\n",
       " ('SYK', 'SYK'): 2,\n",
       " ('DVA', 'DVA'): 3,\n",
       " ('CSCO', 'VRSN'): 1,\n",
       " ('VRSN', 'CSCO'): 1,\n",
       " ('VRSN', 'VRSN'): 1,\n",
       " ('WHR', 'WHR'): 1,\n",
       " ('TSN', 'WHR'): 1,\n",
       " ('TSN', 'JNJ'): 1,\n",
       " ('WHR', 'TSN'): 1,\n",
       " ('WHR', 'JNJ'): 1,\n",
       " ('JNJ', 'TSN'): 1,\n",
       " ('JNJ', 'WHR'): 1,\n",
       " ('IR', 'IR'): 3,\n",
       " ('SRE', 'SRE'): 5,\n",
       " ('LYB', 'LYB'): 2,\n",
       " ('LYB', 'GS'): 1,\n",
       " ('GS', 'LYB'): 1,\n",
       " ('VMC', 'VMC'): 6,\n",
       " ('VMC', 'DHI'): 1,\n",
       " ('DHI', 'VMC'): 1,\n",
       " ('CINF', 'CINF'): 1,\n",
       " ('GE', 'GE'): 4,\n",
       " ('PGR', 'PGR'): 5,\n",
       " ('UNH', 'UNH'): 3,\n",
       " ('REG', 'REG'): 5,\n",
       " ('REG', 'FRT'): 1,\n",
       " ('FRT', 'REG'): 1,\n",
       " ('EL', 'EL'): 7,\n",
       " ('ADP', 'ADP'): 5,\n",
       " ('PG', 'PG'): 4,\n",
       " ('VNO', 'VNO'): 3,\n",
       " ('MAC', 'MAC'): 6,\n",
       " ('AAL', 'AAL'): 4,\n",
       " ('ORLY', 'ORLY'): 4,\n",
       " ('PFG', 'PFG'): 2,\n",
       " ('ETN', 'ETN'): 3,\n",
       " ('ETN', 'PH'): 1,\n",
       " ('PH', 'ETN'): 1,\n",
       " ('AFL', 'AFL'): 1,\n",
       " ('FRT', 'FRT'): 4,\n",
       " ('SHW', 'SHW'): 2,\n",
       " ('AWK', 'AWK'): 3,\n",
       " ('FCX', 'FCX'): 1,\n",
       " ('LNT', 'LNT'): 5,\n",
       " ('PEP', 'PEP'): 2,\n",
       " ('REGN', 'REGN'): 5,\n",
       " ('DGX', 'DGX'): 3,\n",
       " ('DGX', 'TGT'): 1,\n",
       " ('TGT', 'DGX'): 1,\n",
       " ('AMP', 'AMP'): 3,\n",
       " ('GPS', 'GPS'): 1,\n",
       " ('CSCO', 'CSCO'): 4,\n",
       " ('vrsk', 'vrsk'): 10,\n",
       " ('schw', 'schw'): 10,\n",
       " ('txn', 'txn'): 10,\n",
       " ('wdc', 'wdc'): 10,\n",
       " ('rrc', 'rrc'): 10,\n",
       " ('nue', 'nue'): 10,\n",
       " ('ayi', 'ayi'): 10,\n",
       " ('are', 'are'): 10,\n",
       " ('unm', 'unm'): 10,\n",
       " ('stt', 'stt'): 10,\n",
       " ('qrvo', 'qrvo'): 10,\n",
       " ('pvh', 'pvh'): 10,\n",
       " ('tmo', 'tmo'): 10,\n",
       " ('usb', 'usb'): 10,\n",
       " ('cf', 'cf'): 10,\n",
       " ('ctas', 'ctas'): 10,\n",
       " ('ew', 'ew'): 10,\n",
       " ('wmt', 'wmt'): 10,\n",
       " ('rjf', 'rjf'): 10,\n",
       " ('jwn', 'jwn'): 10,\n",
       " ('uaa', 'uaa'): 10,\n",
       " ('mmc', 'mmc'): 10,\n",
       " ('eqr', 'eqr'): 10,\n",
       " ('xyl', 'xyl'): 10,\n",
       " ('hban', 'hban'): 10,\n",
       " ('mgm', 'mgm'): 10,\n",
       " ('nclh', 'nclh'): 10,\n",
       " ('mchp', 'mchp'): 10,\n",
       " ('met', 'met'): 10,\n",
       " ('it', 'it'): 10,\n",
       " ('o', 'o'): 10,\n",
       " ('ms', 'ms'): 10,\n",
       " ('omc', 'omc'): 10,\n",
       " ('fe', 'fe'): 10,\n",
       " ('crm', 'crm'): 10,\n",
       " ('fls', 'fls'): 10,\n",
       " ('see', 'see'): 10,\n",
       " ('rl', 'rl'): 10,\n",
       " ('mco', 'mco'): 10,\n",
       " ('pcg', 'pcg'): 10,\n",
       " ('nws', 'nws'): 10,\n",
       " ('algn', 'algn'): 10,\n",
       " ('mro', 'mro'): 10,\n",
       " ('msi', 'msi'): 10,\n",
       " ('msft', 'msft'): 10,\n",
       " ('sig', 'sig'): 10,\n",
       " ('wba', 'wba'): 10,\n",
       " ('payx', 'payx'): 10,\n",
       " ('sbux', 'sbux'): 10,\n",
       " ('ph', 'ph'): 10,\n",
       " ('syy', 'syy'): 10,\n",
       " ('xel', 'xel'): 10,\n",
       " ('adm', 'adm'): 10,\n",
       " ('snps', 'snps'): 10,\n",
       " ('mnst', 'mnst'): 10,\n",
       " ('hst', 'hst'): 10,\n",
       " ('luv', 'luv'): 10,\n",
       " ('pm', 'pm'): 10,\n",
       " ('rop', 'rop'): 10,\n",
       " ('wu', 'wu'): 10,\n",
       " ('tdg', 'tdg'): 10,\n",
       " ('spg', 'spg'): 10,\n",
       " ('hca', 'hca'): 10,\n",
       " ('ppg', 'ppg'): 10,\n",
       " ('gs', 'gs'): 10,\n",
       " ('gpn', 'gpn'): 10,\n",
       " ('mat', 'mat'): 10,\n",
       " ('slb', 'slb'): 10,\n",
       " ('leg', 'leg'): 10,\n",
       " ('pkg', 'pkg'): 10,\n",
       " ('aptv', 'aptv'): 10,\n",
       " ('vtr', 'vtr'): 10,\n",
       " ('prgo', 'prgo'): 10,\n",
       " ('ntrs', 'ntrs'): 10,\n",
       " ('aig', 'aig'): 10,\n",
       " ('holx', 'holx'): 10,\n",
       " ('nflx', 'nflx'): 10,\n",
       " ('psx', 'psx'): 10,\n",
       " ('tjx', 'tjx'): 10,\n",
       " ('amgn', 'amgn'): 10,\n",
       " ('nem', 'nem'): 10,\n",
       " ('peg', 'peg'): 10,\n",
       " ('m', 'm'): 10,\n",
       " ('gd', 'gd'): 10,\n",
       " ('nov', 'nov'): 10,\n",
       " ('wynn', 'wynn'): 10,\n",
       " ('zts', 'zts'): 10,\n",
       " ('nrg', 'nrg'): 10,\n",
       " ('nee', 'nee'): 10,\n",
       " ('pdco', 'pdco'): 10,\n",
       " ('mas', 'mas'): 10,\n",
       " ('bk', 'bk'): 10,\n",
       " ('spgi', 'spgi'): 10,\n",
       " ('tsn', 'tsn'): 10,\n",
       " ('syf', 'syf'): 10,\n",
       " ('ma', 'ma'): 10,\n",
       " ('qcom', 'qcom'): 10,\n",
       " ('expe', 'expe'): 10,\n",
       " ('lmt', 'lmt'): 10,\n",
       " ('phm', 'phm'): 10,\n",
       " ('bax', 'bax'): 10,\n",
       " ('wec', 'wec'): 10,\n",
       " ('jnj', 'jnj'): 10,\n",
       " ('mdt', 'mdt'): 10,\n",
       " ('gt', 'gt'): 10,\n",
       " ('cci', 'cci'): 10,\n",
       " ('rsg', 'rsg'): 10,\n",
       " ('mrk', 'mrk'): 10,\n",
       " ('mos', 'mos'): 10,\n",
       " ('rok', 'rok'): 10,\n",
       " ('wmb', 'wmb'): 10,\n",
       " ('ba', 'ba'): 10,\n",
       " ('ilmn', 'ilmn'): 10,\n",
       " ('pwr', 'pwr'): 10,\n",
       " ('so', 'so'): 10,\n",
       " ('maa', 'maa'): 10,\n",
       " ('ci', 'ci'): 10,\n",
       " ('swks', 'swks'): 10,\n",
       " ('tel', 'tel'): 10,\n",
       " ('noc', 'noc'): 10,\n",
       " ('nke', 'nke'): 10,\n",
       " ('anss', 'anss'): 10,\n",
       " ('vfc', 'vfc'): 10,\n",
       " ('zion', 'zion'): 10,\n",
       " ('wfc', 'wfc'): 10,\n",
       " ('pfe', 'pfe'): 10,\n",
       " ('foxa', 'foxa'): 10,\n",
       " ('pypl', 'pypl'): 10,\n",
       " ('ibm', 'ibm'): 10,\n",
       " ('mpc', 'mpc'): 10,\n",
       " ('tap', 'tap'): 10,\n",
       " ('wat', 'wat'): 10,\n",
       " ('xrx', 'xrx'): 10,\n",
       " ('aes', 'aes'): 10,\n",
       " ('gm', 'gm'): 10,\n",
       " ('mkc', 'mkc'): 10,\n",
       " ('ual', 'ual'): 10,\n",
       " ('pnc', 'pnc'): 10,\n",
       " ('pnw', 'pnw'): 10,\n",
       " ('tgt', 'tgt'): 10,\n",
       " ('isrg', 'isrg'): 10,\n",
       " ('bxp', 'bxp'): 10,\n",
       " ('navi', 'navi'): 10,\n",
       " ('wrk', 'wrk'): 10,\n",
       " ('csx', 'csx'): 10,\n",
       " ('rcl', 'rcl'): 10,\n",
       " ('rost', 'rost'): 10,\n",
       " ('adi', 'adi'): 10,\n",
       " ('sbac', 'sbac'): 10,\n",
       " ('rf', 'rf'): 10,\n",
       " ('v', 'v'): 10,\n",
       " ('aph', 'aph'): 10,\n",
       " ('mar', 'mar'): 10,\n",
       " ('nwsa', 'nwsa'): 10,\n",
       " ('sna', 'sna'): 10,\n",
       " ('klac', 'klac'): 10,\n",
       " ('vz', 'vz'): 10,\n",
       " ('aiv', 'aiv'): 10,\n",
       " ('tpr', 'tpr'): 10,\n",
       " ('swk', 'swk'): 10,\n",
       " ('nvda', 'nvda'): 10,\n",
       " ('mck', 'mck'): 10,\n",
       " ('trv', 'trv'): 10,\n",
       " ('apd', 'apd'): 10,\n",
       " ('gpc', 'gpc'): 10,\n",
       " ('fox', 'fox'): 10,\n",
       " ('chrw', 'chrw'): 10,\n",
       " ('cmi', 'cmi'): 10,\n",
       " ('dis', 'dis'): 10,\n",
       " ('exc', 'exc'): 10,\n",
       " ('oke', 'oke'): 10,\n",
       " ('mtb', 'mtb'): 10,\n",
       " ('ua', 'ua'): 10,\n",
       " ('akam', 'akam'): 10,\n",
       " ('hp', 'hp'): 10,\n",
       " ('ffiv', 'ffiv'): 10,\n",
       " ('ndaq', 'ndaq'): 10,\n",
       " ('cl', 'cl'): 10,\n",
       " ('jnpr', 'jnpr'): 10,\n",
       " ('xray', 'xray'): 10,\n",
       " ('cmcsa', 'cmcsa'): 10,\n",
       " ('pru', 'pru'): 10,\n",
       " ('dvn', 'dvn'): 10,\n",
       " ('px', 'px'): 10,\n",
       " ('uri', 'uri'): 10,\n",
       " ('orcl', 'orcl'): 10,\n",
       " ('psa', 'psa'): 10,\n",
       " ('mu', 'mu'): 10,\n",
       " ('mtd', 'mtd'): 10,\n",
       " ('has', 'has'): 10,\n",
       " ('fis', 'fis'): 10,\n",
       " ('yum', 'yum'): 10,\n",
       " ('aon', 'aon'): 10,\n",
       " ('mcd', 'mcd'): 10,\n",
       " ('pld', 'pld'): 10,\n",
       " ('stx', 'stx'): 10,\n",
       " ('trip', 'trip'): 10,\n",
       " ('rhi', 'rhi'): 10,\n",
       " ('nwl', 'nwl'): 10,\n",
       " ('pcar', 'pcar'): 10,\n",
       " ('trow', 'trow'): 10,\n",
       " ('ups', 'ups'): 10,\n",
       " ('axp', 'axp'): 10,\n",
       " ('mlm', 'mlm'): 10,\n",
       " ('ntap', 'ntap'): 10,\n",
       " ('ni', 'ni'): 10,\n",
       " ('zbh', 'zbh'): 10,\n",
       " ('fti', 'fti'): 10,\n",
       " ('ppl', 'ppl'): 10,\n",
       " ('rmd', 'rmd'): 10,\n",
       " ('pnr', 'pnr'): 10,\n",
       " ('tsco', 'tsco'): 10,\n",
       " ('mdlz', 'mdlz'): 10,\n",
       " ('oxy', 'oxy'): 10,\n",
       " ('bsx', 'bsx'): 10,\n",
       " ('cof', 'cof'): 10,\n",
       " ('ulta', 'ulta'): 10,\n",
       " ('vrtx', 'vrtx'): 10,\n",
       " ('txt', 'txt'): 10,\n",
       " ('nsc', 'nsc'): 10,\n",
       " ('ko', 'ko'): 10,\n",
       " ('mhk', 'mhk'): 10,\n",
       " ('incy', 'incy'): 10,\n",
       " ('wy', 'wy'): 10,\n",
       " ('wm', 'wm'): 10,\n",
       " ('intu', 'intu'): 10,\n",
       " ('syk', 'syk'): 10,\n",
       " ('dva', 'dva'): 10,\n",
       " ('vrsn', 'vrsn'): 10,\n",
       " ('whr', 'whr'): 10,\n",
       " ('ir', 'ir'): 10,\n",
       " ('sre', 'sre'): 10,\n",
       " ('lyb', 'lyb'): 10,\n",
       " ('vmc', 'vmc'): 10,\n",
       " ('cinf', 'cinf'): 10,\n",
       " ('ge', 'ge'): 10,\n",
       " ('pgr', 'pgr'): 10,\n",
       " ('vlo', 'vlo'): 10,\n",
       " ('unh', 'unh'): 10,\n",
       " ('reg', 'reg'): 10,\n",
       " ('lkq', 'lkq'): 10,\n",
       " ('el', 'el'): 10,\n",
       " ('adp', 'adp'): 10,\n",
       " ('pg', 'pg'): 10,\n",
       " ('srcl', 'srcl'): 10,\n",
       " ('slg', 'slg'): 10,\n",
       " ('udr', 'udr'): 10,\n",
       " ('vno', 'vno'): 10,\n",
       " ('mac', 'mac'): 10,\n",
       " ('dhi', 'dhi'): 10,\n",
       " ('aal', 'aal'): 10,\n",
       " ('orly', 'orly'): 10,\n",
       " ('pfg', 'pfg'): 10,\n",
       " ('etn', 'etn'): 10,\n",
       " ('afl', 'afl'): 10,\n",
       " ('frt', 'frt'): 10,\n",
       " ('shw', 'shw'): 10,\n",
       " ('uhs', 'uhs'): 10,\n",
       " ('awk', 'awk'): 10,\n",
       " ('fcx', 'fcx'): 10,\n",
       " ('lnt', 'lnt'): 10,\n",
       " ('pep', 'pep'): 10,\n",
       " ('regn', 'regn'): 10,\n",
       " ('unp', 'unp'): 10,\n",
       " ('dgx', 'dgx'): 10,\n",
       " ('amp', 'amp'): 10,\n",
       " ('gps', 'gps'): 10,\n",
       " ('csco', 'csco'): 10}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_of_each_day[\"2023-05-06\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('JNJ', 'EW')\n",
      "('EW', 'JNJ')\n"
     ]
    }
   ],
   "source": [
    "for pair in edge_of_each_day[\"2023-05-06\"]:\n",
    "    firm1, firm2 = pair\n",
    "    if edge_of_each_day[\"2023-05-06\"][pair] == 10:\n",
    "        if firm1 != firm2:\n",
    "            print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, dataset[1], dataset[1].edge_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.graphgenerator.edge_generator.edge_of_each_day['2014-01-02'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    a = dataset[i].edge_index[0] == dataset[i].edge_index[1]\n",
    "    if a.sum() != len(dataset.graphgenerator.valid_firm_list):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum_dicts(dict_list):\n",
    "    # Calculate the weights for each dictionary\n",
    "    n = len(dict_list)\n",
    "    weights = [(i + 1) / sum(range(1, n + 1)) for i in range(n)]\n",
    "\n",
    "    # Initialize an empty dictionary to store the weighted sums\n",
    "    weighted_sum = {}\n",
    "\n",
    "    # Iterate through the list of dictionaries\n",
    "    for idx, d in enumerate(dict_list):\n",
    "        weight = weights[idx]\n",
    "        for key, value in d.items():\n",
    "            if key in weighted_sum:\n",
    "                weighted_sum[key] += value * weight\n",
    "            else:\n",
    "                weighted_sum[key] = value * weight\n",
    "\n",
    "\n",
    "    return weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = '2014-01-09'\n",
    "def get_prev_days(day:str, prev_len:int=5, type:str=\"natual\"):\n",
    "        day_index = dataset.graphgenerator.valid_trading_days.index(day)\n",
    "        if day_index >= prev_len-1:     # if there are more than prev_len days before the \"day\"\n",
    "            prev_trading_days = dataset.graphgenerator.valid_trading_days[day_index - prev_len+1: day_index+1]  \n",
    "        else:\n",
    "            prev_trading_days = dataset.graphgenerator.valid_trading_days[ : day_index+1]\n",
    "        for i in range(len(prev_trading_days)):\n",
    "            try:\n",
    "                first_trading_day = dataset.graphgenerator.valid_natural_days.index(prev_trading_days[i])\n",
    "                break\n",
    "            except ValueError:\n",
    "                pass\n",
    "        for i in reversed(range(len(prev_trading_days))):\n",
    "            try:\n",
    "                last_trading_day = dataset.graphgenerator.valid_natural_days.index(prev_trading_days[-1])\n",
    "                break\n",
    "            except ValueError:\n",
    "                pass\n",
    "        prev_natual_days = dataset.graphgenerator.valid_natural_days[first_trading_day:last_trading_day+1]\n",
    "        if type == \"natual\":\n",
    "            return prev_natual_days\n",
    "        elif type == \"trading\":\n",
    "            return prev_trading_days\n",
    "        else:\n",
    "            raise ValueError(f\"Wrong Argument Value for 'type'={type}\")\n",
    "    \n",
    "get_prev_days(day, prev_len=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dynamic_method = ['static', 'cooccur', 'cooccurNcorr', 'corr_rmean5', 'corr_rmean10', 'corr_rmean20', 'corr_std5', 'corr_std5', 'corr_std5', ]\n",
    "valid_dynamic_method[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_edge_of_days = []\n",
    "for i in range(len(prev_natual_days)):\n",
    "    prev_edge_of_days.append(dataset.graphgenerator.edge_generator.edge_of_each_day[prev_natual_days[i]])\n",
    "prev_edge_of_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = weighted_sum_dicts(prev_edge_of_days)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unic_keys = []\n",
    "for dict in prev_edge_of_days:\n",
    "    keys = list(dict.keys())\n",
    "    unic_keys.extend(keys)\n",
    "len(set(unic_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.graphgenerator.edge_generator.tweets_of_each_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.graphgenerator.edge_generator.edge_of_each_day['2014-01-20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.graphgenerator.valid_firm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goog = dataset.graphgenerator.node_generator.ticker_target_dic['GOOG']\n",
    "aapl = dataset.graphgenerator.node_generator.ticker_target_dic['GOOG']\n",
    "# dataset.graphgenerator.edge_generator.edge_of_each_day\n",
    "df= aapl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flash_crash = 9\n",
    "# dev_len, test_len = int(len(dataset)/10), int(len(dataset)/10*1.5)-flash_crash\n",
    "# train_len = len(dataset)-dev_len-test_len-flash_crash\n",
    "# print(train_len, dev_len, test_len)\n",
    "\n",
    "## Train, Dev, Test split\n",
    "dev_len, test_len = int(len(dataset)/10), int(len(dataset)/10)\n",
    "train_len = len(dataset)-dev_len-test_len\n",
    "print(train_len, dev_len, test_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.graphgenerator.edge_generator.edge_of_each_day['2014-01-02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset.graphgenerator.edge_generator.edge_of_each_day['2014-01-02']:\n",
    "    if key[0] == key[1]:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the distribution of Train/Val/Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "feat = 'std5'\n",
    "train_feat = df.iloc[:train_len][feat]\n",
    "dev_feat = df.iloc[train_len : train_len+dev_len][feat]\n",
    "test_feat = df.iloc[train_len+dev_len : train_len+dev_len+test_len][feat]\n",
    "sns.kdeplot(train_feat, shade = True, color='r', label = 'train')\n",
    "sns.kdeplot(dev_feat, shade = True, color='g', label = 'dev')\n",
    "sns.kdeplot(test_feat, shade = True, color='b', label = 'test')\n",
    "plt.xlabel('Feature')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = 'std20'\n",
    "df.iloc[:train_len][feat].plot()\n",
    "df.iloc[train_len : train_len+dev_len][feat].plot()\n",
    "df.iloc[train_len+dev_len : train_len+dev_len+test_len][feat].plot()\n",
    "# df.iloc[train_len+dev_len : train_len+dev_len+test_len][feat].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start: \",train_len+26, \"End: \",train_len+34 )\n",
    "df.iloc[train_len+20:train_len+40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:train_len].r_mean10.plot()\n",
    "df.iloc[train_len : train_len+dev_len].r_mean10.plot()\n",
    "df.iloc[train_len+dev_len : train_len+dev_len+test_len].r_mean10.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:train_len].r_mean20.plot()\n",
    "df.iloc[train_len : train_len+dev_len].r_mean20.plot()\n",
    "df.iloc[train_len+dev_len : train_len+dev_len+test_len].r_mean20.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try Something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incomplete price data record:  CA . Removed from valid_firm_list.\n",
      "Valid Firms:  269\n",
      "Incomplete price data record:  PXD 388\n"
     ]
    }
   ],
   "source": [
    "nodegenerator = NodeGenerator(\n",
    "    dataset_name = \"SPNews\",\n",
    "    num_node_features = 8,\n",
    "    num_Y_features = 4,\n",
    "    Y_type = \"mean\",\n",
    "    estimation_window = 21,\n",
    "    matrix_node_features = True,\n",
    "    classification = True,\n",
    "    node_normalize = \"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "for day in nodegenerator.valid_trading_days:\n",
    "    x = np.array(nodegenerator.build_graph_level_X(day=day))\n",
    "    y, _ = np.array(nodegenerator.build_graph_level_y(day=day))\n",
    "    X.append(x)\n",
    "    Y.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        x = x.to(device=device)\n",
    "        y = y.to(device=device)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "DeferredCudaCallError",
     "evalue": "CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. device=\u0001, num_gpus=\u0001\n\nCUDA call was originally invoked at:\n\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n    self.io_loop.start()\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n    self._run_once()\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n    handle._run()\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/asyncio/events.py\", line 84, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n    await self.process_one()\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n    await dispatch(*args)\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n    await result\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n    reply_content = await reply_content\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n    res = shell.run_cell(\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n    result = self._run_cell(\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n    result = runner(coro)\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_3548972/881608286.py\", line 7, in <module>\n    import torch\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/torch/__init__.py\", line 1478, in <module>\n    _C._initExtension(manager_path())\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 238, in <module>\n    _lazy_call(_check_capability)\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 235, in _lazy_call\n    _queued_calls.append((callable, traceback.format_stack()))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/GNN/lib/python3.11/site-packages/torch/cuda/__init__.py:306\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 306\u001b[0m     \u001b[43mqueued_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/GNN/lib/python3.11/site-packages/torch/cuda/__init__.py:174\u001b[0m, in \u001b[0;36m_check_capability\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[0;32m--> 174\u001b[0m     capability \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_capability\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     major \u001b[38;5;241m=\u001b[39m capability[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/GNN/lib/python3.11/site-packages/torch/cuda/__init__.py:430\u001b[0m, in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the cuda capability of a device.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m    tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m prop \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prop\u001b[38;5;241m.\u001b[39mmajor, prop\u001b[38;5;241m.\u001b[39mminor\n",
      "File \u001b[0;32m~/miniconda3/envs/GNN/lib/python3.11/site-packages/torch/cuda/__init__.py:448\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid device id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 448\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. device=\u0001, num_gpus=\u0001",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDeferredCudaCallError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Example Usage\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBatch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbatch_idx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mData: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/GNN/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/GNN/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/GNN/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/GNN/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures[idx], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     14\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m---> 15\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
      "File \u001b[0;32m~/miniconda3/envs/GNN/lib/python3.11/site-packages/torch/cuda/__init__.py:312\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    308\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    309\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA call failed lazily at initialization with error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA call was originally invoked at:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(orig_traceback)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    311\u001b[0m             )\n\u001b[0;32m--> 312\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DeferredCudaCallError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28mdelattr\u001b[39m(_tls, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_initializing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mDeferredCudaCallError\u001b[0m: CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. device=\u0001, num_gpus=\u0001\n\nCUDA call was originally invoked at:\n\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n    self.io_loop.start()\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n    self._run_once()\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n    handle._run()\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/asyncio/events.py\", line 84, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n    await self.process_one()\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n    await dispatch(*args)\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n    await result\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n    reply_content = await reply_content\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n    res = shell.run_cell(\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n    result = self._run_cell(\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n    result = runner(coro)\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_3548972/881608286.py\", line 7, in <module>\n    import torch\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/torch/__init__.py\", line 1478, in <module>\n    _C._initExtension(manager_path())\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 238, in <module>\n    _lazy_call(_check_capability)\n  File \"/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 235, in _lazy_call\n    _queued_calls.append((callable, traceback.format_stack()))\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(X, Y)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Example Usage\n",
    "for batch_idx, (data, target) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx+1}:\")\n",
    "    print(f\"Data: {data.shape}\")\n",
    "    print(f\"Target: {target.shape}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incomplete price data record:  CA . Removed from valid_firm_list.\n",
      "Valid Firms:  269\n",
      "Incomplete price data record:  PXD 388\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# ROOT_FOLDER = \"/home/yingjie/Year_3/\"\n",
    "ROOT_FOLDER = \"/home/yingjie_niu/Year_3/\"\n",
    "\n",
    "\n",
    "graph_params = {\n",
    "    ##  -------- Graph Params ---------\n",
    "    \"dynamic\" : \"cooccur\",\n",
    "    ##  -------- Node Params ---------\n",
    "    \"num_node_features\" : 8,\n",
    "    \"matrix_node_features\" : True,\n",
    "    \"node_normalize\" : 'mean',\n",
    "    \"num_Y_features\" : 3,\n",
    "    \"Y_type\" : \"std\",\n",
    "    \"classification\" : False,\n",
    "    \"graph_level_task\" : True,\n",
    "    \"estimation_window\" : 21,\n",
    "    ## -------- Edge Params ----------\n",
    "    \"num_edge_features\" : 1,\n",
    "    \"weighted_edge\" : True,\n",
    "    \"threshold\" : 0,\n",
    "    ## -------- Dataset Params --------\n",
    "    \"root\" : ROOT_FOLDER+\"/GNN_longterm/data/graph_sets\",\n",
    "    \"name\" : \"ACL2018\"\n",
    "}\n",
    "\n",
    "\n",
    "nodegenerator = NodeGenerator(\n",
    "    dataset_name = graph_params['name'],\n",
    "    num_node_features = graph_params['num_node_features'],\n",
    "    num_Y_features = graph_params['num_Y_features'],\n",
    "    Y_type = graph_params['Y_type'],\n",
    "    estimation_window = graph_params['estimation_window'],\n",
    "    matrix_node_features = graph_params['matrix_node_features'],\n",
    "    classification = graph_params['classification'],\n",
    "    node_normalize = graph_params['node_normalize']\n",
    ")\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.features[idx], dtype=torch.float64)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.float64)\n",
    "        x = x.to(device=device)\n",
    "        y = y.to(device=device)\n",
    "        return x, y\n",
    "X, Y = [], []\n",
    "for day in nodegenerator.valid_trading_days:\n",
    "    x = np.array(nodegenerator.build_graph_level_X(day=day))\n",
    "    y, _ = nodegenerator.build_graph_level_y(day=day)\n",
    "    y = np.array(y)\n",
    "    X.append(x)\n",
    "    Y.append(y)\n",
    "dataset = CustomDataset(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([21, 8]),\n",
       " torch.Size([4]),\n",
       " tensor([0., 0., 0., 1.], device='cuda:0', dtype=torch.float64))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = dataset[0]\n",
    "x.shape, y.shape, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Values:\n",
    "Assuming:\n",
    "\n",
    "source = [0, 2, 4]\n",
    "torch.arange(len(source)) = [0, 1, 2]\n",
    "When a_input[:, source, torch.arange(len(source)), :] is used, it will update the tensor as follows:\n",
    "\n",
    "For the first head (head 0):\n",
    "* a_input[0, 0, 0, :] is updated with concatenated_features[0, 0, :]\n",
    "* a_input[0, 2, 1, :] is updated with concatenated_features[0, 1, :]  \n",
    "* a_input[0, 4, 2, :] is updated with concatenated_features[0, 2, :]\n",
    "\n",
    "For the second head (head 1):\n",
    "* a_input[1, 0, 0, :] is updated with concatenated_features[1, 0, :]\n",
    "* a_input[1, 2, 1, :] is updated with concatenated_features[1, 1, :]\n",
    "* a_input[1, 4, 2, :] is updated with concatenated_features[1, 2, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(dataset[0].x.shape[0], dataset[0].x.shape[2])\n",
    "y = torch.FloatTensor(dataset[0].y.shape[0], dataset[0].y.shape[1])\n",
    "print(x.shape, y.shape)\n",
    "source, target = dataset[0].edge_index\n",
    "weight = torch.FloatTensor(8, 76, dataset[0].x.shape[2], dataset[0].y.shape[1])\n",
    "h = torch.matmul(x.reshape(-1, 1, dataset[0].x.shape[2]), weight).reshape(8, 76, -1)\n",
    "# a_input = torch.cat([h[:,source], h[:,target]], dim=2)\n",
    "source_feat = h[:, source, :]\n",
    "target_feat = h[:, target, :]\n",
    "concatenated_features = torch.cat([source_feat, target_feat], dim=-1)  # Shape: (8, num_edges, 2 * y.shape[1])\n",
    "\n",
    "a_input = torch.zeros(size = (8, 76, len(source), 2*h.shape[2]))\n",
    "a_input[:, source, torch.arange(len(source)), :] = concatenated_features\n",
    "# for i in range(len(source)):\n",
    "#     s = source[i]\n",
    "#     t = target[i]\n",
    "#     s_feat = h[:, s, :]\n",
    "#     t_feat = h[:, t, :]\n",
    "#     concated_feat = torch.cat((s_feat, t_feat), dim=-1)\n",
    "#     a_input[:, s, i, :] = concated_feat\n",
    "a = nn.Parameter(torch.ones(size=(8, 76, 2*dataset[0].y.shape[1], 1)))\n",
    "e = F.leaky_relu(torch.sum(torch.matmul(a_input, a), dim=1), negative_slope=0.2)\n",
    "N = h.shape[1]\n",
    "attention = -1e20*torch.ones([8, N, N], requires_grad=True)\n",
    "attention[:, source, target] = e[:,:, 0]\n",
    "attention = F.softmax(attention, dim=2)\n",
    "attention = F.dropout(attention, 0.2)\n",
    "h_prime = torch.matmul(attention, h)\n",
    "print(h_prime.shape)\n",
    "concat = False\n",
    "if concat:\n",
    "    h_prime = h_prime.permute(1,0,2).reshape(N,-1)\n",
    "else:\n",
    "    h_prime = torch.sum(h_prime, dim=0)\n",
    "h_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones([92]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention[:, source, target] = e[:,:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.shape, e.shape, torch.arange(len(source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_input = torch.zeros(size = (8, 76, len(source), 2*h.shape[2]))\n",
    "for i in range(len(source)):\n",
    "    s = source[i]\n",
    "    t = target[i]\n",
    "    s_feat = h[:, s, :]\n",
    "    t_feat = h[:, t, :]\n",
    "    concated_feat = torch.cat((s_feat, t_feat), dim=-1)\n",
    "    a_input[:, s, i, :] = concated_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape, a_input.shape, \n",
    "torch.sum(torch.matmul(a_input, a), dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8*76*6*1, 8*76*92*6, a_input[;, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review SPNews Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spnews_fp = \"/home/yingjie_niu/Year_3/GNN_longterm/data/raw/SPNews/\"\n",
    "spnews_price_raw = spnews_fp+\"price/\"\n",
    "spnews_news_raw = spnews_fp+\"news/raw/\"\n",
    "spnews_news_raw_202310 = spnews_fp+\"news/raw_202310_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_news = pd.read_csv(\"/home/yingjie_niu/Year_3/GNN_longterm/data/raw/SPNews/news/preprocessed_daily_2/2024-04-03.csv\")\n",
    "day_news['related_companies'] = day_news.apply(lambda x: ast.literal_eval(x['Related Company']) if x['Related Company'][0]==\"[\" else [x['Company']], axis=1)\n",
    "day_news = day_news.drop_duplicates(subset='Link', keep=\"first\")\n",
    "day_news.drop(columns=['Publisher', 'Link', 'ProviderPublishTime', 'Type'], inplace=True)\n",
    "# df_exploded = day_news.explode('related_companies')\n",
    "# mask = df_exploded['Company'] != df_exploded['related_companies']\n",
    "day_news[day_news['Text'].str.startswith(\"[In this article\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+2+3+4+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "weights = [(i + 1) / sum(range(1, n + 1)) for i in range(n)]\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg = EdgeGenerator(dataset_name=\"SPNews\",num_edge_features=1,weighted_edge=True,threshold=0,dynamic=True)\n",
    "day_news = eg.read_day_news(day='2022-09-24')\n",
    "day_news['related_companies'] = day_news.apply(lambda x: ast.literal_eval(x['Related Company']) if x['Related Company'][0]==\"[\" else [x['Company']], axis=1)\n",
    "# df_exploded = day_news.explode('related_companies')\n",
    "# df_exploded['related_companies']\n",
    "day_news['permutations'] = day_news.apply(lambda x: list(itertools.permutations(x['related_companies'], 2)) if len(x['related_companies'])>1 else [(x['related_companies'][0], x['related_companies'][0])], axis=1)\n",
    "all_pairs = [pair for sublist in day_news[\"permutations\"] for pair in sublist]\n",
    "pair_counts = Counter(all_pairs)\n",
    "# day_edges_dict = {pair: count for pair, count in pair_counts.items() if pair[0] in eg.valid_firm_list and pair[1] in eg.valid_firm_list}\n",
    "# print(len(day_edges_dict))\n",
    "# day_edges_dict = eg.add_self_edge(day_edges_dict) \n",
    "# print(len(day_edges_dict))\n",
    "day_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_news['related_companies'][1]) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firm_count = {}\n",
    "\n",
    "for pair, count in pair_counts.items():\n",
    "    firm1, firm2 = pair\n",
    "    if firm1 in firm_count:\n",
    "        firm_count[firm1] +=1 \n",
    "    else:\n",
    "        firm_count[firm1] = 1\n",
    "    \n",
    "    if firm2 in firm_count:\n",
    "        firm_count[firm2] +=1 \n",
    "    else:\n",
    "        firm_count[firm2] = 1\n",
    "firm_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firm_pair = []\n",
    "\n",
    "for pair, count in pair_counts.items():\n",
    "    firm1, firm2 = pair\n",
    "    if \"AAPL\" == firm1:\n",
    "        firm_pair.append(pair)\n",
    "\n",
    "firm_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair, count in pair_counts.items():\n",
    "    firm1, firm2 = pair\n",
    "    print(firm1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_news = eg.read_day_news(day='2022-09-24')\n",
    "day_news['related_companies'] = day_news.apply(lambda x: ast.literal_eval(x['Related Company']) if x['Related Company'][0]==\"[\" else [x['Company']], axis=1)\n",
    "day_edges_dict = {} \n",
    "df_exploded = day_news.explode('related_companies')\n",
    "mask = df_exploded['Company'] != df_exploded['related_companies']\n",
    "df_exploded = df_exploded[mask]\n",
    "# df_exploded\n",
    "\n",
    "# Generate pairs (source, related) and (related, source)\n",
    "pairs = pd.concat([\n",
    "    df_exploded[['Company', 'related_companies']],\n",
    "    df_exploded[['related_companies', 'Company']].rename(columns={'related_companies': 'Company', 'Company': 'related_companies'})\n",
    "])\n",
    "# Filter out the ticker that is not in valid_firm_list\n",
    "pairs.reset_index(drop=True, inplace=True)\n",
    "pairs = pairs[pairs['related_companies'].isin(eg.valid_firm_list)]\n",
    "pairs.reset_index(drop=True, inplace=True)\n",
    "pairs = pairs[pairs['Company'].isin(eg.valid_firm_list)]\n",
    "pairs.reset_index(drop=True, inplace=True)      \n",
    "\n",
    "# Count occurrences of each pair\n",
    "pair_counts = pairs.groupby(['Company', 'related_companies']).size().reset_index(name='count')\n",
    "\n",
    "# Set the index to be a MultiIndex with source and related_companies\n",
    "pair_counts = pair_counts.set_index(['Company', 'related_companies'])\n",
    "\n",
    "# Convert to dictionary\n",
    "day_edges_dict = pair_counts['count'].to_dict()\n",
    "print(len(day_edges_dict))\n",
    "day_edges_dict = eg.add_self_edge(day_edges_dict) \n",
    "print(len(day_edges_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yingjie_niu/Year_3/GNN_longterm/data/raw/SPNews/price/AAL.csv\")\n",
    "df['log return'] = 100 * np.log(df['Close']/df['Close'].shift(1))      # Calculate Log return = 100 * r(t+1)/r(t)\n",
    "for m in [1,5,10,21]:   \n",
    "    # for each step length, calculate the r_mean and std of log returns\n",
    "    df['r_mean'+str(m)] = df['log return'].shift(-m).rolling(window=m).mean()\n",
    "    df['std'+str(m)] = df['log return'].shift(-m).rolling(window=m).std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "## Train, Dev, Test split\n",
    "dev_len, test_len = int(399/10), int(399/10)\n",
    "train_len = 399-dev_len-test_len\n",
    "print(train_len, dev_len, test_len)\n",
    "feat = 'std10'\n",
    "train_feat = df.iloc[:train_len][feat]\n",
    "dev_feat = df.iloc[train_len : train_len+dev_len][feat]\n",
    "test_feat = df.iloc[train_len+dev_len : train_len+dev_len+test_len][feat]\n",
    "sns.kdeplot(train_feat, shade = True, color='r', label = 'train')\n",
    "sns.kdeplot(dev_feat, shade = True, color='g', label = 'dev')\n",
    "sns.kdeplot(test_feat, shade = True, color='b', label = 'test')\n",
    "plt.xlabel('Feature')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of valid firms\n",
    "valid_firms = {'HUM', 'CNC', 'ITW', 'BA'} \n",
    "\n",
    "# Flatten the Series into a list of tuples\n",
    "all_pairs = [pair for sublist in day_news[\"permutations\"] for pair in sublist]\n",
    "\n",
    "# Count the occurrences of each tuple using Counter\n",
    "pair_counts = Counter(all_pairs)\n",
    "\n",
    "# Convert the Counter to a dictionary (if needed)\n",
    "filtered_pair_counts = {pair: count for pair, count in pair_counts.items() if pair[0] in valid_firms and pair[1] in valid_firms}\n",
    "\n",
    "filtered_pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Generate preprocessed_tickerwise\n",
    "\n",
    "\n",
    "# # Specify the folder containing the CSV files\n",
    "# save_path = '/home/yingjie_niu/Year_3/GNN_longterm/data/raw/SPNews/news/preprocessed_tickerwise'\n",
    "\n",
    "# # Define the header\n",
    "# header = ['Id', 'Company', 'Date', 'Title', 'Publisher', 'Link', 'ProviderPublishTime', 'Type', 'Related Company', 'Text']\n",
    "\n",
    "\n",
    "# # Function to convert date format to string\n",
    "# def convert_date_format(date_str):\n",
    "#     # print(date_str)\n",
    "#     if len(date_str.split(\"-\")) == 3:\n",
    "#         y, m, d = date_str.split(\"-\")[0], date_str.split(\"-\")[1], date_str.split(\"-\")[2]\n",
    "#         # print(y,m,d,len(m), len(d))\n",
    "#         if len(m) == 1:\n",
    "#             m = \"0\"+m\n",
    "#         if len(d) == 1:\n",
    "#             d = \"0\"+d\n",
    "#         # print(y+'-'+m+'-'+d)\n",
    "#         return y+'-'+m+'-'+d\n",
    "#     else:\n",
    "#         return date_str\n",
    "\n",
    "\n",
    "# # Iterate through each file in the folder\n",
    "# for filename in os.listdir(spnews_news_raw_202310):\n",
    "#     # Check if the file is a CSV file\n",
    "#     if filename.endswith('.csv'):\n",
    "#         file_path = os.path.join(spnews_news_raw, filename)\n",
    "#         file_path2 = os.path.join(spnews_news_raw_202310, filename)\n",
    "        \n",
    "#         # Read the CSV file\n",
    "#         df = pd.read_csv(file_path, header=None)\n",
    "#         df2 = pd.read_csv(file_path2, header=None)        \n",
    "        \n",
    "#         # Add the header\n",
    "#         df.columns = header\n",
    "#         df2.columns = header\n",
    "        \n",
    "#         # Concat df and df2\n",
    "#         df_concat = pd.concat([df, df2])\n",
    "#         df_concat.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#         # Convert the 'Date' column format to string\n",
    "#         df_concat['Date'] = df_concat['Date'].apply(lambda x: convert_date_format(x) if pd.notnull(x) else x)\n",
    "        \n",
    "#         # Save the CSV file back\n",
    "#         df_concat.to_csv(os.path.join(save_path, filename), index=False)\n",
    "        \n",
    "#         print(f'Updated {filename} with header.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yingjie_niu/Year_3/GNN_longterm/data/raw/SPNews/news/preprocessed_daily_2/2022-09-21.csv\")\n",
    "df.drop_duplicates(subset=\"Link\", keep=\"first\", inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate preprocessed_daily\n",
    "\n",
    "\n",
    "node_generator = NodeGenerator(\n",
    "            \"SPNews\", \n",
    "            num_node_features=8, \n",
    "            node_normalize=\"mean\", \n",
    "            num_Y_features=3, \n",
    "            classification=False,\n",
    "            Y_type=\"std\", \n",
    "            estimation_window=21, \n",
    "            matrix_node_features=True\n",
    "            )\n",
    "\n",
    "# for day in tqdm(node_generator.valid_natural_days):\n",
    "#     if day+\".csv\" in os.listdir(\"/home/yingjie_niu/Year_3/GNN_longterm/data/raw/SPNews/news/preprocessed_daily_2\"):\n",
    "#         pass\n",
    "#     else:\n",
    "#         dfs = []\n",
    "#         for ticker in node_generator.valid_firm_list:\n",
    "#             df = pd.read_csv(\"/home/yingjie_niu/Year_3/GNN_longterm/data/raw/SPNews/news/preprocessed_tickerwise/\"+ticker+'_news.csv')\n",
    "#             df.drop_duplicates(subset=\"Link\", keep=\"first\", inplace=True)\n",
    "#             df = df[df['Date'] == day]\n",
    "#             dfs.append(df)\n",
    "#         day_news = pd.concat(dfs, ignore_index=True)\n",
    "#         day_news.to_csv(\"/home/yingjie_niu/Year_3/GNN_longterm/data/raw/SPNews/news/preprocessed_daily_2/\"+day+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(node_generator.valid_firm_list), len(node_generator.all_firm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import ast\n",
    "day_edges_dict = {}\n",
    "# for i in range(len(df['Related Company'])):\n",
    "#     source = df['Company'].iloc[i]\n",
    "#     target_list = df['Related Company'].iloc[i]\n",
    "#     if target_list[0] == \"[\":\n",
    "#         target_list = ast.literal_eval(target_list)\n",
    "#         target_list.append(source)\n",
    "#         permutations = list(itertools.permutations(target_list , 2))\n",
    "#         for firm_pair in permutations:\n",
    "#             if firm_pair not in day_edges_dict:\n",
    "#                 day_edges_dict[firm_pair] = 1\n",
    "#             else:\n",
    "#                 day_edges_dict[firm_pair] += 1\n",
    "df['related_companies'] = df.apply(lambda x: ast.literal_eval(x['Related Company']) if x['Related Company'][0]==\"[\" else [x['Company']], axis=1)\n",
    "# Explode the related_companies list into multiple rows\n",
    "df_exploded = df.explode('related_companies')\n",
    "# df_exploded\n",
    "# Generate pairs (source, related) and (related, source)\n",
    "pairs = pd.concat([\n",
    "    df_exploded[['Company', 'related_companies']],\n",
    "    df_exploded[['related_companies', 'Company']].rename(columns={'related_companies': 'Company', 'Company': 'related_companies'})\n",
    "])\n",
    "print(pairs.shape)\n",
    "pairs[pairs['related_companies'].isin(node_generator.valid_firm_list)]\n",
    "# # Count occurrences of each pair\n",
    "# pair_counts = pairs.groupby(['Company', 'related_companies']).size().reset_index(name='count')\n",
    "# pair_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index to be a MultiIndex with source and related_companies\n",
    "pair_counts = pair_counts.set_index(['Company', 'related_companies'])\n",
    "\n",
    "# Convert to dictionary\n",
    "pairs_dict = pair_counts['count'].to_dict()\n",
    "pairs_dict\n",
    "# print(day_edges_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incomplete price data record:  CA . Removed from valid_firm_list.\n",
      "Valid Firms:  269\n",
      "Incomplete price data record:  PXD 388\n",
      "Generator for SPNews is initialized ...\n",
      "Total Firms:  286 , Valid Firms:  268\n",
      "388 trading days.  532 natual days.\n",
      "Incomplete price data record:  CA . Removed from valid_firm_list.\n",
      "Valid Firms:  269\n",
      "Incomplete price data record:  PXD 388\n",
      "Incomplete price data record:  CA . Removed from valid_firm_list.\n",
      "Valid Firms:  269\n",
      "Incomplete price data record:  PXD 388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "100%|██████████| 388/388 [01:22<00:00,  4.69it/s]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = FirmRelationGraph(\n",
    "    dynamic=\"cooccur\",\n",
    "    num_node_features=8,\n",
    "    matrix_node_features=True,\n",
    "    node_normalize='mean',\n",
    "    num_Y_features=3,\n",
    "    Y_type=\"std\",\n",
    "    classification=False,\n",
    "    estimation_window=21,\n",
    "    num_edge_features=1,\n",
    "    graph_level_task=False,\n",
    "    weighted_edge=True,\n",
    "    threshold=0,\n",
    "    root = \"/home/yingjie/Year_3/GNN_longterm/data/graph_sets\",\n",
    "    name=\"SPNews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "388\n",
      "Data(x=[268, 21, 8], edge_index=[2, 1612], edge_attr=[1612, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 1614], edge_attr=[1614, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 1724], edge_attr=[1724, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 1790], edge_attr=[1790, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 1900], edge_attr=[1900, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 1956], edge_attr=[1956, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 1994], edge_attr=[1994, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2024], edge_attr=[2024, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2090], edge_attr=[2090, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2190], edge_attr=[2190, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2230], edge_attr=[2230, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2262], edge_attr=[2262, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2314], edge_attr=[2314, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2384], edge_attr=[2384, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2462], edge_attr=[2462, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2494], edge_attr=[2494, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2534], edge_attr=[2534, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2590], edge_attr=[2590, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2634], edge_attr=[2634, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2770], edge_attr=[2770, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 1997], edge_attr=[1997, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2072], edge_attr=[2072, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2092], edge_attr=[2092, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2031], edge_attr=[2031, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2125], edge_attr=[2125, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2110], edge_attr=[2110, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2153], edge_attr=[2153, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2173], edge_attr=[2173, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2110], edge_attr=[2110, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2196], edge_attr=[2196, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2219], edge_attr=[2219, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2233], edge_attr=[2233, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2259], edge_attr=[2259, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2234], edge_attr=[2234, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2267], edge_attr=[2267, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2257], edge_attr=[2257, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2209], edge_attr=[2209, 1], y=[268, 3])\n",
      "Data(x=[268, 21, 8], edge_index=[2, 2179], edge_attr=[2179, 1], y=[268, 3])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "dev_len, test_len = int(len(dataset)/10), int(len(dataset)/10)\n",
    "train_len = len(dataset)-dev_len-test_len\n",
    "print(test_len)\n",
    "test_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "print(len(test_loader))\n",
    "for i in range(test_len):\n",
    "    batch = dataset[i].to(device)\n",
    "    print(batch)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_firm_list = dataset.graphgenerator.all_firm_list\n",
    "\n",
    "valid_firm_list = dataset.graphgenerator.valid_firm_list\n",
    "\n",
    "# Convert valid_firm_list to a set for efficient lookup\n",
    "valid_firm_set = set(valid_firm_list)\n",
    "\n",
    "# Create a mask where True represents nodes of interest\n",
    "mask = torch.tensor([firm in valid_firm_set for firm in all_firm_list], dtype=torch.bool)\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alist = dataset.graphgenerator.edge_generator.tweets_of_each_day['2014-01-24']\n",
    "seen = set()\n",
    "unique_dicts = []\n",
    "repeat_dicts = []\n",
    "for d in alist:\n",
    "    dict_str = json.dumps(d, sort_keys=True)\n",
    "    if dict_str not in seen:\n",
    "        seen.add(dict_str)\n",
    "        unique_dicts.append(d)\n",
    "    else:\n",
    "        repeat_dicts.append(d)\n",
    "print(len(seen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(dict_list:list) -> list:\n",
    "        \"\"\"\n",
    "        This function is to remove duplicate tweets in a day. It is specifically for ACL2018 dataset.\n",
    "        \"\"\"\n",
    "        seen = set()\n",
    "        unique_dicts = []\n",
    "        \n",
    "        for d in dict_list:\n",
    "            # Convert the dictionary to a JSON string\n",
    "            dict_str = json.dumps(d, sort_keys=True)\n",
    "            \n",
    "            if dict_str not in seen:\n",
    "                seen.add(dict_str)\n",
    "                unique_dicts.append(d)\n",
    "        \n",
    "        return unique_dicts\n",
    "\n",
    "remove_duplicates(alist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree = []\n",
    "# for i in range(len(dataset)):    \n",
    "#     degree.append(dataset[i].edge_index.shape[1]/ 76)\n",
    "# np.mean(degree)\n",
    "\n",
    "degree = []\n",
    "for i in range(len(dataset)):\n",
    "    # print(dataset[i].edge_index.shape)    \n",
    "    degree.append(dataset[i].edge_index.shape[1]/ 268)\n",
    "np.mean(degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimension N of the identity matrix\n",
    "N = 4  # You can change this to any desired value\n",
    "\n",
    "# Create a single identity matrix of shape [N, N]\n",
    "identity_matrix = torch.eye(N)\n",
    "\n",
    "# Repeat this identity matrix 3 times along a new dimension to get a tensor of shape [3, N, N]\n",
    "identity_matrices = identity_matrix.unsqueeze(0).repeat(3, 1, 1)\n",
    "\n",
    "print(identity_matrices)\n",
    "print(identity_matrices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_matrices + identity_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 10\n",
    "edge_idx = np.ones([2,num_nodes], dtype=int)\n",
    "edge_idx[:,range(num_nodes)] = range(num_nodes)   # edge_index only have self loops, i.e. [[0,0],[1,1],[2,2]...]\n",
    "edge_idx_T = edge_idx.T\n",
    "edge_attr = np.ones([num_nodes, 1], dtype=int)\n",
    "edge_idx_T.dtype, edge_attr.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_target_dict = dataset.graphgenerator.node_generator.ticker_target_dic\n",
    "for t in ticker_target_dict:\n",
    "    # if np.any(np.isnan(ticker_target_dict[t])):\n",
    "    #     print(t)\n",
    "    if ticker_target_dict[t].isnull().any().any():\n",
    "        print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yingjie_niu/Year_3/GNN_longterm/data/raw/SPNews/price/AAL.csv\")\n",
    "df[df[df['Date']=='2022-09-20'].index[0]:df[df['Date']=='2024-04-24'].index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_len, test_len = int(len(dataset)/10), int(len(dataset)/10)\n",
    "train_len = len(dataset)-dev_len-test_len\n",
    "print(train_len, dev_len, test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_of_each_day = dataset.graphgenerator.edge_generator.edge_of_each_day\n",
    "# len(edge_of_each_day.keys()), len(dataset.graphgenerator.valid_natural_days)\n",
    "edge_of_each_day['2023-10-20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yingjie_niu/Year_3/GNN_longterm/data/raw/SPNews/price/AAL.csv\")\n",
    "start = df[df[\"Date\"] == \"2023-10-20\"].index[0]\n",
    "end = df[df[\"Date\"] == \"2023-11-20\"].index[0]\n",
    "print(list(df[start:end]['Date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = torch.tensor([1,2])\n",
    "target = torch.tensor([2,1])\n",
    "adj = torch.zeros(3,3)\n",
    "adj[source,target] = torch.ones(len(source))\n",
    "adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = \"2014-01-02\"\n",
    "ticker_target_df = dataset.graphgenerator.node_generator.ticker_target_dic['AAPL']\n",
    "estimation_end = ticker_target_df.index[ticker_target_df['Date'] == day].item()+1   # datafrme slice need the ending index+1 so that the record of that day be included\n",
    "estimation_start = estimation_end - 20\n",
    "ticker_target_df_of_day = ticker_target_df.iloc[estimation_start:estimation_end]    # shape [estimation_window, 14], from \"day-estimation_window\" to \"day\", columns:  ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'log return', 'r_mean5', 'std5', 'r_mean10', 'std10', 'r_mean20', 'std20']\n",
    "ticker_day_Y = ticker_target_df_of_day[['std5']]\n",
    "y = ticker_day_Y.iloc[-1].item()\n",
    "y, ticker_day_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataset[0].x\n",
    "a = a.to(device='cuda:0')\n",
    "a.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "path = '/home/yingjie_niu/Year_3/AD_GAT/data/ACL2018/'    \n",
    "with open(path+'x_textual.pkl', 'rb') as handle:\n",
    "    y_load = pickle.load(handle) # 730, 198\n",
    "y_load.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = \"2014-06-02\"\n",
    "ticker_target_df = dataset.graphgenerator.node_generator.ticker_target_dic['AAPL']\n",
    "estimation_end = ticker_target_df.index[ticker_target_df['Date'] == day].item()+1   # datafrme slice need the ending index+1 so that the record of that day be included\n",
    "estimation_start = estimation_end - 20\n",
    "ticker_target_df_of_day = ticker_target_df.iloc[estimation_start:estimation_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_day_Y = ticker_target_df_of_day[['r_mean5', 'r_mean10', 'r_mean20',]]\n",
    "ticker_day_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ticker_day_Y.loc[estimation_end-1]\n",
    "def to_classification_target(y, ticker_day_Y, Y_type):\n",
    "    if Y_type == \"mean\":\n",
    "        iter = [(\"r_mean5\", 5), (\"r_mean10\", 10), (\"r_mean20\", 19)]\n",
    "    elif Y_type == \"std\":\n",
    "        iter = [(\"std5\", 5), (\"std10\", 10), (\"std20\", 19)]\n",
    "    elif Y_type == \"all\":\n",
    "        iter = [(\"r_mean5\", 5), (\"r_mean10\", 10), (\"r_mean20\", 19), (\"std5\", 5), (\"std10\", 10), (\"std20\", 19)]\n",
    "    elif Y_type == \"r_mean5\":\n",
    "        iter = [(\"r_mean5\", 5)]\n",
    "    elif Y_type == \"r_mean10\":\n",
    "        iter = [(\"r_mean10\", 10)]\n",
    "    elif Y_type == \"r_mean20\":\n",
    "        iter = [(\"r_mean20\", 20)]\n",
    "    elif Y_type == \"std5\":\n",
    "        iter = [(\"std5\", 5)]\n",
    "    elif Y_type == \"std10\":\n",
    "        iter = [(\"std10\", 10)]\n",
    "    elif Y_type == \"std20\":\n",
    "        iter = [(\"std20\", 19)]\n",
    "    \n",
    "    for col, lagging in iter:\n",
    "        y[col] = 1 if ticker_day_Y[col].iloc[-1] > ticker_day_Y[col].iloc[-1-lagging] else 0\n",
    "    return y\n",
    "\n",
    "post_y = to_classification_target(y, ticker_day_Y=ticker_day_Y, Y_type=\"mean\")\n",
    "print(y, post_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the distribution of Train/Val/Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "goog = dataset.graphgenerator.node_generator.ticker_target_dic['GOOG']\n",
    "aapl = dataset.graphgenerator.node_generator.ticker_target_dic['AAPL']\n",
    "# dataset.graphgenerator.edge_generator.edge_of_each_day\n",
    "df= aapl\n",
    "\n",
    "dev_len, test_len = int(len(dataset)/10), int(len(dataset)/10)\n",
    "train_len = len(dataset)-dev_len-test_len\n",
    "print(train_len, dev_len, test_len)\n",
    "\n",
    "feat = 'std5'\n",
    "train_feat = df.iloc[:train_len][feat]\n",
    "dev_feat = df.iloc[train_len : train_len+dev_len][feat]\n",
    "test_feat = df.iloc[train_len+dev_len : train_len+dev_len+test_len][feat]\n",
    "sns.kdeplot(train_feat, shade = True, color='r', label = 'train')\n",
    "sns.kdeplot(dev_feat, shade = True, color='g', label = 'dev')\n",
    "sns.kdeplot(test_feat, shade = True, color='b', label = 'test')\n",
    "plt.xlabel('Feature')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat.plot()\n",
    "dev_feat.plot()\n",
    "test_feat.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "# ROOT_FOLDER = \"/home/yingjie/Year_3/\"\n",
    "ROOT_FOLDER = \"/home/yingjie_niu/Year_3/\"\n",
    "import torch\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "sys.path.append(ROOT_FOLDER+\"GNN_longterm/code/models/\")\n",
    "from GAT import GAT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gat = GAT(8, 3, 1, matrix_in=True, batch_size=2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_fp = \"/home/yingjie_niu/Year_3/GNN_longterm/saved/ACL2018/dynamic-cooccur/threshold-0/weighted_edge-True/edge_feature-1/Y_feature-4_Ytype-mean/node_feature-8_norm-mean_matrix-True_ESwindow-21/Epoch-100_lr-0.0001_GAT_-20240723-115031.pt\"\n",
    "gat = torch.load(load_fp, map_location='cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM_layer(\n",
      "    (lstm): LSTM(8, 8, batch_first=True)\n",
      "  )\n",
      "  (output): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# print(gat.hidden, gat.out_head, gat.dropout)\n",
    "print(gat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.DoubleTensor([0.9, 0.22])\n",
    "torch.nn.functional.softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ = torch.concatenate([a,a], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_.shape, li.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = torch.DoubleTensor(2, 8, 3)\n",
    "torch.matmul(a_, li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "F.softmax(a,dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(a,dim=2) + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.DoubleTensor([0.5,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try LLM Generating Relationship\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Qwen2-7B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (0.33.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: psutil in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from accelerate) (0.24.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from accelerate) (0.4.4)\n",
      "Requirement already satisfied: filelock in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: requests in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingjie/miniconda3/envs/GNN/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading shards: 100%|██████████| 4/4 [00:57<00:00, 14.37s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import torch\n",
    "# device = \"cuda\" # the device to load the model onto\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-7B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, model=model, tokenizer=tokenizer):\n",
    "\n",
    "    # prompt = \"Give me a short introduction to large language model.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful financial text analyser.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=256\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(\n",
    "    prompt=\"\"\"\n",
    "Given the following text, what could be the strength of relationship between the mentioned firms? Please generate the output in the format of Python dictionary where the key is pair of firms and the value is its strength between -1 to 1, e.g. {(\"RPT\", \"TGT\") : 0.1}. \n",
    "\n",
    "Mentioned Firms: \"RPT\", \"TGT\", \"FRT\", \"SPG\"\n",
    "\n",
    "Text: When you get a 1-2 punch of inflation and recessionary fears as we’ve had in 2022, investors begin to lose faith in the ability of shopping centers and regional malls to thrive under adverse conditions. As a result, the prices of retail stocks have been hit hard in recent months. Analysts have been forced to lower their target prices, yet their upside targets from current levels remain high. Here are three retail REITs that analysts have recently cited as having the highest potential upside:RPT Realty (NYSE: RPT) is a NYC-based REIT that owns, operates and manages 57 open air shopping centers across 16 states, mostly in the Northeastern U.S. RPT Realty has a 93.3% overall occupancy rate in its portfolio of stores. This rate has been fairly stable over the last two years.The 52-week range for RPT is $7.51 to $14.99, but the stock has fallen 42% since last November. The stock is currently sitting right near its 52-week low.2nd quarter 2022 revenue declined, but earnings per share (EPS) was up from the previous quarter. RPT Realty pays a $0.52 annual dividend, yielding just above 6%.Raymond James analyst R.J. Milligan recently maintained an Outperform rating on RPT, while lowering the price target to $13 from $15. At a recent price of $7.55, this creates a huge upside potential of 72%.Federal Realty Investment (NYSE: FRT) is another retail REIT, based in North Bethesda, MD. Federal Realty Investment was established in 1962 and is a member of the S&amp;P 500. The REIT owns and operates 105 retail stores in grocery-anchored shopping centers and mixed-use retail centers.Federal Realty prides itself on having the longest record of annual dividend rate increases among U.S. REITs. The 52-week price range is $86.50 to $140.51. Just like RPT Realty, it is currently sitting near its 52-week low and seems to be in a serious downtrend.However, Milligan continues to see value in the stock. He also maintained a Strong Buy on Federal Realty, even while lowering the price target from $140 to $130. Therefore, from its recent price of $88, the analyst’s view would be that Federal Realty now has an upside potential of 47.7%. That’s a lot of ground to make up, but with its long-standing history, an annual dividend of $4.32 and yield of 4.7%, this REIT could be a long-term winner. However, investors may want to see some price stability first.Story continuesSimon Property Group Inc (NYSE: SPG) is one of the largest and most well-known retail REITs. Simon owns and manages shopping centers and premier outlet malls in 37 states and Puerto Rico. It also owns properties in Asia, Canada and Europe. The Indianapolis-based REIT is a member of the S&amp;P 100.Simon Property Group stock dropped significantly in the 2020 COVID-19 market crash, declining from $128 in January to less than $37 by the end of March. Investors who stayed the course were rewarded when SPG rebounded to $161 by November 2021. However, since then, the stock has again languished due to inflation and recessionary concerns. The stock is hitting new lows for the year.Regardless, Richard Hill of Morgan Stanley recently maintained his Overweight position on Simon, while slightly lowering the target price from $133 to $131. This represents about a 51% potential upside from Simon’s recent price of $86.75. Given the REIT’s history of rebounding, and an annual dividend of $7.00 that yields 7.5%, Simon Property Group could be a substantial bargain at this level.Check out: This Little Known REIT Has Produced Double-Digit Annual Returns For The Past Five YearsPlease bear in mind that analysts’ opinions are not always correct, and the best analysts are only right about half the time. Investors should therefore perform their own due diligence when making decisions about what stocks to buy, and simply use the price targets as a guide.Latest Private Market Real Estate Insights:Arrived Homes expanded its offerings to include shares in short-term rental properties with a minimum investment of $100. The platform has already funded over 160 single-family rentals valued at over $60 million.The Flagship Real Estate Fund through Fundrise is up 7.3% year to date and has just added a new rental home community in Charleston, SC to its portfolio.Find more news, insights and offerings on Benzinga Alternative InvestmentsImage by Vintage Tone on ShutterstockSee more from Benzinga3 Health Care REITs With The Highest Upside, According to Analysts3 REITs with the Highest Upside According to AnalystsDon't miss real-time alerts on your stocks - join Benzinga Pro for free! Try the tool that will help you invest smarter, faster, and better.© 2022 Benzinga.com. Benzinga does not provide investment advice. All rights reserved.                   \n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing the provided text, we can see that there are relationships between the firms discussed and how they are perceived by analysts. However, the text doesn't provide direct quantitative data to measure the strength of these relationships. Instead, it offers qualitative insights into analyst opinions and potential stock performance.\n",
      "\n",
      "To create a dictionary representing the strength of the relationship between the firms, we might consider the following:\n",
      "\n",
      "1. **Analyst Opinions**: If an analyst maintains an \"Outperform\" or \"Strong Buy\" rating on a stock, it could indicate a positive relationship between the firm and the analyst, suggesting a higher potential for growth or a more optimistic outlook.\n",
      "   \n",
      "2. **Price Target Changes**: Decreases in price targets might suggest a less favorable view of a firm's future prospects, while increases could imply a stronger relationship or more positive outlook.\n",
      "\n",
      "3. **Stock Performance**: Stocks that are close to their 52-week lows might be seen as undervalued or offer significant upside potential, which could imply a strong relationship between the firm and analysts considering the potential for recovery.\n",
      "\n",
      "4. **Historical Performance**: References to past stock performance, especially during significant market events like the COVID-19 market crash, can indicate how analysts perceive a firm's resilience or vulnerability.\n",
      "\n",
      "Based on these\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Iterate through daily news in preprocessed_daily_2 folder, generate the response for each row. \n",
    "## Skip the rows where there is no text.\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "preprocessed_daily_2 = \"/home/yingjie_niu/Year_3/GNN_longterm/data/raw/SPNews/news/preprocessed_daily_2/\"\n",
    "for f in os.listdir(preprocessed_daily_2):\n",
    "    df = pd.read_csv(preprocessed_daily_2+\"2022-09-27.csv\", index_col=0)\n",
    "    df = df[~df['Text'].str.startswith(\"[\"\"]\")]     # skip the rows without \"Text\"\n",
    "    df = df[~df['Text'].str.startswith(\"['']\")]     # skip the rows without \"Text\"\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    print(df['Text'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Generated Text'] = df.apply(lambda x: generate(prompt=f\"\"\"Given the following text, what could be the strength of relationship between the mentioned firms? Please generate the output in the format of Python dictionary where the key is pair of firms and the value is its strength between -1 to 1, e.g. (\"RPT\", \"TGT\") : 0.1. \n",
    "\n",
    "Mentioned Firms: {x[\"Related Company\"].strip(\"[]\")}\n",
    "\n",
    "Text: {x[\"Text\"].strip(\"[]\")} \"\"\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Generated Text'] = df['Date']\n",
    "for i in range(df.shape[0]):\n",
    "    df['Generated Text'][i] = generate(prompt=f\"\"\"Given the following text, what could be the strength of relationship between the mentioned firms? Please generate the output in the format of Python dictionary where the key is pair of firms and the value is its strength between -1 to 1, e.g. (\"RPT\", \"TGT\") : 0.1. \n",
    "\n",
    "Mentioned Firms: {df[\"Related Company\"][i].strip(\"[]\")}\n",
    "\n",
    "Text: {df[\"Text\"][i].strip(\"[]\")} \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Text'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Generated Text'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68256"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import comb\n",
    "\n",
    "comb(5, 3) * 0.6**3 * 0.4**2 + comb(5, 4) * 0.6**4 * 0.4**1 + comb(5, 5) * 0.6**5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
